\documentclass{rapportECL}
\usepackage{listings}
\usepackage{lipsum}
\usepackage{gensymb}
\usepackage{float}
\usepackage{xcolor}
\usepackage{biblatex}
\bibliography{biblio} 
\usepackage{minted}
\usepackage{caption}
\usepackage{amssymb} % pour \checkmark
\usepackage{verbatim}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{placeins}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{url} % Pour gérer les URLs
\usepackage{graphicx}
\usepackage{array}     % For more control over column alignment
\usepackage{colortbl}  % Table coloring
\usepackage{tikz} % figs
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, bending}

\title{ArchiLogiciel } %Titre du fichier
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{techheader}{HTML}{0F172A}
\definecolor{techlinea}{HTML}{F8FAFF}
\definecolor{techlineb}{HTML}{EEF4FF}
\definecolor{techroute}{HTML}{E0F2FE}

\lstset{
    backgroundcolor=\color{white},   
    basicstyle=\footnotesize\ttfamily,   
    breaklines=true,                 
    captionpos=b,                    
    commentstyle=\color{mygreen},    
    keywordstyle=\color{blue},       
    stringstyle=\color{mymauve},     
    frame=single,                    
    rulecolor=\color{black},         
    language=C,                      
    showspaces=false,                
    showstringspaces=false,          
    tabsize=4 ,
    inputencoding=utf8,           % Indique que l'encodage utilisé est UTF-8
    extendedchars=true,           % Utilise des caractères étendus
    literate=%
     {é}{{\'e}}1                 % Configuration pour les accents
     {è}{{\`e}}1
     {à}{{\`a}}1
     {ç}{{\c{c}}}1
     {ê}{{\^e}}1
     {ù}{{\`u}}1
     {ô}{{\^o}}1
     {â}{{\^a}}1
}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\begin{document}

%----------- Informations du rapport ---------

\titre{OCULA} %Titre du fichier .pdf
\UE{\textbf{Projet ZZ2 F2}} %Nom de la UE
\enseignant{Loïc \textsc{Yon}} %Nom de l'enseignant
\eleves{Mouad \textsc{Ismaili M'hamdi} \\
{Brahim \textsc{Id Benouakrim}}\\
{Achraf \textsc{El Allali}}
 } %Nom des élèves

%----------- Initialisation -------------------
        
\fairemarges %Afficher les marges
\fairepagedegarde %Créer la page de gard
\tabledematieres %Créer la table de matières

%------------ Corps du rapport ----------------



\section{Introduction}
\subsection{Contexte et motivation}
\noindent \textit{À compléter.}
\subsection{Problématique}
\noindent \textit{À compléter.}
\subsection{Objectifs du projet}
\noindent \textit{À compléter.}
\subsection{Périmètre et hypothèses}
\noindent \textit{À compléter.}
\subsection{Organisation du rapport}
\noindent \textit{À compléter.}
\section{Organisation et gestion du projet}
\noindent \textit{À compléter.}
\subsection{Répartition des responsabilités et approche collaborative}
\noindent \textit{À compléter.}
\subsection{Méthodologie Agile itérative}
\noindent \textit{À compléter.}
\subsection{Conception UX : maquettes et validation fonctionnelle}
\noindent \textit{À compléter.}
\subsection{Approche DevOps et stratégie Docker-first}
\noindent \textit{À compléter.}
\subsection{Planification et diagramme de Gantt}
\noindent \textit{À compléter.}
\subsection{Processus de validation et revue de code}
\noindent \textit{À compléter.}
\subsection{Difficultés rencontrées et gestion des risques}
\noindent \textit{À compléter.}

\section{État de l'art et fondements théoriques}
\subsection{Analyse de contenu vidéo : enjeux et défis}
\noindent \textit{À compléter.}
\subsubsection{Données non structurées et difficulté d'indexation}
\noindent \textit{À compléter.}
\subsubsection{Contraintes de latence, coût et confidentialité}
\noindent \textit{À compléter.}

\subsection{Reconnaissance automatique de la parole (ASR)}
ASR (automatic speech recognition) est une technologie qui convertit le langage parlé en texte écrit. Elle traite les signaux audio, identifie les modèles de parole et les transcrit en texte avec une grande précision.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/fonctionnement-automatic-speech-recognition.jpg}
    \caption{Reconnaissance automatique de la parole}
    \label{fig:placeholder}
\end{figure}
\subsubsection{Principes généraux de la transcription audio--texte}
Principalement on distingue entre deux approches pour la reconnaissance automatique de la parole:

\begin{itemize}
    \item \textbf{Approche hybride classique: }est une méthode qui combine un modèle acoustique, un modèle de lexique et un modèle linguistique pour transcrire la parole en texte, en s’appuyant sur des données audio alignées afin d’associer précisément les segments aux phonèmes et aux mots.

    \item  \textbf{Deep Learning: }utilise des réseaux de neurones pour convertir directement la parole en texte, sans séparer explicitement les modèles acoustique, lexique et linguistique. Elle apprend automatiquement les relations entre les sons et les mots à partir de grandes quantités de données audio.
\end{itemize}

\subsubsection{Choix de modèle (Whisper)}

Whisper est un système de reconnaissance automatique de la parole (ASR) basé sur l’intelligence artificielle, entraîné sur environ 680 000 heures de données audio multilingues provenant du web. Grâce à cette très grande diversité de données, il est particulièrement robuste face aux accents, au bruit de fond et au vocabulaire technique. Whisper est capable non seulement de transcrire la parole dans plusieurs langues, mais aussi de traduire directement ces langues vers l’anglais, ce qui en fait un outil polyvalent pour de nombreuses applications de traitement vocal et de recherche en intelligence artificielle.


\begin{table}[h]
\centering
\caption{Comparaision des Performance des Models ASR}
\begin{tabular}{lccccc}
\hline
\textbf{Criteria} & \textbf{Whisper} & \textbf{AssemblyAI U3} & \textbf{Amazon} & \textbf{Microsoft} \\
\hline
Accuracy (English) & 92.4\% & \textbf{94.1\%} & 92.4\% & 92.5\% \\
Accuracy (Multilingual) & \textbf{92.6\%} & 91.3\% & 89.9\% & 88.9\% \\
WER (English) & 6.5\% & \textbf{5.9\%} & 7.6\% & 7.5\% \\
WER (Multilingual) & \textbf{7.4\%} & 8.7\% & 10.1\% & 11.1\% \\
\hline
\end{tabular}
\end{table}


Le choix de Whisper a été évident , du a ça capacité impressive dans la transcription multilingue , et sont WER minimal par rapport aux autre modèle. De plus nous favorisons les modèles open-source, et Whisper étant le meilleur dans cette catégorie.


\subsection{Représentations vectorielles et recherche sémantique}
\subsubsection{Définition des embeddings}
Un embedding est une représentation vectorielle d’éléments comme les textes, les images, les vidéos ou les signaux audio. Cette transformation en valeurs numériques permet aux modèles de machine learning de comprendre les relations sémantiques et d’effectuer des tâches telles que la recherche, la similarité ou la classification.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/embedding.png}
    \caption{Fonctionnement de embedding}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Indexation et recherche vectorielle}
L’indexation vectorielle est une technique qui consiste à organiser des données sous forme de vecteurs numériques afin de permettre une recherche rapide et efficace dans des espaces de grande dimension.

La recherche vectorielle est le processus qui consiste à comparer le vecteur d’une requête avec ceux stockés dans cet index afin d’identifier les éléments les plus proches sémantiquement.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/example_index_vec.png}
    \caption{Illustration de l’indexation vectorielle}
    \label{fig:placeholder}
\end{figure}

La figure illustre le principe de l’indexation et de la recherche vectorielle. 
Chaque point coloré représente un objet (mot, image ou donnée) transformé en vecteur 
dans un espace numérique. Les éléments ayant un sens similaire se retrouvent proches 
les uns des autres, formant des groupes sémantiques. Par exemple, les mots ``cat'', 
``dog'' et ``lion'' sont regroupés car ils appartiennent à la même catégorie d’animaux, 
tandis que ``car'', ``truck'' et ``vehicle'' forment un autre groupe lié aux moyens 
de transport. Lors d’une recherche vectorielle, le système consiste simplement à 
retrouver les points les plus proches du vecteur de la requête afin d’identifier 
les résultats les plus pertinents. 

\subsection{Modèles de langage (LLM)}
\subsubsection{Capacités et limites des LLM}
Un LLM (Large Language Model ou Grand Modèle de Langage) est un système d'intelligence artificielle entraîné sur d'immenses quantités de données textuelles pour comprendre, générer et manipuler le langage humain de manière fluide et cohérente.

\begin{itemize}
    \item \textbf{Capacités:}
    \begin{itemize}
        \item \textbf{Génération de contenu:} ils peuvent rédiger instantanément des textes.
        \item \textbf{Synthèse d'informations:} ils peuvent résumer des documents très long en extrayant les points clés.
        \item \textbf{Traduction et adaptation de style:} Ils peuvent traduire des langues et modifier le style d'un texte.
    \end{itemize}
    \item \textbf{Limites}
    \begin{itemize}
        \item \textbf{Hallucinations:} ils peuvent confirmer des informations fausses.
        \item \textbf{Coût de calcul élevé:} ils sont très chers à entraîner et à faire fonctionner. 
    \end{itemize}
\end{itemize}

\subsubsection{Choix de modèle (gpt-oss-120b)}
Lors de notre recherche des modèles de LLM à utiliser, nous avons identifié deux candidats principaux : gpt-oss-120b et llama3.1-8b, tous deux accessibles via l’API fournie par Cerebras.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{images/gpt-vs-llama.png}
    \caption{Camparaison entre gpt-oss-120b et llama3.1-8b}
    \label{fig:placeholder}
\end{figure}

En analysant les deux indices de performance, GPQA (Graduate-Level Google-Proof Q\&A) et MMLU (Massive Multitask Language Understanding), nous pouvons clairement constater que le modèle gpt-oss-120b surpasse llama3.1-8b. Cette supériorité s’explique notamment par son nombre beaucoup plus élevé de paramètres (120 milliards) ainsi que par son architecture plus avancée, lui permettant d’obtenir de meilleurs résultats sur des tâches complexes de raisonnement et de compréhension.


\subsection{Retrieval-Augmented Generation (RAG)}
\subsubsection{Principe général}
Le RAG (Retrieval-Augmented Generation) est une technique qui permet d'optimiser les réponses d'un modèle d'IA en lui donnant accès à des données externes fiables, au-delà de ses connaissances d'entraînement initiales.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/rag_pipeline.png}
    \caption{Pipeline de RAG}
    \label{fig:placeholder}
\end{figure}

Un pipeline de RAG fonctionne en trois étapes simples : d’abord, des documents sont collectés, découpés en petits morceaux puis transformés en vecteurs pour être stockés dans une base vectorielle. Ensuite, quand un utilisateur pose une question, elle est aussi convertie en vecteur afin de retrouver les passages les plus pertinents dans cette base. Enfin, ces informations sont envoyées avec la question à un LLM, qui s’en sert comme contexte pour produire une réponse plus précise et fiable.

\subsubsection{Forces et limites du RAG}
Le RAG est une approche qui combine la recherche d’informations externes avec la génération de texte par un LLM. Cette architecture permet d’améliorer la qualité des réponses, mais elle présente également certaines contraintes techniques.

\begin{itemize}
    \item \textbf{Amélioration de la précision :} Le RAG permet de générer des réponses plus fiables en s’appuyant sur des documents externes, ce qui réduit fortement les hallucinations des modèles de langage.
    
    \item \textbf{Dépendance à la qualité de la recherche :} La performance du système dépend fortement de la pertinence des documents récupérés.
    
    \item \textbf{Latence et complexité :} L’étape de récupération des données ajoute du temps de réponse et augmente les coûts techniques et computationnels.
\end{itemize}

\section{Spécifications et conception de l'application OCULA}
\subsection{Présentation générale de la solution}
Notre application \textbf{Ocula} est une solution dédiée au traitement et à l’analyse de vidéos. Elle permet d’extraire automatiquement les transcriptions, puis de les exploiter pour générer des titres et des résumés pertinents. De plus, elle intègre une approche RAG (Retrieval-Augmented Generation) afin de répondre de manière précise et contextuelle aux questions des utilisateurs concernant le contenu des vidéos.

\subsection{Cas d'utilisation et fonctionnalités}
\subsubsection{Upload et gestion des vidéos}
Notre solution \textbf{Ocula} gère les vidéos téléversées en s’appuyant sur le service de \textbf{Azure Blob Storage}. L’utilisateur peut importer une vidéo depuis la partie client, puis une requête est envoyée au backend afin de créer automatiquement une entrée correspondante dans notre base de données. La vidéo est ensuite stockée dans notre object storage, où elle est associée à une clé unique. Cette clé permet de générer une URL sécurisée donnant accès à la vidéo lorsque cela est nécessaire.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.15\linewidth]{images/upload_video.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Transcription et génération de titre et de résumé}
Notre solution \textbf{Ocula} permet d’extraire automatiquement la transcription des vidéos en exploitant les capacités de \textbf{Whisper}. Après le téléversement d’une vidéo, une tâche de traitement est déclenchée afin de l’analyser. Une fois la transcription extraite, nous enregistrons ses différents segments, puis nous divisons le script en plusieurs chunks.

Ces chunks sont ensuite transformés en embeddings et stockés dans une base de données vectorielle, afin de pouvoir être exploités ultérieurement dans un système \textbf{RAG} (Retrieval-Augmented Generation). Enfin, grâce à une pipeline composée d’un prompt et d’un LLM (\textbf{gpt-oss-120b}), la solution génère automatiquement un titre pertinent ainsi qu’un résumé du contenu de la vidéo.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/transcription.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Chatbot de question--réponse sur la vidéo}
Notre solution \textbf{Ocula} intègre un assistant \textbf{chatbot} basé sur l’intelligence artificielle. Cet assistant utilise un \textbf{LLM} afin de répondre aux questions des utilisateurs tout en s’appuyant sur une approche \textbf{RAG} (Retrieval-Augmented Generation) et sur les différents chunks générés lors du traitement des vidéos.

Lorsqu’un utilisateur envoie un message, le chatbot déclenche une pipeline RAG qui recherche les informations pertinentes dans les chunks de la transcription associés à la vidéo concernée. Le contexte ainsi récupéré est ensuite transmis au LLM (\textbf{gpt-oss-120b}), ce qui lui permet de fournir des réponses précises, pertinentes et adaptées au contenu réel de la vidéo.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.1\linewidth]{images/chat_case.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsection{Exigences fonctionnelles}

Les exigences fonctionnelles décrivent les capacités et services que doit fournir l’application OCULA du point de vue des utilisateurs et du système métier.

\subsubsection{Gestion des utilisateurs}

\begin{itemize}
	\item Le système doit permettre à un utilisateur de créer un compte à l’aide d’une adresse e-mail et d’un mot de passe.
	\item Le système doit permettre l’authentification sécurisée des utilisateurs.
	\item Le système doit permettre la modification des informations du profil (nom, prénom, e-mail, mot de passe, avatar).
	\item Le système doit gérer les sessions utilisateurs via un mécanisme d’authentification basé sur des tokens jwt.
\end{itemize}

\subsubsection{Gestion des vidéos}

\begin{itemize}
	\item Le système doit permettre à un utilisateur d’importer une vidéo depuis l’interface web.
	\item Le système doit stocker les vidéos dans un stockage objet.
	\item Le système doit associer chaque vidéo à son utilisateur propriétaire.
	\item Le système doit permettre d’afficher la liste des vidéos d’un utilisateur.
	\item Le système doit permettre de consulter les détails d’une vidéo (titre, résumé).
\end{itemize}

\subsubsection{Traitement automatique des vidéos}

\begin{itemize}
	\item Le système doit extraire automatiquement la transcription d’une vidéo importée.
	\item Le système doit segmenter la transcription en portions temporelles.
	\item Le système doit transformer ces segments en représentations vectorielles (embeddings).
	\item Le système doit générer automatiquement un titre et un résumé à partir de la transcription.
\end{itemize}

\subsubsection{Recherche sémantique et question--réponse}

\begin{itemize}
	\item Le système doit permettre à l’utilisateur de poser des questions sur le contenu d’une vidéo.
	\item Le système doit rechercher les segments les plus pertinents dans la base de données vectorielle.
	\item Le système doit générer des réponses contextuelles à l’aide d’un LLM.
	\item Le système doit conserver l’historique des échanges entre l’utilisateur et le chatbot.
\end{itemize}

\subsubsection{Suivi du traitement}

\begin{itemize}
	\item Le système doit gérer les différents états du traitement d’une vidéo (en attente, en cours, terminé, échoué).
	\item Le système doit permettre à l’utilisateur de suivre l’avancement du traitement en temps réel.
	\item Le système doit notifier l’interface utilisateur lors des changements d’état.
\end{itemize}

\subsection{Exigences non fonctionnelles}

Les exigences non fonctionnelles décrivent les contraintes techniques et qualitatives que doit respecter l’application OCULA afin d’assurer sa fiabilité, sa performance et sa sécurité.

\subsubsection{Performance et latence}

\begin{itemize}
	\item Le système doit assurer un temps de réponse rapide pour les interactions utilisateur (navigation, affichage des vidéos, consultation des résultats).
	\item Le traitement des vidéos doit être effectué de manière asynchrone afin de ne pas bloquer l’interface utilisateur.
	\item Le temps de réponse du chatbot doit rester raisonnable pour garantir une expérience fluide.
	\item Les opérations de recherche sémantique doivent être optimisées pour réduire la latence lors des requêtes.
\end{itemize}

\subsubsection{Sécurité et confidentialité}

\begin{itemize}
	\item Le système doit garantir l’authentification et l’autorisation des utilisateurs.
	\item Les données sensibles doivent être protégées lors du stockage et des communications.
	\item L’accès aux vidéos doit être restreint uniquement à leur propriétaire.
	\item la communication entre le backend et le service d’intelligence artificielle repose sur l’utilisation de clés d’API (API keys) permettant d’authentifier les requêtes et de restreindre l’accès aux endpoints internes.
\end{itemize}


\subsection{Architecture globale}
\subsubsection{Vue d'ensemble (Frontend, Backend, AI Service)}
Notre application \textbf{Ocula} suit une architecture structurée en trois principales couches. D’abord, le frontend permet à l’utilisateur de téléverser une vidéo et de demander son analyse. Ensuite, le backend gère les requêtes, enregistre les métadonnées dans la base de données et stocke les fichiers dans un système de stockage objet. Enfin, la partie IA prend en charge le traitement de la vidéo, elle génère la transcription, crée des embeddings stockés dans une base vectorielle, puis exploite ces données pour alimenter le LLM, qui produit des résumés, des titres et des réponses pertinentes aux questions des utilisateurs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/app_workflow.png}
    \caption{Vue d'ensemble}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Choix technologiques et justification}
Pour le développement de l’application \textbf{Ocula}, nous avons choisi des technologies adaptées à une architecture moderne et scalable.

Le \textbf{backend}, nous avons choisi \textbf{Spring-Boot}, un framework Java largement adopté pour la conception d’API robustes et sécurisées. Il offre une intégration avec les bases de données relationnelles, ainsi qu’un écosystème facilitant le développement.

Le \textbf{frontend} a été développé avec \textbf{Angular}, un framework structuré et performant, particulièrement adapté aux applications web complexes. \textbf{Angular} permet une bonne organisation du code, une communication fluide avec les API REST, ainsi qu’une expérience utilisateur dynamique et réactive.

La \textbf{partie intelligence artificielle}, nous avons utilisé \textbf{Python} avec \textbf{Flask} pour exposer les services IA sous forme d’API légères et rapides. L’intégration de \textbf{LangChain} a permis de faciliter la mise en place des pipelines RAG.

Pour la \textbf{persistance des données}, nous avons choisi \textbf{PostgreSQL} comme base de données relationnelle, en raison de sa capacité à gérer des structures de données complexes. En complément, nous avons utilisé \textbf{ChromaDB} comme base de données vectorielle, spécialement conçue pour le stockage et la recherche d’embeddings.

\subsection{Conception}
\subsubsection{Modèle conceptuel et logique}

Le modèle conceptuel d’OCULA est structuré autour de l’entité \textit{Video}, qui constitue le cœur fonctionnel du système. Bien que l’application soit multi-utilisateur, l’ensemble du cycle de traitement IA est vidéo-centré : les opérations d’analyse, de transcription, de génération d’embeddings et de question–réponse s’appuient systématiquement sur l’identifiant \texttt{video\_id}.
Ainsi, l’entité \textit{User} possède les vidéos, mais c’est la \textit{Video} qui orchestre le pipeline métier.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/diagramme de class entités.png}
\caption{Diagramme de classes du domaine métier d’OCULA}
\label{fig:domain-model}
\end{figure}

\paragraph{Relation User -- Video}

Un utilisateur peut posséder plusieurs vidéos (\textbf{1--N}). Cette relation n’est pas uniquement logique, elle répond à plusieurs exigences architecturales :

\begin{itemize}
    \item \textbf{Sécurité} : le contrôle d’accès repose sur le propriétaire de la vidéo ; un utilisateur ne peut accéder qu’à ses propres ressources.
    \item \textbf{Isolation des données} : chaque compte constitue un espace logique distinct.
    \item \textbf{Scalabilité} : les requêtes peuvent être filtrées et paginées par \texttt{userId}, améliorant les performances.
    \item \textbf{Architecture multi-tenant} : la base est partagée, mais le cloisonnement est assuré au niveau applicatif.
\end{itemize}

\paragraph{Relation Video -- VideoTranscript}  


Contrairement à une approche monolithique consistant à stocker une transcription complète sous forme d’un simple champ texte, OCULA adopte une modélisation segmentée. Une vidéo possède donc \textbf{0..*} entités \textit{VideoTranscript}, chacune caractérisée par un \texttt{startTime}, un \texttt{endTime} et un \texttt{transcriptText}.

Ce choix répond à plusieurs objectifs :

\begin{itemize}
    \item \textbf{Traçabilité temporelle} : chaque portion de texte peut être reliée à un intervalle précis de la vidéo.
    \item \textbf{Robustesse du pipeline} : en cas d’échec partiel du traitement, les segments déjà générés restent persistés.
    \item \textbf{Réindexation ciblée} : il est possible de retraiter ou ré-encoder certains segments sans recalcul global.
    \item \textbf{Granularité adaptée au RAG} : les segments constituent la base naturelle du découpage en chunks.
\end{itemize}

\paragraph{Relation Video -- Message}

Chaque vidéo peut contenir \textbf{0..*} messages correspondant aux échanges entre l’utilisateur et le chatbot. La persistance de ces messages dépasse le simple besoin d’affichage :

\begin{itemize}
    \item \textbf{Historique multi-session} : la conversation est conservée même après rechargement ou changement d’appareil.
    \item \textbf{Audit et explicabilité} : il est possible d’analyser les requêtes et réponses produites.
    \item \textbf{Évolutivité} : ces données peuvent servir à des analyses ultérieures ou à l’amélioration du système.
\end{itemize}

\paragraph{Gestion d’état du traitement}

L’attribut \texttt{status} de l’entité \textit{Video} (PENDING, PROCESSING, COMPLETED, FAILED) modélise explicitement le cycle de vie du traitement asynchrone.
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/Gestion d’état du traitement-2026-02-22-192854.png}
\caption{Gestion d’état du traitement (VideoStatus)}
\label{fig:domain-model}
\end{figure}
Dans un système distribué où la transcription et l’indexation peuvent prendre plusieurs minutes, ce champ est essentiel pour :

\begin{itemize}
    \item informer le frontend de l’avancement,
    \item permettre une mise à jour en temps réel via WebSocket,
    \item gérer proprement les erreurs et les tentatives de relance,
    \item éviter les états incohérents ou bloqués.
\end{itemize}

Ainsi, le modèle conceptuel ne se limite pas à une simple représentation des données : il structure le comportement global du système et soutient la robustesse de l’architecture.

\subsubsection{Modélisation des transcriptions, chunks et embeddings}

Si le modèle précédent décrit la structure relationnelle des données métier, le mécanisme de recherche sémantique repose sur une modélisation complémentaire adaptée au RAG.

\medskip
\textbf{1. De la transcription segmentée aux chunks}

La transcription est initialement stockée sous forme de segments temporels (\textit{VideoTranscript}). 
Toutefois, une recherche sémantique efficace ne peut s’appuyer ni sur un texte monolithique complet, ni sur des segments trop courts et isolés.

Le système applique donc une étape de \textit{chunking}, consistant à regrouper et découper les segments afin de produire des blocs textuels cohérents et exploitables pour l’indexation vectorielle.

Cette granularité intermédiaire permet :

\begin{itemize}
    \item d’améliorer la précision lors de la récupération de contexte,
    \item de réduire le bruit informationnel,
    \item d’optimiser l’utilisation de la fenêtre de contexte du LLM,
    \item de maintenir un équilibre entre cohérence globale et pertinence locale.
\end{itemize}

\medskip
\textbf{2. Génération des embeddings}


Chaque chunk est transformé en vecteur numérique à l’aide du modèle 
\texttt{sentence-transformers/all-MiniLM-L6-v2}, chargé via \textit{HuggingFaceEmbeddings}.

Ce modèle encode les textes en vecteurs de dimension fixe permettant une comparaison sémantique par mesure de similarité ou de distance vectorielle. 
Chaque chunk correspond ainsi à un triplet logique :

\begin{itemize}
    \item texte source,
    \item embedding haute dimension,
    \item métadonnées associées.
\end{itemize}

Cette représentation vectorielle permet de comparer la question de l’utilisateur avec le contenu vidéo et d’identifier les passages les plus pertinents.

\medskip
\textbf{3. Organisation dans la base vectorielle}

Les embeddings sont stockés dans ChromaDB selon une organisation par vidéo. 
Pour chaque vidéo, une collection dédiée (\texttt{video\_<video\_id>}) est utilisée ou créée lors de l’initialisation du flux d’indexation.

Chaque document vectoriel contient :

\begin{itemize}
    \item le texte du chunk,
    \item l’embedding associé,
    \item des métadonnées : \texttt{video\_id}, \texttt{start\_time}, \texttt{end\_time}.
\end{itemize}
\begin{figure}[H]
    \centering
        \includegraphics[width=0.4\textwidth]{images/chroma_document_model.png}
    \caption{Structure d’un document vectoriel dans ChromaDB}
    \label{fig:chroma-document}
\end{figure}

Lors d’une requête utilisateur, le mécanisme RAG :

\begin{itemize}
    \item effectue une recherche par similarité dans la collection de la vidéo concernée,
    \item sélectionne les $k$ passages les plus proches,
    \item applique un filtrage et un éventuel \textit{reranking} afin d’améliorer la pertinence du contexte transmis au LLM.
\end{itemize}

Cette organisation garantit :

\begin{itemize}
    \item l’isolation des recherches à une vidéo donnée,
    \item la traçabilité des sources utilisées dans la réponse,
    \item une correspondance directe entre passage récupéré et intervalle temporel.
\end{itemize}

Le modèle relationnel assure la cohérence transactionnelle des données métier, tandis que la couche vectorielle optimise la récupération contextuelle pour le RAG ; les deux couches sont donc complémentaires et synchronisées par \texttt{video\_id}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/rag_pipeline_matrix.png}
    \caption{Pipeline RAG : phases de retrieval, construction du contexte et génération}
    \label{fig:rag-pipeline}
\end{figure}

\subsubsection{Persistance : base relationnelle, stockage objet et base vectorielle}

La nature hétérogène des données manipulées par OCULA impose une stratégie de persistance adaptée à chaque type d’information. 
Le système distingue ainsi trois couches de stockage complémentaires : relationnelle, objet et vectorielle.

\paragraph{Base relationnelle (PostgreSQL)}

Les entités métier structurées (\textit{User}, \textit{Video}, \textit{Message}, \textit{VideoTranscript}) sont stockées dans une base relationnelle.

Ce choix permet :

\begin{itemize}
    \item de garantir l’intégrité référentielle entre les entités,
    \item d’assurer la cohérence transactionnelle au niveau des données métier,
    \item de faciliter les requêtes applicatives (filtrage, pagination, jointures),
    \item de maintenir une structure normalisée et contrôlée.
\end{itemize}

Il est toutefois important de noter qu’il n’existe pas de transaction globale entre PostgreSQL, le stockage objet et la base vectorielle : l’architecture repose sur une coordination applicative entre services.

\paragraph{Stockage objet (Azure Blob Storage)}

Les fichiers volumineux tels que les vidéos, les miniatures et les avatars sont stockés dans un service de type objet.

La base relationnelle conserve principalement la \textit{clé d’objet} (object key) associée à chaque ressource (\texttt{content}, \texttt{thumbnail}, \texttt{avatar}). 
Une URL signée est générée dynamiquement lors de l’accès au fichier.

Ce choix permet :

\begin{itemize}
    \item d’optimiser le stockage des contenus binaires,
    \item de réduire la charge sur la base relationnelle,
    \item d’assurer une meilleure scalabilité des médias,
    \item de séparer clairement données structurées et fichiers lourds.
\end{itemize}

\paragraph{Base vectorielle (ChromaDB)}

Les embeddings générés à partir des chunks sont stockés dans une base vectorielle dédiée (ChromaDB).

Chaque vidéo dispose d’une collection nommée \texttt{video\_<video\_id>}. 

Cette organisation assure :

\begin{itemize}
    \item l’isolement des recherches par vidéo,
    \item la traçabilité temporelle des passages récupérés,
    \item une recherche efficace par mesure de similarité ou distance vectorielle.
\end{itemize}


\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/hybrid-architecture.png}
\caption{Architecture de persistance hybride d’OCULA : base relationnelle, stockage objet et base vectorielle}
\label{fig:persistence-hybrid}
\end{figure}
 
\medskip

Le modèle relationnel assure la cohérence des données métier, le stockage objet prend en charge les contenus binaires via des clés d’objet, et la base vectorielle optimise la récupération sémantique pour le RAG. 
Ces trois couches sont complémentaires et interconnectées principalement par \texttt{video\_id} (et \texttt{user\_id} pour les ressources utilisateur). 

\section{Implémentation}
\subsection{Frontend : application Angular}
\subsubsection{Principales pages et navigation}
\paragraph{5.1.1.1 Pages et routes}
Routes principales (source : \texttt{app.routes.ts}).
Le tableau~\ref{tab:frontend-routes} présente une vue synthétique des chemins disponibles, des composants associés et du niveau de protection appliqué.
\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.28}
\newcommand{\badgepub}{\colorbox{green!15}{\strut\texttt{publique}}}
\newcommand{\badgeguard}[1]{\colorbox{blue!10}{\strut\texttt{#1}}}
\begin{tabular}{|>{\raggedright\arraybackslash}p{1.9cm}|>{\raggedright\arraybackslash}p{2.7cm}|>{\raggedright\arraybackslash}p{2.3cm}|>{\raggedright\arraybackslash}p{7.3cm}|}
\hline
\rowcolor{techheader}
\textcolor{white}{\textbf{Route}} & \textcolor{white}{\textbf{Page}} & \textcolor{white}{\textbf{Protection}} & \textcolor{white}{\textbf{R\^ole principal}} \\
\hline
\rowcolor{techlinea}
\cellcolor{techroute}\texttt{/} & Page d'accueil & \badgepub & Page d'accueil et point d'entr\'ee vers l'authentification. \\
\hline
\rowcolor{techlineb}
\cellcolor{techroute}\texttt{/auth} & Page d'authentification & \badgeguard{LoginGuard} & Connexion/inscription dans une seule vue. \\
\hline
\rowcolor{techlinea}
\cellcolor{techroute}\texttt{/upload} & Page d'upload & \badgeguard{AuthGuard} & Page pour envoyer une vid\'eo \`a la plateforme. \\
\hline
\rowcolor{techlineb}
\cellcolor{techroute}\texttt{/myvideos} & Page My Videos & \badgeguard{AuthGuard} & Liste des vid\'eos de l'utilisateur. \\
\hline
\rowcolor{techlinea}
\cellcolor{techroute}\texttt{/video/:id} & Page vid\'eo & \badgeguard{AuthGuard} & Lecture vid\'eo, affichage des m\'etadonn\'ees (titre/r\'esum\'e) et suivi du statut de traitement en temps r\'eel. \\
\hline
\rowcolor{techlineb}
\cellcolor{techroute}\texttt{/settings} & Page param\`etres & \badgeguard{AuthGuard} & Mise \`a jour du profil (nom/pr\'enom/avatar), de l'email et du mot de passe. \\
\hline
\end{tabular}
\caption{Routes frontend, pages associ\'ees et responsabilit\'es}
\label{tab:frontend-routes}
\end{table}

\paragraph{5.1.1.2 La navigation métier et les redirections automatiques}

Pour compléter cette vue statique, la figure~\ref{fig:frontend-nav} illustre la navigation réelle entre les pages, y compris les redirections liées aux guards et les transitions métier.
\begin{figure}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[
    page/.style={rectangle, rounded corners=6pt, draw=#1!70, fill=#1!10,
                 text width=3.7cm, align=center, font=\scriptsize\bfseries,
                 minimum height=1.0cm, inner sep=3pt},
    guard/.style={rectangle, rounded corners=3pt, draw=orange!70, fill=orange!15,
                 text width=2.5cm, align=center, font=\scriptsize, minimum height=0.6cm, inner sep=2.5pt},
    arrow/.style={-{Stealth[length=4pt]}, thick, #1},
    dashed arrow/.style={-{Stealth[length=4pt]}, dashed, thick, #1},
    label/.style={font=\scriptsize\itshape, text=gray, fill=white, inner sep=1.5pt, align=center}
]

%% NOEUDS PAGES (espacement augmenté)
\node[page=gray]   (home)     at (0,0)        {/ \\ Page d'accueil};
\node[page=gray]   (auth)     at (6.4,0)      {/auth \\ Page d'authentification};

\node[page=green]  (myvideos) at (0,-3.6)     {/myvideos \\ Page My Videos};
\node[page=green]  (upload)   at (6.4,-3.6)   {/upload \\ Page d'upload};

\node[page=green]  (video)    at (0,-7.2)     {/video/:id \\ Page vidéo};
\node[page=green]  (settings) at (6.4,-7.2)   {/settings \\ Page paramètres};

%% GUARDS
\node[guard]  (ag1) at (0,-2.1)   {AuthGuard};
\node[guard]  (ag2) at (6.4,-2.1) {LoginGuard};
\node[guard]  (ag3) at (0,-5.7)   {AuthGuard};
\node[guard]  (ag4) at (6.4,-5.7) {AuthGuard};

%% NAVIGATION DIRECTE
\draw[arrow=black] (home) -- node[label, above]{Sign in / Sign up} (auth);
\draw[arrow=black] (home) -- (ag1);
\draw[arrow=black] (ag1) -- (myvideos);
\draw[arrow=black] (auth) -- (ag2);
\draw[arrow=black] (ag2) -- (upload);
\draw[arrow=black] (myvideos) -- (ag3);
\draw[arrow=black] (ag3) -- (video);
\draw[arrow=black] (upload) -- (ag4);
\draw[arrow=black] (ag4) -- (settings);

%% TRANSITIONS METIER (labels déplacés)
\draw[dashed arrow=violet, bend right=22]
    (upload.south west) to node[label, pos=0.55, below]{après upload} (video.east);

\draw[dashed arrow=teal, bend right=24]
    (auth.south west) to node[label, pos=0.45, left]{après login} (myvideos.north east);

\draw[dashed arrow=gray, bend left=10]
    (myvideos.south) to node[label, pos=0.55, right]{clic vidéo} (video.north);

\draw[dashed arrow=red, bend left=30]
    (ag2.west) to node[label, pos=0.58, below]{déjà connecté\\$\rightarrow$ /myvideos} (myvideos.east);

%% LEGENDE (décalée à droite)
\node[font=\scriptsize\bfseries, draw=gray!40, rounded corners=3pt, fill=gray!5,
      text width=2.8cm, align=center] at (11.5,-0.3) {Légende};

\node[page=green, minimum height=0.55cm, text width=2.4cm, font=\scriptsize\bfseries]
    at (11.5,-1.5) {Page protégée};

\node[page=gray, minimum height=0.55cm, text width=2.4cm, font=\scriptsize\bfseries]
    at (11.5,-2.6) {Page publique};

\node[guard, minimum height=0.55cm, text width=2.4cm]
    at (11.5,-3.7) {Guard};

\draw[dashed arrow=violet] (10.7,-4.8) -- (11.8,-4.8) node[right, font=\scriptsize]{transition métier};
\draw[dashed arrow=red]    (10.7,-5.8) -- (11.8,-5.8) node[right, font=\scriptsize]{redirection guard};
\draw[arrow=black]         (10.7,-6.8) -- (11.8,-6.8) node[right, font=\scriptsize]{navigation directe};

\end{tikzpicture}
}%
\caption{Parcours principal des pages du frontend}
\label{fig:frontend-nav}
\end{figure}

\noindent \textit{Remarque :} ce schéma montre le chemin principal. L'utilisateur peut aussi changer de page via le menu de son avatar.




\subsubsection{Échanges avec le backend}

\paragraph{5.1.2.1 Rôle des services\\[0pt]} 
\begin{itemize}
    \item \textbf{AuthService        :} gère la connexion et l'inscription. Il garde les informations de session côté navigateur et permet de savoir si l'utilisateur est encore connecté.
    \item \textbf{VideoService       :} s'occupe des actions liées aux vidéos : envoyer une vidéo, afficher une vidéo, charger la liste et lancer le traitement.
    \item \textbf{BlobStorageService :} récupère les liens nécessaires pour afficher les fichiers (vidéo, miniature, avatar).
    \item \textbf{UserService        :} met à jour les informations du profil utilisateur.
    \item \textbf{WsService          :} permet de voir l'avancement du traitement d'une vidéo en direct, sans recharger la page.
\end{itemize}

\paragraph{5.1.2.2 Séquence de traitement d'une vidéo : upload, déclenchement IA et suivi temps réel}
Le schéma suivant synthétise le scénario métier principal après l'upload d'une vidéo, depuis l'appel REST initial jusqu'aux mises à jour de statut en temps réel.

\begin{figure}[H]
\centering
\resizebox{0.94\textwidth}{!}{%
\begin{tikzpicture}[
    actor/.style={rectangle, draw=gray!60, rounded corners=3pt, fill=gray!10, minimum width=2.7cm, minimum height=0.8cm, align=center, font=\scriptsize\bfseries},
    lifeline/.style={gray!60, dashed},
    call/.style={-{Stealth[length=4pt]}, thick, blue!70!black},
    async/.style={-{Stealth[length=4pt]}, thick, dashed, violet!80!black},
    note/.style={rectangle, draw=gray!40, fill=gray!7, rounded corners=2pt, font=\scriptsize, align=left}
]

\node[actor] (u) at (0,0) {UploadPage};
\node[actor] (v) at (4.2,0) {VideoService};
\node[actor] (b) at (8.4,0) {Backend API};
\node[actor] (w) at (12.6,0) {WsService};

\draw[lifeline] (u) -- (0,-6.1);
\draw[lifeline] (v) -- (4.2,-6.1);
\draw[lifeline] (b) -- (8.4,-6.1);
\draw[lifeline] (w) -- (12.6,-6.1);

\draw[call] (0,-0.9) -- node[above, font=\tiny]{1) upload + metadata + thumbnail} (4.2,-0.9);
\draw[call] (4.2,-1.6) -- node[above, font=\tiny]{2) POST /api/videos/upload} (8.4,-1.6);
\draw[call] (8.4,-2.3) -- node[above, font=\tiny]{3) id vidéo} (4.2,-2.3);
\draw[call] (4.2,-3.0) -- node[above, font=\tiny]{4) POST /api/videos/process/\{id\}} (8.4,-3.0);
\draw[async] (8.4,-4.0) -- node[above, font=\tiny]{5) statut PENDING/PROCESSING/COMPLETED} (12.6,-4.0);
\draw[call] (12.6,-4.8) -- node[above, font=\tiny]{6) refresh GET /api/videos/\{id\}} (8.4,-4.8);

\node[note] at (6.3,-5.7) {Mise à jour UI sans polling\\grâce au topic \texttt{/topic/videos/\{id\}}.};

\end{tikzpicture}
}%
\caption{Séquence de traitement d'une vidéo : upload, déclenchement IA et suivi temps réel}
\label{fig:frontend-services}
\end{figure}
\subsubsection{Aperçu des pages principales}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/upload.png}
    \caption{Page d'upload}
    \label{fig:frontend-upload-page}
\end{figure}
\noindent Cette page permet de téléverser une vidéo puis de lancer son traitement.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/myvideos.png}
    \caption{Page \textit{My Videos}}
    \label{fig:frontend-myvideos-page}
\end{figure}
\noindent Cette page affiche les vidéos dans une grille simple. Le menu au dessus de l'avatar, visible à droite en haut, permet aussi d'aller vers le reste des pages.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/video.png}
    \caption{Page vidéo}
    \label{fig:frontend-video-page}
\end{figure}
\noindent Cette page affiche la vidéo traitée, accompagnée d’un titre et d’un résumé. Elle propose également un espace d’échange avec l’IA pour discuter du contenu de la vidéo et poser des questions complémentaires.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/settings.png}
    \caption{Page \textit{Settings}}
    \label{fig:frontend-settings-page}
\end{figure}
\noindent Cette page centralise les actions de gestion du compte utilisateur, notamment la mise à jour du profil, de l’adresse e-mail et du mot de passe.

\subsection{Backend : API Spring Boot}
\subsubsection{Architecture (controllers, services, repositories)}
\paragraph{5.2.1.1 Organisation en couches}
Le backend Spring Boot suit une architecture en couches qui sépare la responsabilité HTTP, la logique métier et la persistance :
\begin{itemize}
    \item \textbf{Couche controllers :} expose les endpoints REST (\texttt{/auth}, \texttt{/users}, \texttt{/videos}, \texttt{/messages}, \texttt{/video-transcripts}, \texttt{/files}).
    \item \textbf{Couche services :} applique les règles métier (contrôle d'existence, unicité email, validation des timestamps de transcript, orchestration du traitement IA, notifications WebSocket).
    \item \textbf{Couche repositories :} s'appuie sur Spring Data JPA pour accéder à PostgreSQL via des repositories typés (\texttt{JpaRepository}).
    \item \textbf{Couche mapping :} les mappers MapStruct isolent les conversions Entity $\leftrightarrow$ DTO.
\end{itemize}

\paragraph{5.2.1.2 Vue d'ensemble des interactions}
\begin{figure}[H]
\centering
\includegraphics[width=0.98\linewidth]{images/Architecture technique du backend.png}
\caption{Architecture technique du backend OCULA}
\label{fig:backend-architecture}
\end{figure}

\noindent Cette séparation permet de faire évoluer chaque couche de façon indépendante. Les changements d'API restent dans les controllers, les règles métier restent dans les services, et la couche d'accès aux données reste concentrée dans les repositories.

\subsubsection{Gestion des utilisateurs et des vidéos}
\paragraph{5.2.2.1 Gestion des utilisateurs}
La gestion des utilisateurs s'appuie sur deux chemins complémentaires :
\begin{itemize}
    \item \textbf{Authentification} : \texttt{POST /auth/signup} crée un compte avec mot de passe hashé (\texttt{BCrypt}), et \texttt{POST /auth/login} retourne un JWT (\texttt{token} + \texttt{expiresIn} + informations utilisateur).
    \item \textbf{Profil} : \texttt{PUT /users/update/\{id\}} accepte un payload \texttt{multipart/form-data} composé d'un avatar optionnel et d'un objet JSON \texttt{user}. Si un nouvel avatar est envoyé, l'ancien fichier est supprimé du Blob Storage avant mise à jour.
\end{itemize}

\paragraph{5.2.2.2 Gestion des vidéos et cycle de traitement}
Le cycle principal de vie d'une vidéo est orchestré par \texttt{VideoController} :
\begin{enumerate}
    \item \textbf{Upload} (\texttt{POST /videos/upload}) : réception de la vidéo, de la miniature et des métadonnées, génération de clés Blob (\texttt{userId/videos/...}, \texttt{userId/thumbnails/...}), envoi des fichiers vers Azure Blob, puis création de la ligne \texttt{Video} en base avec statut initial \texttt{PENDING}.
    \item \textbf{Déclenchement IA} (\texttt{POST /videos/process/\{id\}}) : délégation au service Flask via \texttt{AiService}.
    \item \textbf{Callbacks techniques} : le service IA met à jour l'état via \texttt{PUT /videos/update/status/\{id\}} (PROCESSING/COMPLETED/FAILED), injecte les transcripts via \\ \texttt{POST /video-transcripts}, puis met à jour titre et résumé via \texttt{PUT /videos/update/\{id\}}.
    \item \textbf{Retour temps réel} : chaque changement de statut déclenche \texttt{WebsocketNotifier} qui publie sur \texttt{/topic/videos/\{id\}}.
\end{enumerate}

\noindent La pagination est gérée côté backend sur les listes métier (\texttt{/videos/page/\{userId\}}, \texttt{/messages/page/\{videoId\}}), ce qui limite la charge transmise au frontend.

\subsubsection{Endpoints REST : conception et validation}
\paragraph{5.2.3.1 Endpoints clefs}
Le tableau~\ref{tab:backend-endpoints} résume les routes principales et leur rôle.
\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.25}
\newcommand{\backendpub}{\colorbox{green!15}{\strut\scriptsize\texttt{publique}}}
\newcommand{\backendjwt}{\colorbox{blue!10}{\strut\scriptsize\texttt{JWT}}}
\newcommand{\backendint}{\colorbox{orange!18}{\strut\scriptsize\texttt{interne}}}
\begin{tabular}{|>{\raggedright\arraybackslash}p{5cm}|>{\centering\arraybackslash}p{1.6cm}|>{\raggedright\arraybackslash}p{4.5cm}|>{\raggedright\arraybackslash}p{5.5cm}|}
\hline
\rowcolor{techheader}
\textcolor{white}{\textbf{Endpoint}} & \textcolor{white}{\textbf{Acc\`es}} & \textcolor{white}{\textbf{Validation / r\`egles}} & \textcolor{white}{\textbf{R\^ole / r\'eponse}} \\
\hline
\rowcolor{techlinea}
\texttt{POST /auth/signup} & \backendpub & Email requis, email non déjà utilisé. & Crée un utilisateur, renvoie l'entité persistée. \\
\hline
\rowcolor{techlineb}
\texttt{POST /auth/login} & \backendpub & Email requis, mot de passe requis, identifiants valides. & Renvoie \texttt{token}, \texttt{expiresIn}, \texttt{userInfo}. \\
\hline
\rowcolor{techlinea}
\texttt{POST /videos/upload} & \backendjwt & \texttt{userId} valide, fichier vidéo présent, miniature présente. & Crée la vidéo (statut \texttt{PENDING}) et retourne \texttt{VideoDto}. \\
\hline
\rowcolor{techlineb}
\texttt{POST /videos/process/\{id\}} & \backendjwt / \backendint & Id vidéo existant. & Déclenche \texttt{POST /process} côté service IA. \\
\hline
\rowcolor{techlinea}
\texttt{PUT /videos/update/status/\{id\}} & \backendjwt / \backendint & Id vidéo existant, statut autorisé dans \texttt{VideoStatus}. & Met à jour le statut et notifie WebSocket. \\
\hline
\rowcolor{techlineb}
\texttt{PUT /users/update/\{id\}} & \backendjwt & Id utilisateur existant, format \texttt{multipart/form-data} valide. & Met à jour profil/email/password/avatar. \\
\hline
\rowcolor{techlinea}
\texttt{GET /videos/page/\{userId\}} & \backendjwt & \texttt{userId} valide, paramètres de pagination cohérents. & Retourne un \texttt{Page<VideoDto>}. \\
\hline
\rowcolor{techlineb}
\texttt{POST /video-transcripts} & \backendjwt / \backendint & \texttt{videoId} valide, temps requis, temps positifs, \texttt{endTime >= startTime}. & Persiste un segment de transcript. \\
\hline
\rowcolor{techlinea}
\texttt{GET /messages/page/\{videoId\}} & \backendjwt & \texttt{videoId} valide, pagination cohérente. & Historique chat paginé pour la vidéo. \\
\hline
\texttt{GET /files/get-url?key=...} & \backendjwt & Clé Blob fournie, objet Blob accessible. & Renvoie une URL signée de lecture Blob. \\
\hline
\end{tabular}
\caption{Synthèse des endpoints backend principaux}
\label{tab:backend-endpoints}
\end{table}

\paragraph{5.2.3.2 Stratégie de validation et gestion d'erreurs}
La validation combine des contrôles techniques et métier :
\begin{itemize}
    \item contrôles d'existence (utilisateur, vidéo, message, transcript) avant mutation
    \item contraintes métier explicites (ex. timestamps de transcript, unicité email) dans les services
    \item exception handling centralisé via \texttt{@ControllerAdvice} avec mapping HTTP : \texttt{400} (transcript invalide), \texttt{401} (authentification), \texttt{404} (ressource absente), \texttt{409} (email déjà utilisé)
\end{itemize}

\subsubsection{Sécurité : authentification et autorisation}

La sécurité est définie par \texttt{SecurityFilterChain} en mode \textit{stateless} :
\begin{itemize}
    \item \texttt{/auth/**} et \texttt{/**} sont publics
    \item toutes les autres routes exigent une authentification
    \item deux mecanismes sont supportes : JWT utilisateur (\texttt{Authorization: Bearer ...}) et cle interne machine-a-machine (\texttt{X-API-KEY}) pour les callbacks du service IA.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/Chaîne de sécurité backen.png}
\caption{Chaîne de sécurité backend (JWT + clé interne)}
\label{fig:backend-security-chain}
\end{figure}

\subsection{Service IA : LangChain + Flask}
\noindent \textit{À compléter.}

\subsubsection{Extraction audio et transcription (Whisper)}
\noindent \textit{À compléter.}

\subsubsection{Chunking : stratégies et paramètres}
\noindent \textit{À compléter.}

\subsubsection{Génération d'embeddings (MiniLM)}
\noindent \textit{À compléter.}

\subsubsection{RAG : retrieval et construction du prompt}
\noindent \textit{À compléter.}

\subsubsection{Appel au LLM externe et post-traitements}
\noindent \textit{À compléter.}

\subsection{Stockage et (infrastructure)}
\noindent \textit{À compléter.}

\subsubsection{Stockage vidéo et fichiers (Azure Blob)}
\noindent \textit{À compléter.}

\subsubsection{Base de données : choix et schéma}
\noindent \textit{À compléter.}

\subsubsection{Dockerisation}

\section{Évaluation et résultats}
\noindent \textit{À compléter.}

\subsection{Protocole de test}
\noindent \textit{À compléter.}

\subsubsection{Jeu de vidéos utilisé}
\noindent \textit{À compléter.}

\subsubsection{Scénarios de test (upload, transcription, Q\&A)}
\noindent \textit{À compléter.}

\subsubsection{Critères : qualité, latence, stabilité}
\noindent \textit{À compléter.}

\subsection{Analyse qualitative}
\noindent \textit{À compléter.}

\subsubsection{Pertinence des résumés et titres}
\noindent \textit{À compléter.}

\subsubsection{Qualité des réponses du chatbot}
\noindent \textit{À compléter.}

\subsubsection{Comportement sur contenu bruité / long / multilingue}
\noindent \textit{À compléter.}

\subsection{Analyse quantitative}
\noindent \textit{À compléter.}

\subsubsection{Temps de traitement par étape (pipeline)}
\noindent \textit{À compléter.}

\subsubsection{Temps de réponse du chatbot}
\noindent \textit{À compléter.}

\subsubsection{Impact du chunking et du top-k retrieval}
\noindent \textit{À compléter.}

\subsection{Discussion}
\noindent \textit{À compléter.}

\subsubsection{Limites observées}
\noindent \textit{À compléter.}

\subsubsection{Risques et biais potentiels}
\noindent \textit{À compléter.}

\subsubsection{Comparaison : RAG vs LLM seul}
\noindent \textit{À compléter.}

\section{Sécurité, conformité et considérations éthiques}
\noindent \textit{À compléter.}

\subsection{Confidentialité des données et stockage}
\noindent \textit{À compléter.}

\subsection{Gestion des accès et des permissions}
\noindent \textit{À compléter.}

\subsection{Propriété intellectuelle et droits sur les contenus vidéo}
\noindent \textit{À compléter.}

\subsection{Limites et usages responsables}
\noindent \textit{À compléter.}

\section{Conclusion et perspectives}
\noindent \textit{À compléter.}

\subsection{Bilan du projet}
\noindent \textit{À compléter.}

\subsection{Améliorations techniques possibles}
\noindent \textit{À compléter.}

\subsubsection{Optimisation de la recherche semantique}
\noindent \textit{À compléter.}

\subsubsection{Streaming transcription}
\noindent \textit{À compléter.}

\subsubsection{Amélioration de la segmentation (chunking adaptatif)}
\noindent \textit{À compléter.}

\subsubsection{Analyse de video frame par frame}
\noindent \textit{À compléter.}

\subsubsection{Infrastructure}
\noindent \textit{À compléter.}

\subsection{Ouverture produit : industrialisation}
\noindent \textit{À compléter.}



% \section{État de l'art}

% Cette section présente les principales applications et plateformes existantes proposant des fonctionnalités proches de celles du projet OCULA. Pour chaque concurrent, sont analysés les services proposés, le modèle économique ainsi que les principales limitations.

% \subsection{Analyse des solutions existantes}

% \subsubsection{Otter.ai}

% \textbf{Description} \\
% Otter.ai est une application spécialisée dans la transcription automatique de contenus audio et vidéo, principalement orientée vers les réunions professionnelles. Elle propose également des résumés automatiques et une recherche textuelle dans les transcriptions.

% \textbf{Fonctionnalités principales}
% \begin{itemize}
%     \item Transcription automatique en temps réel ou différé
%     \item Résumé automatique de réunions
%     \item Recherche par mots-clés dans les transcriptions
% \end{itemize}

% \textbf{Modèle économique}  
% Solution freemium avec des fonctionnalités avancées accessibles uniquement via abonnement payant.

% \textbf{Limites}
% \begin{itemize}
%     \item Orienté réunions, peu adapté aux vidéos longues génériques
%     \item Absence de chatbot conversationnel basé sur RAG
%     \item Pipeline de traitement fermé et non personnalisable
% \end{itemize}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{image.png}
%     \caption{Page tarification de Otter.ai}
%     \label{fig:otter}
% \end{figure}
% ---------------------------------------

% \subsubsection{Fireflies.ai}

% \textbf{Description} \\  
% Fireflies.ai est un outil d’analyse automatique de réunions intégrant transcription, résumé et recherche. Il s’intègre directement à des plateformes de visioconférence.

% \textbf{Fonctionnalités principales}
% \begin{itemize}
%     \item Transcription automatique de réunions
%     \item Résumés et points clés
%     \item Recherche dans les conversations
% \end{itemize}

% \textbf{Modèle économique}  
% Service payant avec abonnement mensuel.

% \textbf{Limites}
% \begin{itemize}
%     \item Fortement dépendant des outils de visioconférence
%     \item Pas conçu pour l’analyse libre de fichiers vidéo
%     \item Absence d’interrogation conversationnelle avancée
% \end{itemize}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{image2.png}
%     \caption{Service et tarification Fireflies.ai}
%     \label{fig:placeholder}
% \end{figure}

% % --------------------------------------------------

% \subsubsection{Google (YouTube / Vertex AI)}

% \textbf{Description}  
% Google propose plusieurs solutions permettant l’analyse de contenus vidéo. YouTube intègre la génération automatique de sous-titres, tandis que Vertex AI et Speech-to-Text permettent la transcription et l’analyse via API.

% \textbf{Fonctionnalités principales}
% \begin{itemize}
%     \item Transcription automatique de vidéos
%     \item Recherche textuelle basique
%     \item APIs d’analyse audio et vidéo
% \end{itemize}

% \textbf{Modèle économique}  
% YouTube est gratuit pour l’utilisateur final, tandis que les APIs Google Cloud sont facturées à l’usage.

% \textbf{Limites}
% \begin{itemize}
%     \item Pas de chatbot conversationnel sur le contenu vidéo
%     \item Solutions fragmentées (plusieurs services à combiner)
%     \item Mise en place technique complexe
% \end{itemize}


% % --------------------------------------------------

% \subsubsection{OpenAI (ChatGPT / Whisper API)}

% \textbf{Description}  
% OpenAI propose des modèles de langage et de transcription permettant l’analyse de contenus textuels et audio. ChatGPT permet l’interrogation conversationnelle de documents, tandis que Whisper assure une transcription automatique de haute qualité.

% \textbf{Fonctionnalités principales}
% \begin{itemize}
%     \item Transcription audio multilingue
%     \item Génération de résumés
%     \item Question-réponse conversationnelle
% \end{itemize}

% \textbf{Modèle économique}  
% APIs payantes facturées à l’usage, avec accès gratuit limité via l’interface ChatGPT.

% \textbf{Limites}
% \begin{itemize}
%     \item Pas de gestion native de vidéos complètes
%     \item Absence de pipeline clé-en-main pour l’analyse vidéo
%     \item Pas d’indexation temporelle automatique
% \end{itemize}


% \begin{table}[H]
% \centering
% \renewcommand{\arraystretch}{1.3}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Solution} & \textbf{Vidéo} & \textbf{Résumé} & \textbf{Chat RAG} & \textbf{Pipeline ouvert} \\
% \hline
% Otter.ai & \checkmark & \checkmark & \texttimes & \texttimes \\
% Fireflies.ai & \checkmark & \checkmark & \texttimes & \texttimes \\
% Google (YouTube) & \checkmark & \texttimes & \texttimes & \texttimes \\
% OpenAI (API) & \texttimes & \checkmark & \checkmark & \texttimes \\
% \textbf{OCULA} & \checkmark & \checkmark & \checkmark & \checkmark \\
% \hline
% \end{tabular}
% \caption{Comparaison fonctionnelle entre OCULA et les solutions existantes}
% \label{tab:comparison}
% \end{table}

% % ==================================================
% \section{Différenciation du projet OCULA}

% Bien que plusieurs applications et plateformes proposent des fonctionnalités proches de celles du projet OCULA, aucune ne couvre l’ensemble du périmètre fonctionnel visé. OCULA se distingue des solutions existantes à plusieurs niveaux, tant sur le plan fonctionnel que technique.

% \subsection{Différence de périmètre fonctionnel}

% Les applications existantes se concentrent généralement sur une seule tâche spécifique, telle que la transcription automatique de réunions, le résumé de documents textuels ou l’édition audio et vidéo. À l’inverse, OCULA adopte une approche globale de l’analyse de contenu vidéo.

% OCULA permet :
% \begin{itemize}
%     \item l’upload libre de vidéos, indépendamment de leur source,
%     \item la transcription automatique du contenu audio,
%     \item la génération d’un titre et d’un résumé représentatif,
%     \item l’interrogation conversationnelle du contenu via un chatbot,
%     \item la recherche d’information contextualisée dans l’ensemble de la vidéo.
% \end{itemize}

% Cette combinaison de fonctionnalités, appliquée spécifiquement au contenu vidéo, constitue une première différence majeure avec les solutions existantes.

% \subsection{Différence dans l’approche technique}

% Contrairement aux applications commerciales clés-en-main, OCULA repose sur une architecture modulaire et transparente. Le pipeline de traitement est entièrement contrôlé et comprend des étapes explicites de transcription, de segmentation, d’indexation sémantique et de génération de réponses.

% L’utilisation de la \textit{Retrieval-Augmented Generation} (RAG) permet d’ancrer les réponses du chatbot dans le contenu réel de la vidéo, réduisant ainsi les hallucinations des modèles de langage. Cette approche est rarement proposée de manière explicite dans les applications existantes, qui reposent souvent sur des traitements opaques.

% \subsection{Différence par rapport aux APIs et frameworks}

% Des plateformes comme OpenAI ou Google proposent des APIs puissantes pour la transcription, la génération de texte ou l’analyse sémantique. Toutefois, ces solutions fournissent uniquement des briques technologiques et non une application utilisateur complète.

% OCULA se distingue en intégrant ces briques au sein d’une application cohérente, orientée utilisateur final, tout en gérant des problématiques concrètes telles que :
% \begin{itemize}
%     \item le stockage de données volumineuses,
%     \item le traitement asynchrone de tâches longues,
%     \item la persistance des résultats,
%     \item l’interaction continue entre l’utilisateur et le contenu analysé.
% \end{itemize}

% \subsection{Différence en termes de finalité}

% Enfin, OCULA se positionne comme un projet à la fois expérimental et pédagogique. L’objectif n’est pas de concurrencer directement des solutions commerciales existantes, mais de démontrer la faisabilité et l’intérêt d’une architecture complète d’analyse et d’interrogation de contenus vidéo basée sur les technologies actuelles de l’intelligence artificielle.

% Ainsi, OCULA se distingue par son caractère intégratif, sa flexibilité et sa transparence, là où les solutions existantes restent soit partielles, soit fermées, soit orientées vers des usages spécifiques.

% % --------------------------------------------------
% % ==================================================
% \section{Justification du choix des modèles}

% Dans le cadre du projet OCULA, le choix des modèles de traitement automatique du langage et de la parole a été guidé par plusieurs critères : qualité des résultats, coût d’utilisation, facilité d’intégration, contrôle du pipeline et adéquation avec un projet académique. Cette section détaille les modèles retenus ainsi que les raisons pour lesquelles certaines alternatives n’ont pas été privilégiées.

% \subsection{Modèle d'embeddings : \texttt{all-MiniLM-L6-v2}}

% Le modèle \texttt{sentence-transformers/all-MiniLM-L6-v2} est utilisé pour transformer des phrases en représentations vectorielles numériques. Ces vecteurs sont ensuite exploités pour la recherche sémantique dans le cadre de la méthode \textit{Retrieval-Augmented Generation} (RAG).

% \textbf{Justification du choix}
% \begin{itemize}
%     \item Modèle open-source, utilisable localement sans coût d’API.
%     \item Temps d’inférence réduit, adapté à une exécution sur CPU.
%     \item Bon compromis entre performance sémantique et légèreté.
%     \item Large adoption dans les systèmes RAG académiques et industriels.
% \end{itemize}

% \textbf{Alternatives considérées et limites}
% \begin{itemize}
%     \item Modèles plus volumineux (ex. MPNet, BERT large) : performances légèrement supérieures mais coûts computationnels plus élevés.
%     \item Embeddings via API (OpenAI, Cohere) : qualité élevée mais dépendance à un service externe payant.
% \end{itemize}

% Ainsi, \texttt{all-MiniLM-L6-v2} représente un compromis optimal entre coût, performance et simplicité d’intégration.

% % --------------------------------------------------

% \subsection{Modèle de transcription : Whisper}

% Le modèle Whisper est utilisé pour la transcription automatique des pistes audio extraites des vidéos.

% \textbf{Justification du choix}
% \begin{itemize}
%     \item Modèle open-source permettant une exécution locale.
%     \item Très bonne qualité de transcription, même en présence de bruit.
%     \item Support multilingue natif.
%     \item Large reconnaissance académique et industrielle.
% \end{itemize}

% \textbf{Alternatives considérées et limites}
% \begin{itemize}
%     \item APIs cloud (Google Speech-to-Text, Azure Speech) : solutions performantes mais facturées à l’usage.
%     \item Modèles plus légers (Vosk) : coût nul mais qualité inférieure.
% \end{itemize}

% L’utilisation de Whisper en local permet de réduire les coûts tout en conservant une qualité de transcription élevée, ce qui est particulièrement adapté à un projet académique manipulant des données volumineuses.

% % --------------------------------------------------

% \subsection{Modèle de langage : \texttt{gpt-oss-120b}}

% Le modèle \texttt{gpt-oss-120b}, accessible via une API externe fournie par Cerebras, est utilisé pour la compréhension et la génération de texte, notamment pour la génération de résumés et l’interrogation conversationnelle.

% \textbf{Justification du choix}
% \begin{itemize}
%     \item Modèle open-source, garantissant une transparence du fonctionnement.
%     \item Capacités avancées de raisonnement et de génération de texte.
%     \item Accès via API évitant la gestion d’une infrastructure lourde.
%     \item Coût inférieur aux grands modèles propriétaires à performances comparables.
% \end{itemize}

% \textbf{Alternatives considérées et limites}
% \begin{itemize}
%     \item Modèles propriétaires (OpenAI, Google) : très bonnes performances mais coûts élevés et dépendance à des services fermés.
%     \item Modèles open-source auto-hébergés (LLaMA, Mistral) : contrôle total mais besoins matériels importants (GPU).
% \end{itemize}

% Le choix de \texttt{gpt-oss-120b} permet ainsi de bénéficier de la puissance d’un grand modèle de langage tout en conservant une approche ouverte et économiquement raisonnable.



% % --------------------------------------------------

% \subsection{Synthèse des choix}

% La combinaison de modèles locaux pour les tâches intensives en données (embeddings, transcription) et d’un modèle de langage accessible via API pour les tâches de génération permet d’optimiser les coûts tout en garantissant de bonnes performances. Cette approche hybride offre un équilibre pertinent entre contrôle, scalabilité et qualité, en adéquation avec les objectifs du projet OCULA.




\end{document}
