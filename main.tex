\documentclass{rapportECL}
\usepackage{listings}
\usepackage{lipsum}
\usepackage{gensymb}
\usepackage{float}
\usepackage{xcolor}
\usepackage{csquotes} % Espace fantôme supprimé ici
\usepackage{booktabs}
\usepackage{minted}
\usepackage{caption}
\usepackage{amssymb} 
\usepackage{verbatim}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{placeins}
\usepackage[french]{babel}
\usepackage{url} 
\usepackage{graphicx}
\usepackage{array} 
\usepackage{colortbl}
\usepackage{tikz} 
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, bending}
\usepackage{cite}
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	citecolor=green,
	filecolor=magenta,      
	urlcolor=cyan,
}
\title{ArchiLogiciel } %Titre du fichier
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{codebg}{RGB}{248,250,252}
\definecolor{codeframe}{RGB}{203,213,225}
\definecolor{techheader}{HTML}{0F172A}
\definecolor{techlinea}{HTML}{F8FAFF}
\definecolor{techlineb}{HTML}{EEF4FF}
\definecolor{techroute}{HTML}{E0F2FE}

\lstdefinestyle{oculacode}{
    language=Java,
    alsoletter={@},
    backgroundcolor=\color{codebg},
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    keywordstyle=[2]\color{teal!70!black}\bfseries,
    commentstyle=\color{mygreen},
    stringstyle=\color{mymauve},
    emph={uploadFile,createVideoWithUpload,getFileUrl},
    emphstyle=\color{orange!80!black}\bfseries,
    morekeywords=[2]{PostMapping,RequestParam,ResponseEntity,MultipartFile,UUID,Duration,OffsetDateTime,BlobClient,BlobSasPermission,BlobServiceSasSignatureValues},
    frame=single,
    framerule=0.6pt,
    rulecolor=\color{codeframe},
    numbers=left,
    numberstyle=\footnotesize\color{mygray},
    stepnumber=1,
    numbersep=10pt,
    xleftmargin=0pt,
    framexleftmargin=2.2em,
    breaklines=true,
    breakatwhitespace=true,
    showspaces=false,
    showstringspaces=false,
    tabsize=4,
    keepspaces=true,
    columns=fullflexible,
    captionpos=b,
    inputencoding=utf8,
    extendedchars=true,
    literate=%
     {é}{{\'e}}1
     {è}{{\`e}}1
     {à}{{\`a}}1
     {ç}{{\c{c}}}1
     {ê}{{\^e}}1
     {ù}{{\`u}}1
     {ô}{{\^o}}1
     {â}{{\^a}}1
}
\lstset{style=oculacode,
    breaklines=true,
    columns=fullflexible}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\begin{document}

%----------- Informations du rapport ---------

\titre{OCULA}
\UE{\textbf{Projet ZZ2 F2}}
\sujet{Application Web Full-Stack pour l’Analyse Intelligente des Contenus Vidéo}
\enseignant{Loïc \textsc{Yon}}
\eleves{
Mouad \textsc{Ismaili M'hamdi} \\
Brahim \textsc{Id Benouakrim} \\
Achraf \textsc{El Allali}
}

%----------- Initialisation -------------------

\fairemarges
\fairepagedegarde
\clearpage

%----------- Résumé (FR) ----------------------

%----------- Résumé (FR) ----------------------
\section*{Résumé}
\addcontentsline{toc}{section}{Résumé}

OCULA est une application web full-stack permettant de transformer automatiquement une vidéo en une ressource intelligemment exploitable. 
Le système s’appuie sur un frontend Angular et un backend Java Spring Boot, complétés par un service IA en Flask orchestrant un pipeline de traitement incluant transcription automatique, génération de titre et de résumé, indexation sémantique et question–réponse en langage naturel via un mécanisme Retrieval-Augmented Generation (RAG).

Après l'importation d'une vidéo, le contenu est stocké en stockage objet (Azure Blob Storage), puis la vidéo est transcrit et segmenté. 
La transcription est découpé en chunks, vectorisés à l’aide d’un modèle d’embeddings, puis indexés dans une base vectorielle afin de permettre une recherche sémantique par vidéo. 
Lorsqu’un utilisateur pose une question, le service IA récupère les passages pertinents, construit un contexte contraint et interroge un modèle de langage externe pour générer une réponse. 
Les réponses sont accompagnées de références temporelles afin d’assurer traçabilité et explicabilité.

Cette architecture distribuée, intégrant traitement asynchrone et gestion d’état explicite, démontre l’intérêt de combiner ASR, recherche vectorielle et génération augmentée par récupération pour l’exploration intelligente de contenus vidéo.

\bigskip
\noindent\textbf{Mots-clés :}
Application web full-stack,
Angular,
Spring Boot,
Java,
Microservice Flask,
Reconnaissance automatique de la parole (ASR),
Transcription audio–texte,
Embeddings,
Recherche sémantique,
Indexation vectorielle,
Retrieval-Augmented Generation (RAG),
Modèles de langage (LLM),
Explicabilité (citations temporelles),
Traitement asynchrone,
Architecture microservices.
\clearpage

%----------- Abstract (EN) ----------------------
\section*{Abstract}
\addcontentsline{toc}{section}{Abstract}

OCULA is a full-stack web application designed to turn a video into an intelligently exploitable resource.
It relies on an Angular frontend and a Java Spring Boot backend, complemented by a Flask-based AI service that orchestrates a processing pipeline including automatic transcription, title and summary generation, semantic indexing, and natural language question answering through a Retrieval-Augmented Generation (RAG) approach.

After a video is uploaded, the content is stored in object storage (Azure Blob Storage), then the video is transcribed and segmented.
The transcription is splited into coherent text chunks, embedded using a sentence embedding model, and indexed in a vector database to enable per-video semantic retrieval.
When a user asks a question, the AI service retrieves the most relevant passages, builds a constrained context, and queries an external language model to generate an answer.
Responses include temporal references to ensure traceability and explainability.

This distributed architecture, combining asynchronous processing and explicit state management, demonstrates the value of integrating ASR, vector search, and retrieval-augmented generation for intelligent exploration of video content.

\bigskip
\noindent\textbf{Keywords:}
Full-stack web application,
Angular,
Spring Boot,
Java,
Flask microservice,
Automatic Speech Recognition (ASR),
Audio-to-Text Transcription,
Embeddings,
Semantic Search,
Vector Indexing,
Retrieval-Augmented Generation (RAG),
Large Language Models (LLM),
Explainability (temporal citations),
Asynchronous Processing,
Microservices Architecture.
\clearpage

%----------- Table des matières ----------------

\addcontentsline{toc}{section}{Table des matières}
\tableofcontents
\newpage

\addcontentsline{toc}{section}{Table des figures}
\listoffigures
\newpage

\addcontentsline{toc}{section}{Liste des tableaux}
\listoftables
\newpage


%------------ Corps du rapport ----------------
\section{Introduction}
\subsection{Contexte et motivation}
La vidéo est devenue un support dominant pour la diffusion d’information, de formation et de communication. 
Cours enregistrés, conférences, tutoriels, réunions, contenus éducatifs ou techniques : ces ressources sont riches, mais difficiles à exploiter rapidement lorsqu’elles dépassent quelques minutes. 
Contrairement au texte, une vidéo ne se \og parcourt \fg{} pas efficacement : retrouver un passage précis, vérifier une information ou synthétiser l’essentiel demande souvent un visionnage long et répétitif.


\begin{table}[H]
\centering
\begin{tabular}{lll}
\toprule
\textbf{Situation} & \textbf{Limite actuelle} & \textbf{Conséquence} \\
\midrule
Vidéo longue & Navigation linéaire uniquement & Temps de recherche élevé \\
Transcript brut & Texte non structuré & Difficulté d’exploration \\
Recherche par mots-clés & Sens non capturé & Résultats incomplets \\
Absence de synthèse & Pas de résumé automatique & Compréhension globale limitée \\
Réponses non traçables & Pas de références temporelles & Manque d’explicabilité \\
\bottomrule
\end{tabular}
\caption{Limites de l’exploration vidéo traditionnelle}
\label{tab:problem-video}
\end{table}
Cette difficulté est accentuée par deux constats. 
D’une part, les métadonnées associées aux vidéos (titre, description, chapitrage) sont souvent incomplètes ou peu fiables. 
D’autre part, la recherche classique par mots-clés reste limitée : elle dépend fortement des termes exacts utilisés et ne capture pas toujours le sens réel d’une question. 
Ainsi, même lorsque l’information est présente dans la vidéo, il est fréquent de ne pas pouvoir la retrouver rapidement.

Dans ce contexte, les progrès récents en reconnaissance automatique de la parole et en modèles de langage permettent d’envisager une nouvelle approche : transformer une vidéo en une source interrogeable, où l’utilisateur peut poser des questions en langage naturel et obtenir des réponses contextualisées. 
L’objectif n’est pas seulement de générer une réponse, mais de fournir un accès plus direct au contenu, tout en gardant une forme de traçabilité (par exemple via des repères temporels) pour faciliter la vérification.


\subsection{Problématique}

Si la transcription automatique permet aujourd’hui de transformer un contenu audio en texte exploitable, elle ne suffit pas à rendre une vidéo réellement intelligible et interrogeable. 
Un transcript brut représente un volume important de texte non structuré, difficile à synthétiser, à explorer et à exploiter efficacement.

La problématique centrale du projet OCULA peut être formulée ainsi :
\medskip


\noindent\textbf{Comment transformer automatiquement une vidéo longue en une ressource structurée, synthétisée et interrogeable en langage naturel, tout en garantissant des réponses pertinentes et traçables ?}
\medskip

Cette problématique implique plusieurs dimensions complémentaires :

\begin{itemize}
    \item générer automatiquement des métadonnées pertinentes (titre et résumé) à partir du contenu réel de la vidéo,
    \item segmenter et structurer le transcript afin de le rendre exploitable,
    \item indexer le contenu de manière sémantique et non uniquement lexicale,
    \item permettre une interaction en langage naturel via un mécanisme de question–réponse,
    \item garantir l’explicabilité des réponses en fournissant des références temporelles précises.
\end{itemize}

Le défi consiste donc à concevoir un système combinant reconnaissance automatique de la parole, indexation vectorielle et génération augmentée par récupération (RAG), capable d’assurer un équilibre entre synthèse, pertinence, robustesse et explicabilité.
\subsection{Objectifs du projet}

Afin de répondre à la problématique formulée précédemment, le projet OCULA poursuit plusieurs objectifs techniques complémentaires.

\paragraph{1. Automatiser la transformation d’une vidéo en ressource exploitable}

Le système doit être capable de traiter automatiquement une vidéo déposée par un utilisateur, d’en extraire l’audio et de produire une transcription fiable à l’aide d’un modèle de reconnaissance automatique de la parole.

\paragraph{2. Enrichir automatiquement les métadonnées}

À partir de la transcription complète, OCULA doit générer un titre pertinent et un résumé synthétique reflétant fidèlement le contenu réel de la vidéo. 
Ces éléments visent à améliorer la compréhension globale et l’accessibilité du contenu.

\paragraph{3. Structurer et indexer le contenu de manière sémantique}

Le transcript doit être segmenté en unités cohérentes, converties en représentations vectorielles et indexées dans une base de données adaptée à la recherche sémantique. 
L’objectif est de permettre une récupération efficace des passages pertinents en fonction du sens de la question posée.

\paragraph{4. Permettre une interaction en langage naturel}

OCULA doit offrir un mécanisme de question–réponse permettant à l’utilisateur d’interroger le contenu d’une vidéo en langage naturel. 
Les réponses générées doivent être contextualisées et basées uniquement sur les informations présentes dans la vidéo.

\paragraph{5. Garantir la traçabilité et l’explicabilité}

Chaque réponse produite doit être accompagnée de références temporelles précises, permettant à l’utilisateur de vérifier l’origine des informations dans la vidéo. 
L’explicabilité constitue ainsi une exigence centrale du système.

\paragraph{6. Concevoir une architecture robuste et modulaire}

Enfin, le projet vise à mettre en place une architecture distribuée séparant clairement frontend, backend et service IA, tout en assurant la robustesse du traitement asynchrone et la gestion explicite des états du pipeline.

\subsection{Périmètre et hypothèses}

Le projet OCULA s’inscrit dans un cadre fonctionnel clairement défini, avec des choix techniques assumés et des limites explicites.

\paragraph{Périmètre fonctionnel}

OCULA traite des vidéos déposées par des utilisateurs authentifiés et repose principalement sur l’analyse du contenu audio. 
Le système permet :

\begin{itemize}
    \item la transcription automatique de la piste video,
    \item la génération d’un titre et d’un résumé à partir du transcript,
    \item l’indexation sémantique du contenu textuel,
    \item l’interrogation de la vidéo via un mécanisme de question–réponse basé sur le transcript.
\end{itemize}

Le système ne réalise pas d’analyse visuelle avancée (reconnaissance d’objets, analyse d’images frame-by-frame ou OCR), et ne prend pas en compte les éléments purement visuels non exprimés oralement.

\paragraph{Hypothèses techniques}

Le fonctionnement d’OCULA repose sur plusieurs hypothèses :

\begin{itemize}
    \item la qualité de la transcription est suffisante pour permettre une indexation sémantique pertinente ;
    \item les réponses générées doivent exclusivement s’appuyer sur le contenu fourni dans le contexte RAG ;
    \item le modèle de langage externe ne dispose pas d’accès direct aux bases de données ;
    \item la charge utilisateur reste compatible avec une architecture microservices non distribuée à grande échelle.
\end{itemize}

\paragraph{Limites actuelles}

Certaines limites sont identifiées dans le périmètre actuel :

\begin{itemize}
    \item absence de traitement multimodal (audio + image),
    \item absence de fine-tuning spécifique du modèle de langage,
\end{itemize}

Ces éléments constituent des pistes d’évolution futures mais ne font pas partie du périmètre initial du projet.
\subsection{Organisation du rapport}

Le présent rapport est structuré de manière progressive afin de présenter successivement le cadre théorique, les choix de conception, l’implémentation et l’évaluation du système OCULA.

Le chapitre 2 décrit l’organisation du projet et la méthodologie adoptée, en détaillant la répartition des responsabilités, la planification et les choix d’ingénierie collaborative.

Le chapitre 3 présente l’état de l’art et les fondements théoriques mobilisés, notamment en reconnaissance automatique de la parole, en représentations vectorielles et en génération augmentée par récupération (RAG).

Le chapitre 4 expose les spécifications et la conception du système, en mettant en évidence l’architecture globale ainsi que le modèle de données retenu.

Le chapitre 5 détaille l’implémentation technique, incluant le frontend, le backend et le service IA, ainsi que les mécanismes internes du pipeline de traitement.

Le chapitre 6 analyse les résultats obtenus à travers une évaluation qualitative et quantitative du système.

Le chapitre 7 aborde les considérations de sécurité, de confidentialité et les aspects éthiques liés à l’utilisation des modèles d’intelligence artificielle.

Enfin, le chapitre 8 conclut le rapport et présente les perspectives d’évolution du projet.
\section{Organisation et gestion du projet}

\subsection{Répartition des responsabilités et approche collaborative}

L'équipe de développement comprend trois étudiants qui ont collaboré sur tous les domaines d'expertise. Plutôt que d'assigner des rôles spécialisés, nous avons adopté une stratégie d'apprentissage transversal où chacun a contribué à la conception et la réalisation de l'ensemble des composantes.

\paragraph{Principes de collaboration}

Les tâches ont été réparties selon le schéma suivant :

\begin{enumerate}
    \item \textbf{Discussions de conception initiale} : avant chaque fonctionnalité majeure, l'équipe se réunit pour en discuter l'architecture, les choix technologiques et l'interface. Cette discussion collective garantit l'alignement et permet à chacun de proposer des solutions.
    
    \item \textbf{Répartition progressive des tâches} : après cette phase, les tâches spécifiques sont distribuées entre les membres selon :
    \begin{itemize}
        \item l'intérêt technique de chacun,
        \item la charge équilibrée,
        \item la nécessité de travail conjoint (pair-programming sur les parties critiques).
    \end{itemize}
    
    \item \textbf{Participation croisée} : chaque membre a touché aux trois domaines (frontend Angular, backend Spring Boot, service IA en Python). Ce chevauchement intentionnel facilite la compréhension du système global et répartit les connaissances.
\end{enumerate}

\subsection{Méthodologie Agile itérative}

Afin de maximiser la productivité dans un laps de temps court (entre quatre et cinq semaines), l'équipe a adopté une méthodologie Agile fondée sur des sprints courts.

\paragraph{Caractéristiques des sprints}

\begin{itemize}
    \item \textbf{Cadence} : sprints d' \textbf{un jour sur deux}.
    \item \textbf{Contenu} : chaque sprint prédéfinit un ensemble restreint d'objectifs (nouvelles fonctionnalités, corrections, tests).
    \item \textbf{Revue et rétrospective} : à la fin de chaque sprint, l'équipe évalue :
    \begin{itemize}
        \item ce qui a été réalisé,
        \item les blocages rencontrés,
        \item les tâches prioritaires pour le sprint suivant.
    \end{itemize}
\end{itemize}

\subsection{Conception UX/UI : maquettes}

\paragraph{Maquettage avec Figma}

Les interfaces utilisateur ont été prototypées avec \raisebox{-1.1ex}{\includegraphics[height=1.5em]{images/Figma-Logo.png}}, un outil collaboratif de conception web. Les maquettes ont couvert les principales pages de l'application :

\begin{itemize}
    \item Page d'authentification (connexion/inscription),
    \item Page d'upload de vidéo,
    \item Page de liste des vidéos (« My Videos »),
    \item Page de lecture d'une vidéo, affichage du titre et résumé,
    \item Page de chatbot de question--réponse,
    \item Page de paramètres utilisateur.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/figmas.jpg}
    \caption{Maquettes Figma de l'application OCULA}
    \label{fig:placeholder}
\end{figure}

Ces maquettes ont guidé le développement du frontend et ont évolué vers une version finale améliorée, fidèle à la vision initiale.

\subsection{Stratégie Docker-first}

La gestion des environments et du déploiement a reposé sur une stratégie favorisant la conteneurisation complète de l'application.

\paragraph{Dockerisation de tous les services}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/docker_compose.png}
    \caption{Architecture Docker de l'application OCULA}
    \label{fig:placeholder}
\end{figure}

L'ensemble de l'application (frontend, backend, service IA, base de données) a été empaquetée dans des conteneurs Docker dès le départ. L'architecture Docker de l'application est orchestrée par Docker Compose, qui coordonne les différents services et garantit leur interaction transparente au sein de l'infrastructure conteneurisée.

\subsection{Planification et diagramme de Gantt}

Un diagramme de Gantt a été élaboré pour visualiser le calendrier, les dépendances entre tâches et les étapes clés du projet.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/gantt.jpg}
    \caption{Planification du projet avec diagramme de Gantt}
    \label{fig:placeholder}
\end{figure}

\subsection{Processus de validation et revue de code}

\paragraph{Rigueur sur les branches et merges}

Sur GitLab, nous avons adopté un workflow structuré pour la gestion du code :

\begin{itemize}
    \item Chaque fonctionnalité ou tâche génère une nouvelle branche (ex. \path{feature/video-upload}, \path{feature/rag-chatbot}).
    \item Une fois la fonctionnalité terminée, la branche est fusionnée dans \path{dev}.
    \item Après tests et vérification que tout fonctionne correctement, le code est fusionné dans \path{main}.
\end{itemize}

Ce workflow garantit une séparation claire entre le développement actif, l'intégration et la version stable du projet.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/gitnew.png}
    \caption{Workflow Git adopté pour la gestion du code}
    \label{fig:placeholder}
\end{figure}

\subsection{Difficultés rencontrées et gestion des risques}

\paragraph{Difficultés principales}

\subsubsection*{Hétérogénéité des environnements de développement}

\textbf{Problème} : les trois membres de l'équipe travaillaient sur des systèmes d'exploitation différents avec des configurations locales variées (versions de Python, Java, Node.js différentes, outils de build divergents).

\textbf{Impact} : incompatibilités lors du partage de code, dépendances non portables, scripts de build défaillants sur certains postes, temps perdu en débogage environnemental.

\textbf{Solution} :
\begin{itemize}
    \item \textbf{Stratégie Docker-first} : l'application a été entièrement conteneurisée, garantissant une cohérence totale entre développement et déploiement.
    \item \textbf{Docker Compose} : orchestration centralisée de tous les services, permettant le lancement de l'application entière en une seule commande, quel que soit l'OS.
    \item \textbf{Documentation d'installation} : guide détaillé pour installer Docker et Docker Compose sur tous les OS, éliminant les frictions de setup.
\end{itemize}

\paragraph{Mécanismes de gestion des risques mis en place}

\begin{itemize}
    \item \textbf{Discussions régulières d'équipe} : réunions quotidiennes pour synchronisation et détection des blocages,
    \item \textbf{Répartition claire des tâches} : chaque membre savait précisément sur quoi travailler, minimisant les doublons.
    \item \textbf{Validation progressive des fonctionnalités} : chaque module était validé manuellement lors de son développement.
\end{itemize}

\section{État de l'art et fondements théoriques}

\subsection{Analyse des solutions existantes}

\subsubsection{Otter.ai}

\textbf{Description} \\
Otter.ai est une application spécialisée dans la transcription automatique de contenus audio et vidéo, principalement orientée vers les réunions professionnelles. Elle propose également des résumés automatiques et une recherche textuelle dans les transcriptions\cite{otter_ai_website}.

\textbf{Fonctionnalités principales}
\begin{itemize}
    \item Transcription automatique en temps réel ou différé
	\item Résumé automatique de réunions
	\item Recherche par mots-clés dans les transcriptions
\end{itemize}

\textbf{Modèle économique}  
Solution freemium avec des fonctionnalités avancées accessibles uniquement via abonnement payant.

\textbf{Limites}
\begin{itemize}
	\item Orienté réunions, peu adapté aux vidéos longues génériques
	\item Absence de chatbot conversationnel basé sur RAG
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{images/otter.png}
	\caption{Page tarification de Otter.ai}
	\label{fig:otter}
\end{figure}

\subsubsection{Fireflies.ai}

\textbf{Description} \\  
Fireflies.ai est un outil d’analyse automatique de réunions intégrant transcription, résumé et recherche. Il s’intègre directement à des plateformes de visioconférence\cite{fireflies_ai_website}.

\textbf{Fonctionnalités principales}
\begin{itemize}
    \item Transcription automatique de réunions
	\item Résumés et points clés
	\item Recherche dans les conversations
\end{itemize}

\textbf{Modèle économique}  
Service payant avec abonnement mensuel.

\textbf{Limites}
\begin{itemize}
    \item Fortement dépendant des outils de visioconférence
    \item Pas conçu pour l’analyse libre de fichiers vidéo
	\item Absence d’interrogation conversationnelle avancée
\end{itemize}

\begin{figure}[H]
	\centering
    \includegraphics[width=\linewidth]{images/fireflies.png}
    \caption{Service et tarification Fireflies.ai}
    \label{fig:placeholder}
\end{figure}


\subsubsection{Google (YouTube / Vertex AI)}

\textbf{Description}  
Google propose plusieurs solutions permettant l’analyse de contenus vidéo. YouTube intègre la génération automatique de sous-titres, tandis que Vertex AI et Speech-to-Text permettent la transcription et l’analyse via API\cite{google_vertex_ai}.

\textbf{Fonctionnalités principales}
\begin{itemize}
    \item Transcription automatique de vidéos
	\item Recherche textuelle basique
	\item APIs d’analyse audio et vidéo
\end{itemize}

\textbf{Modèle économique}  
YouTube est gratuit pour l’utilisateur final, tandis que les APIs Google Cloud sont facturées à l’usage.

\textbf{Limites}
\begin{itemize}
    \item Pas de chatbot conversationnel sur le contenu vidéo
	\item Solutions fragmentées (plusieurs services à combiner)
	\item Mise en place technique complexe
\end{itemize}


\subsubsection{OpenAI (ChatGPT / Whisper API)}

\textbf{Description}  
OpenAI propose des modèles de langage et de transcription permettant l’analyse de contenus textuels et audio. ChatGPT permet l’interrogation conversationnelle de documents, tandis que Whisper assure une transcription automatique de haute qualité\cite{openai_whisper_intro}.

\textbf{Fonctionnalités principales}
\begin{itemize}
    \item Transcription audio multilingue
	\item Génération de résumés
	\item Question-réponse conversationnelle
\end{itemize}

\textbf{Modèle économique}  
APIs payantes facturées à l’usage, avec accès gratuit limité via l’interface ChatGPT.

\textbf{Limites}
\begin{itemize}
	\item Pas de gestion native de vidéos complètes
	\item Absence de pipeline clé-en-main pour l’analyse vidéo
	\item Pas d’indexation temporelle automatique
\end{itemize}

Bien que plusieurs applications et plateformes proposent des fonctionnalités proches
de celles du projet OCULA, aucune ne couvre l’ensemble du périmètre fonctionnel visé.
OCULA se distingue des solutions existantes à plusieurs niveaux, tant sur le plan fonctionnel
que technique.

\begin{table}[H]
	\centering
	\renewcommand{\arraystretch}{1.3}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Solution} & \textbf{Vidéo} & \textbf{Résumé} & \textbf{Chat RAG} \\
		\hline
		Otter.ai & \checkmark & \checkmark & \texttimes \\
		Fireflies.ai & \checkmark & \checkmark & \texttimes \\
		Google (YouTube) & \checkmark & \texttimes & \texttimes \\
		OpenAI (API) & \texttimes & \checkmark & \checkmark \\
		\textbf{OCULA} & \checkmark & \checkmark & \checkmark \\
		\hline
	\end{tabular}
	\caption{Comparaison fonctionnelle entre OCULA et les solutions existantes}
	\label{tab:comparison}
\end{table}


\subsection{Reconnaissance automatique de la parole (ASR)}
ASR (automatic speech recognition) est une technologie qui convertit le langage parlé en texte écrit. Elle traite les signaux audio, identifie les modèles de parole et les transcrit en texte avec une grande précision \cite{ionos_asr_guide, huggingface_asr_tasks}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/fonctionnement-automatic-speech-recognition.jpg}
    \caption{Reconnaissance automatique de la parole}
    \label{fig:placeholder}
\end{figure}
\subsubsection{Principes généraux de la transcription audio--texte}
Principalement on distingue entre deux approches pour la reconnaissance automatique de la parole \cite{ionos_asr_guide, cmusphinx_concepts}:

\begin{itemize}
    \item \textbf{Approche hybride classique: }est une méthode qui combine un modèle acoustique, un modèle de lexique et un modèle linguistique pour transcrire la parole en texte, en s’appuyant sur des données audio alignées afin d’associer précisément les segments aux phonèmes et aux mots.

    \item  \textbf{Deep Learning: }utilise des réseaux de neurones pour convertir directement la parole en texte, sans séparer explicitement les modèles acoustique, lexique et linguistique. Elle apprend automatiquement les relations entre les sons et les mots à partir de grandes quantités de données audio.
\end{itemize}

\subsubsection{Choix de modèle (Whisper)}

Whisper est un système de reconnaissance automatique de la parole (ASR) basé sur l’intelligence artificielle, entraîné sur environ 680 000 heures de données audio multilingues provenant du web. Grâce à cette très grande diversité de données, il est particulièrement robuste face aux accents, au bruit de fond et au vocabulaire technique. Whisper est capable non seulement de transcrire la parole dans plusieurs langues, mais aussi de traduire directement ces langues vers l’anglais, ce qui en fait un outil polyvalent pour de nombreuses applications de traitement vocal et de recherche en intelligence artificielle\cite{openai_whisper_intro, openai_whisper_card}.


\begin{table}[h]
\centering
\caption{Comparaision des Performance des Models ASR}
\begin{tabular}{lccccc}
\hline
\textbf{Criteria} & \textbf{Whisper} & \textbf{AssemblyAI U3} & \textbf{Amazon} & \textbf{Microsoft} \\
\hline
Accuracy (English) & 92.4\% & \textbf{94.1\%} & 92.4\% & 92.5\% \\
Accuracy (Multilingual) & \textbf{92.6\%} & 91.3\% & 89.9\% & 88.9\% \\
WER (English) & 6.5\% & \textbf{5.9\%} & 7.6\% & 7.5\% \\
WER (Multilingual) & \textbf{7.4\%} & 8.7\% & 10.1\% & 11.1\% \\
\hline
\end{tabular}
\end{table}


Le choix de Whisper a été évident , du a ça capacité impressive dans la transcription multilingue , et sont WER minimal par rapport aux autre modèle. De plus nous favorisons les modèles open-source, et Whisper étant le meilleur dans cette catégorie\cite{assemblyai_benchmarks}.


\subsection{Représentations vectorielles et recherche sémantique}
\subsubsection{Définition des embeddings}
Un embedding est une représentation vectorielle d’éléments comme les textes, les images, les vidéos ou les signaux audio. Cette transformation en valeurs numériques permet aux modèles de machine learning de comprendre les relations sémantiques et d’effectuer des tâches telles que la recherche, la similarité ou la classification\cite{cloudflare_embeddings_ai}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/embedding.png}
    \caption{Fonctionnement de embedding}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Indexation et recherche vectorielle}
L’indexation vectorielle est une technique qui consiste à organiser des données sous forme de vecteurs numériques afin de permettre une recherche rapide et efficace dans des espaces de grande dimension.

La recherche vectorielle est le processus qui consiste à comparer le vecteur d’une requête avec ceux stockés dans cet index afin d’identifier les éléments les plus proches sémantiquement\cite{google_bigquery_vector_search}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/example_index_vec.png}
    \caption{Illustration de l’indexation vectorielle}
    \label{fig:placeholder}
\end{figure}

La figure illustre le principe de l’indexation et de la recherche vectorielle. 
Chaque point coloré représente un objet (mot, image ou donnée) transformé en vecteur 
dans un espace numérique. Les éléments ayant un sens similaire se retrouvent proches 
les uns des autres, formant des groupes sémantiques. Par exemple, les mots ``cat'', 
``dog'' et ``lion'' sont regroupés car ils appartiennent à la même catégorie d’animaux, 
tandis que ``car'', ``truck'' et ``vehicle'' forment un autre groupe lié aux moyens 
de transport. Lors d’une recherche vectorielle, le système consiste simplement à 
retrouver les points les plus proches du vecteur de la requête afin d’identifier 
les résultats les plus pertinents\cite{google_bigquery_vector_search}. 

\subsection{Modèles de langage (LLM)}
\subsubsection{Capacités et limites des LLM}
Un LLM (Large Language Model ou Grand Modèle de Langage) est un système d'intelligence artificielle entraîné sur d'immenses quantités de données textuelles pour comprendre, générer et manipuler le langage humain de manière fluide et cohérente\cite{haq_llm_limitations}.

\begin{itemize}
    \item \textbf{Capacités:}
    \begin{itemize}
        \item \textbf{Génération de contenu:} ils peuvent rédiger instantanément des textes.
        \item \textbf{Synthèse d'informations:} ils peuvent résumer des documents très long en extrayant les points clés.
        \item \textbf{Traduction et adaptation de style:} Ils peuvent traduire des langues et modifier le style d'un texte.
    \end{itemize}
    \item \textbf{Limites}
    \begin{itemize}
        \item \textbf{Hallucinations:} ils peuvent confirmer des informations fausses.
        \item \textbf{Coût de calcul élevé:} ils sont très chers à entraîner et à faire fonctionner\cite{haq_llm_limitations}. 
    \end{itemize}
\end{itemize}

\subsubsection{Choix de modèle (gpt-oss-120b)}
Lors de notre recherche des modèles de LLM à utiliser, nous avons identifié deux candidats principaux : gpt-oss-120b et llama3.1-8b, tous deux accessibles via l’API fournie par Cerebras\cite{cerebras_systems, openai_gpt_oss}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{images/gpt-vs-llama.png}
    \caption{Camparaison entre gpt-oss-120b et llama3.1-8b}
    \label{fig:placeholder}
\end{figure}

En analysant les deux indices de performance, GPQA (Graduate-Level Google-Proof Q\&A) et MMLU (Massive Multitask Language Understanding), nous pouvons clairement constater que le modèle gpt-oss-120b surpasse llama3.1-8b. Cette supériorité s’explique notamment par son nombre beaucoup plus élevé de paramètres (120 milliards) ainsi que par son architecture plus avancée, lui permettant d’obtenir de meilleurs résultats sur des tâches complexes de raisonnement et de compréhension\cite{llmstats_comparison}.


\subsection{Retrieval-Augmented Generation (RAG)}
\subsubsection{Principe général}
Le RAG (Retrieval-Augmented Generation) est une technique qui permet d'optimiser les réponses d'un modèle d'IA en lui donnant accès à des données externes fiables, au-delà de ses connaissances d'entraînement initiales\cite{aws_rag_fr}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/rag_pipeline.png}
    \caption{Pipeline de RAG}
    \label{fig:placeholder}
\end{figure}

Un pipeline de RAG fonctionne en trois étapes simples : d’abord, des documents sont collectés, découpés en petits morceaux puis transformés en vecteurs pour être stockés dans une base vectorielle. Ensuite, quand un utilisateur pose une question, elle est aussi convertie en vecteur afin de retrouver les passages les plus pertinents dans cette base. Enfin, ces informations sont envoyées avec la question à un LLM, qui s’en sert comme contexte pour produire une réponse plus précise et fiable\cite{nvidia_rag_explained}.

\subsubsection{Forces et limites du RAG}
Le RAG est une approche qui combine la recherche d’informations externes avec la génération de texte par un LLM. Cette architecture permet d’améliorer la qualité des réponses, mais elle présente également certaines contraintes techniques.

\begin{itemize}
    \item \textbf{Amélioration de la précision :} Le RAG permet de générer des réponses plus fiables en s’appuyant sur des documents externes, ce qui réduit fortement les hallucinations des modèles de langage.
    
    \item \textbf{Dépendance à la qualité de la recherche :} La performance du système dépend fortement de la pertinence des documents récupérés.
    
    \item \textbf{Latence et complexité :} L’étape de récupération des données ajoute du temps de réponse et augmente les coûts techniques et computationnels.
\end{itemize}

\section{Spécifications et conception de l'application OCULA}
\subsection{Présentation générale de la solution}
Notre application \textbf{Ocula} est une solution dédiée au traitement et à l’analyse de vidéos. Elle permet d’extraire automatiquement les transcriptions, puis de les exploiter pour générer des titres et des résumés pertinents. De plus, elle intègre une approche RAG (Retrieval-Augmented Generation) afin de répondre de manière précise et contextuelle aux questions des utilisateurs concernant le contenu des vidéos.

\subsection{Cas d'utilisation et fonctionnalités}
\subsubsection{Upload et gestion des vidéos}
Notre solution \textbf{Ocula} gère les vidéos téléversées en s’appuyant sur le service de \textbf{Azure Blob Storage}. L’utilisateur peut importer une vidéo depuis la partie client, puis une requête est envoyée au backend afin de créer automatiquement une entrée correspondante dans notre base de données. La vidéo est ensuite stockée dans notre object storage, où elle est associée à une clé unique. Cette clé permet de générer une URL sécurisée donnant accès à la vidéo lorsque cela est nécessaire.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.15\linewidth]{images/upload_video.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Transcription et génération de titre et de résumé}
Notre solution \textbf{Ocula} permet d’extraire automatiquement la transcription des vidéos en exploitant les capacités de \textbf{Whisper}. Après le téléversement d’une vidéo, une tâche de traitement est déclenchée afin de l’analyser. Une fois la transcription extraite, nous enregistrons ses différents segments, puis nous divisons le script en plusieurs chunks.

Ces chunks sont ensuite transformés en embeddings et stockés dans une base de données vectorielle, afin de pouvoir être exploités ultérieurement dans un système \textbf{RAG} (Retrieval-Augmented Generation). Enfin, grâce à une pipeline composée d’un prompt et d’un LLM (\textbf{gpt-oss-120b}), la solution génère automatiquement un titre pertinent ainsi qu’un résumé du contenu de la vidéo.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/transcription.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Chatbot de question--réponse sur la vidéo}
Notre solution \textbf{Ocula} intègre un assistant \textbf{chatbot} basé sur l’intelligence artificielle. Cet assistant utilise un \textbf{LLM} afin de répondre aux questions des utilisateurs tout en s’appuyant sur une approche \textbf{RAG} (Retrieval-Augmented Generation) et sur les différents chunks générés lors du traitement des vidéos.

Lorsqu’un utilisateur envoie un message, le chatbot déclenche une pipeline RAG qui recherche les informations pertinentes dans les chunks de la transcription associés à la vidéo concernée. Le contexte ainsi récupéré est ensuite transmis au LLM (\textbf{gpt-oss-120b}), ce qui lui permet de fournir des réponses précises, pertinentes et adaptées au contenu réel de la vidéo.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.1\linewidth]{images/chat_case.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsection{Exigences fonctionnelles}

Les exigences fonctionnelles décrivent les capacités et services que doit fournir l’application OCULA du point de vue des utilisateurs et du système métier.

\subsubsection{Gestion des utilisateurs}

\begin{itemize}
	\item Le système doit permettre à un utilisateur de créer un compte à l’aide d’une adresse e-mail et d’un mot de passe.
	\item Le système doit permettre l’authentification sécurisée des utilisateurs.
	\item Le système doit permettre la modification des informations du profil (nom, prénom, e-mail, mot de passe, avatar).
	\item Le système doit gérer les sessions utilisateurs via un mécanisme d’authentification basé sur des tokens jwt.
\end{itemize}

\subsubsection{Gestion des vidéos}

\begin{itemize}
	\item Le système doit permettre à un utilisateur d’importer une vidéo depuis l’interface web.
	\item Le système doit stocker les vidéos dans un stockage objet.
	\item Le système doit associer chaque vidéo à son utilisateur propriétaire.
	\item Le système doit permettre d’afficher la liste des vidéos d’un utilisateur.
	\item Le système doit permettre de consulter les détails d’une vidéo (titre, résumé).
\end{itemize}

\subsubsection{Traitement automatique des vidéos}

\begin{itemize}
	\item Le système doit extraire automatiquement la transcription d’une vidéo importée.
	\item Le système doit segmenter la transcription en portions temporelles.
	\item Le système doit transformer ces segments en représentations vectorielles (embeddings).
	\item Le système doit générer automatiquement un titre et un résumé à partir de la transcription.
\end{itemize}

\subsubsection{Recherche sémantique et question--réponse}

\begin{itemize}
	\item Le système doit permettre à l’utilisateur de poser des questions sur le contenu d’une vidéo.
	\item Le système doit rechercher les segments les plus pertinents dans la base de données vectorielle.
	\item Le système doit générer des réponses contextuelles à l’aide d’un LLM.
	\item Le système doit conserver l’historique des échanges entre l’utilisateur et le chatbot.
\end{itemize}

\subsubsection{Suivi du traitement}

\begin{itemize}
	\item Le système doit gérer les différents états du traitement d’une vidéo (en attente, en cours, terminé, échoué).
	\item Le système doit permettre à l’utilisateur de suivre l’avancement du traitement en temps réel.
	\item Le système doit notifier l’interface utilisateur lors des changements d’état.
\end{itemize}

\subsection{Exigences non fonctionnelles}

Les exigences non fonctionnelles décrivent les contraintes techniques et qualitatives que doit respecter l’application OCULA afin d’assurer sa fiabilité, sa performance et sa sécurité.

\subsubsection{Performance et latence}

\begin{itemize}
	\item Le système doit assurer un temps de réponse rapide pour les interactions utilisateur (navigation, affichage des vidéos, consultation des résultats).
	\item Le traitement des vidéos doit être effectué de manière asynchrone afin de ne pas bloquer l’interface utilisateur.
	\item Le temps de réponse du chatbot doit rester raisonnable pour garantir une expérience fluide.
	\item Les opérations de recherche sémantique doivent être optimisées pour réduire la latence lors des requêtes.
\end{itemize}

\subsubsection{Sécurité et confidentialité}

\begin{itemize}
	\item Le système doit garantir l’authentification et l’autorisation des utilisateurs.
	\item Les données sensibles doivent être protégées lors du stockage et des communications.
	\item L’accès aux vidéos doit être restreint uniquement à leur propriétaire.
	\item la communication entre le backend et le service d’intelligence artificielle repose sur l’utilisation de clés d’API (API keys) permettant d’authentifier les requêtes et de restreindre l’accès aux endpoints internes.
\end{itemize}

\subsection{Architecture globale}
\subsubsection{Vue d'ensemble (Frontend, Backend, AI Service)}
Notre application \textbf{Ocula} suit une architecture structurée en trois principales couches. D’abord, le frontend permet à l’utilisateur de téléverser une vidéo et de demander son analyse. Ensuite, le backend gère les requêtes, enregistre les métadonnées dans la base de données et stocke les fichiers dans un système de stockage objet. Enfin, la partie IA prend en charge le traitement de la vidéo, elle génère la transcription, crée des embeddings stockés dans une base vectorielle, puis exploite ces données pour alimenter le LLM, qui produit des résumés, des titres et des réponses pertinentes aux questions des utilisateurs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/app_workflow.png}
    \caption{Vue d'ensemble}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Choix technologiques et justification}
Pour le développement de l’application \textbf{Ocula}, nous avons choisi des technologies adaptées à une architecture moderne et scalable.

Le \textbf{backend}, nous avons choisi \textbf{Spring-Boot}, un framework Java largement adopté pour la conception d’API robustes et sécurisées. Il offre une intégration avec les bases de données relationnelles, ainsi qu’un écosystème facilitant le développement.

Le \textbf{frontend} a été développé avec \textbf{Angular}, un framework structuré et performant, particulièrement adapté aux applications web complexes. \textbf{Angular} permet une bonne organisation du code, une communication fluide avec les API REST, ainsi qu’une expérience utilisateur dynamique et réactive.

La \textbf{partie intelligence artificielle}, nous avons utilisé \textbf{Python} avec \textbf{Flask} pour exposer les services IA sous forme d’API légères et rapides. L’intégration de \textbf{LangChain} a permis de faciliter la mise en place des pipelines RAG.

Pour la \textbf{persistance des données}, nous avons choisi \textbf{PostgreSQL} comme base de données relationnelle, en raison de sa capacité à gérer des structures de données complexes. En complément, nous avons utilisé \textbf{ChromaDB} comme base de données vectorielle, spécialement conçue pour le stockage et la recherche d’embeddings.

\subsection{Conception}
\subsubsection{Modèle conceptuel et logique}

Le modèle conceptuel d’OCULA est structuré autour de l’entité \textit{Video}, qui constitue le cœur fonctionnel du système. Bien que l’application soit multi-utilisateur, l’ensemble du cycle de traitement IA est vidéo-centré : les opérations d’analyse, de transcription, de génération d’embeddings et de question–réponse s’appuient systématiquement sur l’identifiant \path{video\id}.
Ainsi, l’entité \textit{User} possède les vidéos, mais c’est la \textit{Video} qui orchestre le pipeline métier.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/diagramme de class entités.png}
\caption{Diagramme de classes du domaine métier d’OCULA}
\label{fig:domain-model}
\end{figure}

\paragraph{Relation User -- Video}

Un utilisateur peut posséder plusieurs vidéos (\textbf{1--N}). Cette relation n’est pas uniquement logique, elle répond à plusieurs exigences architecturales :

\begin{itemize}
    \item \textbf{Sécurité} : le contrôle d’accès repose sur le propriétaire de la vidéo ; un utilisateur ne peut accéder qu’à ses propres ressources.
    \item \textbf{Isolation des données} : chaque compte constitue un espace logique distinct.
    \item \textbf{Scalabilité} : les requêtes peuvent être filtrées et paginées par \path{userId}, améliorant les performances.
    \item \textbf{Architecture multi-tenant} : la base est partagée, mais le cloisonnement est assuré au niveau applicatif.
\end{itemize}

\paragraph{Relation Video -- VideoTranscript}  


Contrairement à une approche monolithique consistant à stocker une transcription complète sous forme d’un simple champ texte, OCULA adopte une modélisation segmentée. Une vidéo possède donc \textbf{0..*} entités \textit{VideoTranscript}, chacune caractérisée par un \path{startTime}, un \path{endTime} et un \path{transcriptText}.

Ce choix répond à plusieurs objectifs :

\begin{itemize}
    \item \textbf{Traçabilité temporelle} : chaque portion de texte peut être reliée à un intervalle précis de la vidéo.
    \item \textbf{Robustesse du pipeline} : en cas d’échec partiel du traitement, les segments déjà générés restent persistés.
    \item \textbf{Réindexation ciblée} : il est possible de retraiter ou ré-encoder certains segments sans recalcul global.
    \item \textbf{Granularité adaptée au RAG} : les segments constituent la base naturelle du découpage en chunks.
\end{itemize}

\paragraph{Relation Video -- Message}

Chaque vidéo peut contenir \textbf{0..*} messages correspondant aux échanges entre l’utilisateur et le chatbot. La persistance de ces messages dépasse le simple besoin d’affichage :

\begin{itemize}
    \item \textbf{Historique multi-session} : la conversation est conservée même après rechargement ou changement d’appareil.
    \item \textbf{Audit et explicabilité} : il est possible d’analyser les requêtes et réponses produites.
    \item \textbf{Évolutivité} : ces données peuvent servir à des analyses ultérieures ou à l’amélioration du système.
\end{itemize}

\paragraph{Gestion d’état du traitement}

L’attribut \path{status} de l’entité \textit{Video} (PENDING, PROCESSING, COMPLETED, FAILED) modélise explicitement le cycle de vie du traitement asynchrone.
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/Gestion d’état du traitement-2026-02-22-192854.png}
\caption{Gestion d’état du traitement (VideoStatus)}
\label{fig:domain-model} 
\end{figure}
Dans un système distribué où la transcription et l’indexation peuvent prendre plusieurs minutes, ce champ est essentiel pour :

\begin{itemize}
    \item informer le frontend de l’avancement,
    \item permettre une mise à jour en temps réel via WebSocket,
    \item gérer proprement les erreurs et les tentatives de relance,
    \item éviter les états incohérents ou bloqués.
\end{itemize}

Ainsi, le modèle conceptuel ne se limite pas à une simple représentation des données : il structure le comportement global du système et soutient la robustesse de l’architecture.

\subsubsection{Modélisation des transcriptions, chunks et embeddings}

Si le modèle précédent décrit la structure relationnelle des données métier, le mécanisme de recherche sémantique repose sur une modélisation complémentaire adaptée au RAG.

\medskip
\textbf{1. De la transcription segmentée aux chunks}

La transcription est initialement stockée sous forme de segments temporels (\textit{VideoTranscript}). 
Toutefois, une recherche sémantique efficace ne peut s’appuyer ni sur un texte monolithique complet, ni sur des segments trop courts et isolés.

Le système applique donc une étape de \textit{chunking}, consistant à regrouper et découper les segments afin de produire des blocs textuels cohérents et exploitables pour l’indexation vectorielle.

Cette granularité intermédiaire permet :

\begin{itemize}
    \item d’améliorer la précision lors de la récupération de contexte,
    \item de réduire le bruit informationnel,
    \item d’optimiser l’utilisation de la fenêtre de contexte du LLM,
    \item de maintenir un équilibre entre cohérence globale et pertinence locale.
\end{itemize}

\medskip
\textbf{2. Génération des embeddings}


Chaque chunk est transformé en vecteur numérique à l’aide du modèle 
\path{sentence-transformers/all-MiniLM-L6-v2}, chargé via \textit{HuggingFaceEmbeddings}.

Ce modèle encode les textes en vecteurs de dimension fixe permettant une comparaison sémantique par mesure de similarité ou de distance vectorielle. 
Chaque chunk correspond ainsi à un triplet logique :

\begin{itemize}
    \item texte source,
    \item embedding haute dimension,
    \item métadonnées associées.
\end{itemize}

Cette représentation vectorielle permet de comparer la question de l’utilisateur avec le contenu vidéo et d’identifier les passages les plus pertinents.

\medskip
\textbf{3. Organisation dans la base vectorielle}

Les embeddings sont stockés dans ChromaDB selon une organisation par vidéo. 
Pour chaque vidéo, une collection dédiée (\path{video\<video\id>}) est utilisée ou créée lors de l’initialisation du flux d’indexation.

Chaque document vectoriel contient :

\begin{itemize}

    \item le texte du chunk,
    \item l’embedding associé,
    \item des métadonnées : \path{video\id}, \path{start\time}, \path{end\time}.
\end{itemize}
\begin{figure}[H]
    \centering
        \includegraphics[width=0.4\textwidth]{images/chroma_document_model.png}
    \caption{Structure d’un document vectoriel dans ChromaDB}
    \label{fig:chroma-document}
\end{figure}

Lors d’une requête utilisateur, le mécanisme RAG :

\begin{itemize}
    \item effectue une recherche par similarité dans la collection de la vidéo concernée,
    \item sélectionne les $k$ passages les plus proches,
    \item applique un filtrage et un éventuel \textit{reranking} afin d’améliorer la pertinence du contexte transmis au LLM.
\end{itemize}

Cette organisation garantit :

\begin{itemize}
    \item l’isolation des recherches à une vidéo donnée,
    \item la traçabilité des sources utilisées dans la réponse,
    \item une correspondance directe entre passage récupéré et intervalle temporel.
\end{itemize}

Le modèle relationnel assure la cohérence transactionnelle des données métier, tandis que la couche vectorielle optimise la récupération contextuelle pour le RAG ; les deux couches sont donc complémentaires et synchronisées par \path{video\id}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/rag_pipeline_matrix.png}
    \caption{Pipeline RAG : phases de retrieval, construction du contexte et génération}
    \label{fig:rag-pipeline}
\end{figure}

\subsubsection{Persistance : base relationnelle, stockage objet et base vectorielle}

La nature hétérogène des données manipulées par OCULA impose une stratégie de persistance adaptée à chaque type d’information. 
Le système distingue ainsi trois couches de stockage complémentaires : relationnelle, objet et vectorielle.

\paragraph{Base relationnelle (PostgreSQL)}

Les entités métier structurées (\textit{User}, \textit{Video}, \textit{Message}, \textit{VideoTranscript}) sont stockées dans une base relationnelle.

Ce choix permet :

\begin{itemize}
    \item de garantir l’intégrité référentielle entre les entités,
    \item d’assurer la cohérence transactionnelle au niveau des données métier,
    \item de faciliter les requêtes applicatives (filtrage, pagination, jointures),
    \item de maintenir une structure normalisée et contrôlée.
\end{itemize}

Il est toutefois important de noter qu’il n’existe pas de transaction globale entre PostgreSQL, le stockage objet et la base vectorielle : l’architecture repose sur une coordination applicative entre services.

\paragraph{Stockage objet (Azure Blob Storage)}

Les fichiers volumineux tels que les vidéos, les miniatures et les avatars sont stockés dans un service de type objet.

La base relationnelle conserve principalement la \textit{clé d’objet} (object key) associée à chaque ressource (\path{content}, \path{thumbnail}, \path{avatar}). 
Une URL signée est générée dynamiquement lors de l’accès au fichier.

Ce choix permet :

\begin{itemize}
    \item d’optimiser le stockage des contenus binaires,
    \item de réduire la charge sur la base relationnelle,
    \item d’assurer une meilleure scalabilité des médias,
    \item de séparer clairement données structurées et fichiers lourds.
\end{itemize}

\paragraph{Base vectorielle (ChromaDB)}

Les embeddings générés à partir des chunks sont stockés dans une base vectorielle dédiée (ChromaDB).

Chaque vidéo dispose d’une collection nommée \path{video\<video\id>}. 

Cette organisation assure :

\begin{itemize}
    \item l’isolement des recherches par vidéo,
    \item la traçabilité temporelle des passages récupérés,
    \item une recherche efficace par mesure de similarité ou distance vectorielle.
\end{itemize}


\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/hybrid-architecture.png}
\caption{Architecture de persistance hybride d’OCULA : base relationnelle, stockage objet et base vectorielle}
\label{fig:persistence-hybrid}
\end{figure}
 
\medskip

Le modèle relationnel assure la cohérence des données métier, le stockage objet prend en charge les contenus binaires via des clés d’objet, et la base vectorielle optimise la récupération sémantique pour le RAG. 
Ces trois couches sont complémentaires et interconnectées principalement par \path{video\id} (et \path{user\id} pour les ressources utilisateur). 
\medskip

La conception présentée précédemment définit la structure logique et la stratégie de persistance du système. 
Nous détaillons désormais son implémentation concrète à travers les différentes couches applicatives : frontend, backend et service IA.
\section{Implémentation}
\subsection{Frontend : application Angular}
\subsubsection{Principales pages et navigation}
\paragraph{5.1.1.1 Pages et routes}
Routes principales (source : \path{app.routes.ts}).
Le tableau~\ref{tab:frontend-routes} présente une vue synthétique des chemins disponibles, des composants associés et du niveau de protection appliqué.
\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.28}
\newcommand{\badgepub}{\colorbox{green!15}{\strut\path{publique}}}
\newcommand{\badgeguard}[1]{\colorbox{blue!10}{\strut\path{#1}}}
\begin{tabular}{|>{\raggedright\arraybackslash}p{1.9cm}|>{\raggedright\arraybackslash}p{2.7cm}|>{\raggedright\arraybackslash}p{2.3cm}|>{\raggedright\arraybackslash}p{7.3cm}|}
\hline
\rowcolor{techheader}
\textcolor{white}{\textbf{Route}} & \textcolor{white}{\textbf{Page}} & \textcolor{white}{\textbf{Protection}} & \textcolor{white}{\textbf{R\^ole principal}} \\
\hline
\rowcolor{techlinea}
\cellcolor{techroute}\path{/} & Page d'accueil & \badgepub & Page d'accueil et point d'entr\'ee vers l'authentification. \\
\hline
\rowcolor{techlineb}
\cellcolor{techroute}\path{/auth} & Page d'authentification & \badgeguard{LoginGuard} & Connexion/inscription dans une seule vue. \\
\hline
\rowcolor{techlinea}
\cellcolor{techroute}\path{/upload} & Page d'upload & \badgeguard{AuthGuard} & Page pour envoyer une vid\'eo \`a la plateforme. \\
\hline
\rowcolor{techlineb}
\cellcolor{techroute}\path{/myvideos} & Page My Videos & \badgeguard{AuthGuard} & Liste des vid\'eos de l'utilisateur. \\
\hline
\rowcolor{techlinea}
\cellcolor{techroute}\path{/video/:id} & Page vid\'eo & \badgeguard{AuthGuard} & Lecture vid\'eo, affichage des m\'etadonn\'ees (titre/r\'esum\'e) et suivi du statut de traitement en temps r\'eel. \\
\hline
\rowcolor{techlineb}
\cellcolor{techroute}\path{/settings} & Page param\`etres & \badgeguard{AuthGuard} & Mise \`a jour du profil (nom/pr\'enom/avatar), de l'email et du mot de passe. \\
\hline
\end{tabular}
\caption{Routes frontend, pages associ\'ees et responsabilit\'es}
\label{tab:frontend-routes}
\end{table}

\paragraph{5.1.1.2 La navigation métier et les redirections automatiques}

Pour compléter cette vue statique, la figure~\ref{fig:frontend-nav} illustre la navigation réelle entre les pages, y compris les redirections liées aux guards et les transitions métier.
\begin{figure}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[
    page/.style={rectangle, rounded corners=6pt, draw=#1!70, fill=#1!10,
                 text width=3.7cm, align=center, font=\scriptsize\bfseries,
                 minimum height=1.0cm, inner sep=3pt},
    guard/.style={rectangle, rounded corners=3pt, draw=orange!70, fill=orange!15,
                 text width=2.5cm, align=center, font=\scriptsize, minimum height=0.6cm, inner sep=2.5pt},
    arrow/.style={-{Stealth[length=4pt]}, thick, #1},
    dashed arrow/.style={-{Stealth[length=4pt]}, dashed, thick, #1},
    label/.style={font=\scriptsize\itshape, text=gray, fill=white, inner sep=1.5pt, align=center}
]

%% NOEUDS PAGES (espacement augmenté)
\node[page=gray]   (home)     at (0,0)        {/ \\ Page d'accueil};
\node[page=gray]   (auth)     at (6.4,0)      {/auth \\ Page d'authentification};

\node[page=green]  (myvideos) at (0,-3.6)     {/myvideos \\ Page My Videos};
\node[page=green]  (upload)   at (6.4,-3.6)   {/upload \\ Page d'upload};

\node[page=green]  (video)    at (0,-7.2)     {/video/:id \\ Page vidéo};
\node[page=green]  (settings) at (6.4,-7.2)   {/settings \\ Page paramètres};

%% GUARDS
\node[guard]  (ag1) at (0,-2.1)   {AuthGuard};
\node[guard]  (ag2) at (6.4,-2.1) {LoginGuard};
\node[guard]  (ag3) at (0,-5.7)   {AuthGuard};
\node[guard]  (ag4) at (6.4,-5.7) {AuthGuard};

%% NAVIGATION DIRECTE
\draw[arrow=black] (home) -- node[label, above]{Sign in / Sign up} (auth);
\draw[arrow=black] (home) -- (ag1);
\draw[arrow=black] (ag1) -- (myvideos);
\draw[arrow=black] (auth) -- (ag2);
\draw[arrow=black] (ag2) -- (upload);
\draw[arrow=black] (myvideos) -- (ag3);
\draw[arrow=black] (ag3) -- (video);
\draw[arrow=black] (upload) -- (ag4);
\draw[arrow=black] (ag4) -- (settings);

%% TRANSITIONS METIER (labels déplacés)
\draw[dashed arrow=violet, bend right=22]
    (upload.south west) to node[label, pos=0.55, below]{après upload} (video.east);

\draw[dashed arrow=teal, bend right=24]
    (auth.south west) to node[label, pos=0.45, left]{après login} (myvideos.north east);

\draw[dashed arrow=gray, bend left=10]
    (myvideos.south) to node[label, pos=0.55, right]{clic vidéo} (video.north);

\draw[dashed arrow=red, bend left=30]
    (ag2.west) to node[label, pos=0.58, below]{déjà connecté\\$\rightarrow$ /myvideos} (myvideos.east);

%% LEGENDE (décalée à droite)
\node[font=\scriptsize\bfseries, draw=gray!40, rounded corners=3pt, fill=gray!5,
      text width=2.8cm, align=center] at (11.5,-0.3) {Légende};

\node[page=green, minimum height=0.55cm, text width=2.4cm, font=\scriptsize\bfseries]
    at (11.5,-1.5) {Page protégée};

\node[page=gray, minimum height=0.55cm, text width=2.4cm, font=\scriptsize\bfseries]
    at (11.5,-2.6) {Page publique};

\node[guard, minimum height=0.55cm, text width=2.4cm]
    at (11.5,-3.7) {Guard};

\draw[dashed arrow=violet] (10.7,-4.8) -- (11.8,-4.8) node[right, font=\scriptsize]{transition métier};
\draw[dashed arrow=red]    (10.7,-5.8) -- (11.8,-5.8) node[right, font=\scriptsize]{redirection guard};
\draw[arrow=black]         (10.7,-6.8) -- (11.8,-6.8) node[right, font=\scriptsize]{navigation directe};

\end{tikzpicture}
}%
\caption{Parcours principal des pages du frontend}
\label{fig:frontend-nav}
\end{figure}

\noindent \textit{Remarque :} ce schéma montre le chemin principal. L'utilisateur peut aussi changer de page via le menu de son avatar.




\subsubsection{Échanges avec le backend}

\paragraph{5.1.2.1 Rôle des services\\[0pt]} 
\begin{itemize}
    \item \textbf{AuthService        :} gère la connexion et l'inscription. Il garde les informations de session côté navigateur et permet de savoir si l'utilisateur est encore connecté.
    \item \textbf{VideoService       :} s'occupe des actions liées aux vidéos : envoyer une vidéo, afficher une vidéo, charger la liste et lancer le traitement.
    \item \textbf{BlobStorageService :} récupère les liens nécessaires pour afficher les fichiers (vidéo, miniature, avatar).
    \item \textbf{UserService        :} met à jour les informations du profil utilisateur.
    \item \textbf{WsService          :} permet de voir l'avancement du traitement d'une vidéo en direct, sans recharger la page.
\end{itemize}

\paragraph{5.1.2.2 Séquence de traitement d'une vidéo : upload, déclenchement IA et suivi temps réel}
Le schéma suivant synthétise le scénario métier principal après l'upload d'une vidéo, depuis l'appel REST initial jusqu'aux mises à jour de statut en temps réel.

\begin{figure}[H]
\centering
\resizebox{0.94\textwidth}{!}{%
\begin{tikzpicture}[
    actor/.style={rectangle, draw=gray!60, rounded corners=3pt, fill=gray!10, minimum width=2.7cm, minimum height=0.8cm, align=center, font=\scriptsize\bfseries},
    lifeline/.style={gray!60, dashed},
    call/.style={-{Stealth[length=4pt]}, thick, blue!70!black},
    async/.style={-{Stealth[length=4pt]}, thick, dashed, violet!80!black},
    note/.style={rectangle, draw=gray!40, fill=gray!7, rounded corners=2pt, font=\scriptsize, align=left}
]

\node[actor] (u) at (0,0) {UploadPage};
\node[actor] (v) at (4.2,0) {VideoService};
\node[actor] (b) at (8.4,0) {Backend API};
\node[actor] (w) at (12.6,0) {WsService};

\draw[lifeline] (u) -- (0,-6.1);
\draw[lifeline] (v) -- (4.2,-6.1);
\draw[lifeline] (b) -- (8.4,-6.1);
\draw[lifeline] (w) -- (12.6,-6.1);

\draw[call] (0,-0.9) -- node[above, font=\tiny]{1) upload + metadata + thumbnail} (4.2,-0.9);
\draw[call] (4.2,-1.6) -- node[above, font=\tiny]{2) POST /api/videos/upload} (8.4,-1.6);
\draw[call] (8.4,-2.3) -- node[above, font=\tiny]{3) id vidéo} (4.2,-2.3);
\draw[call] (4.2,-3.0) -- node[above, font=\tiny]{4) POST /api/videos/process/\{id\}} (8.4,-3.0);
\draw[async] (8.4,-4.0) -- node[above, font=\tiny]{5) statut PENDING/PROCESSING/COMPLETED} (12.6,-4.0);
\draw[call] (12.6,-4.8) -- node[above, font=\tiny]{6) refresh GET /api/videos/\{id\}} (8.4,-4.8);

\node[note] at (6.3,-5.7) {Mise à jour UI sans polling\\grâce au topic \path{/topic/videos/\{id\}}.};

\end{tikzpicture}
}%
\caption{Séquence de traitement d'une vidéo : upload, déclenchement IA et suivi temps réel}
\label{fig:frontend-services}
\end{figure}
\subsubsection{Aperçu des pages principales}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/upload.png}
    \caption{Page d'upload}
    \label{fig:frontend-upload-page}
\end{figure}
\noindent Cette page permet de téléverser une vidéo puis de lancer son traitement.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/myvideos.png}
    \caption{Page \textit{My Videos}}
    \label{fig:frontend-myvideos-page}
\end{figure}
\noindent Cette page affiche les vidéos dans une grille simple. Le menu au dessus de l'avatar, visible à droite en haut, permet aussi d'aller vers le reste des pages.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/video.png}
    \caption{Page vidéo}
    \label{fig:frontend-video-page}
\end{figure}
\noindent Cette page affiche la vidéo traitée, accompagnée d’un titre et d’un résumé. Elle propose également un espace d’échange avec l’IA pour discuter du contenu de la vidéo et poser des questions complémentaires.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/settings.png}
    \caption{Page \textit{Settings}}
    \label{fig:frontend-settings-page}
\end{figure}
\noindent Cette page centralise les actions de gestion du compte utilisateur, notamment la mise à jour du profil, de l’adresse e-mail et du mot de passe.

\subsection{Backend : API Spring Boot}
\subsubsection{Organisation en couches}

L'architecture du backend suit le pattern classique stratifié, où chaque couche assume une responsabilité bien définie et interagit explicitement avec ses voisines.

\paragraph{Structure des couches}

\begin{figure}[H]
\centering
\includegraphics[width=0.98\linewidth]{images/Architecture technique du backend.png}
\caption{Architecture en couches du backend OCULA}
\label{fig:backend-architecture}
\end{figure}

\noindent Comme l'illustre la figure~\ref{fig:backend-architecture}, l'architecture en couches du backend segmente les responsabilités en quatre niveaux distincts qui sont:

\begin{itemize}
    \item \textbf{Couche présentation (Controllers)} : expose les endpoints REST et gère la sérialisation HTTP. Elle valide les paramètres entrants, délègue aux services et retourne les réponses formatées. Elle n'applique pas de logique métier complexe.
    
    \item \textbf{Couche métier (Services)} : concentre les règles applicatives : vérification d'existence des entités, validation des contraintes (unicité d'email, cohérence temporelle des transcripts), orchestration du pipeline IA, notifications WebSocket. Les services constituent le contrat fonctionnel du système.
    
    \item \textbf{Couche persistance (Repositories)} : assure l'accès aux données via Spring Data JPA, sans exposition de détails techniques sur la base relationnelle. Les repositories masquent la complexité SQL et permettent des migrations futures sans impact applicatif.
    
    \item \textbf{Couche mapping (DTOs \& Mappers)} : transforme les entités persistées en objets de transfert (Data Transfer Objects) adaptés aux réponses API. Cet isolement protège le modèle interne d'évolutions externes.
\end{itemize}

\paragraph{Avantages architecturaux}

Cette organisation garantit une cohérence comportementale : la logique de validation ou d'autorisation n'est jamais dupliquée entre contrôleurs et services, et les repositories évitent les requêtes ad hoc parsemées dans le code applicatif. En cas de changement de base de données ou de technologie de persistence, seule la couche Repository est affectée.



\subsubsection{Gestion des entités}

Le modèle de données d'OCULA repose sur quatre entités principales structurant les domaines clés : utilisateurs, vidéos, transcriptions et conversations. Chaque entité est persistée en base relationnelle via JPA et possède des responsibilities explicites dans le cycle de traitement des vidéos.

\paragraph{Entités principales}


\noindent \textit{Les attributs détaillés de chaque entité et le diagramme complet du modèle de classes sont présentés à la section \hyperref[fig:domain-model]{Conception}.}

\paragraph{Description détaillée des entités \\}

\noindent\textbf{Entity : User}

\noindent\begin{minipage}[t]{0.33\textwidth}
\vspace{0pt}
\centering
\begin{tikzpicture}[
    x=1.5cm,
    y=1.1cm,
    class/.style={rectangle, minimum width=3.3cm, minimum height=0.55cm, font=\large\bfseries, line width=1.2pt, align=center},
    divider/.style={line width=1.2pt},
    attr/.style={font=\normalsize, align=left, text width=5cm}
]
% Boîte UML (titre + attributs + section méthodes vide)
\draw[divider] (0,0) rectangle (3.6,4.9);
\draw[divider] (0,4.2) -- (3.6,4.2);
\draw[divider] (0,0.6) -- (3.6,0.6);

\node[class] at (1.8,4.55) {User};
\node[attr, anchor=north west] at (0.15,4.05) {
- id: UUID \\
- email: String \\
- firstName: String \\
- lastName: String \\
- password: String \\
- avatar: URL
};
\end{tikzpicture}
\end{minipage}\hfill
\begin{minipage}[t]{0.62\textwidth}
\vspace{0pt}
Représente un utilisateur authentifié du système. Identifiée par un UUID, elle stocke les identifiants de connexion (email, mot de passe haché), le profil (prénom, nom) et optionnellement un avatar. L'entité implémente \texttt{UserDetails} de Spring Security, fournissant les informations nécessaires à l'authentification.
\end{minipage}

\vspace{0.5cm}

\noindent\textbf{Entity : Video}

\noindent\begin{minipage}[t]{0.33\textwidth}
\vspace{0pt}
\centering
\begin{tikzpicture}[
    x=1.5cm,
    y=1.cm,
    class/.style={rectangle, minimum width=3.3cm, minimum height=0.55cm, font=\large\bfseries, line width=1.2pt, align=center},
    divider/.style={line width=1.2pt},
    attr/.style={font=\normalsize, align=left, text width=5cm}
]
\draw[divider] (0,0) rectangle (3.6,5.8);
\draw[divider] (0,5.1) -- (3.6,5.1);
\draw[divider] (0,0.6) -- (3.6,0.6);

\node[class] at (1.8,5.45) {Video};
\node[attr, anchor=north west] at (0.15,4.95) {
- id: UUID \\
- title: String \\
- summary: Text \\
- content: URL \\
- thumbnail: URL \\
- uploadedAt: DateTime \\
- status: Enum \\
- duration: Double
};
\end{tikzpicture}
\end{minipage}\hfill
\begin{minipage}[t]{0.62\textwidth}
\vspace{0pt}
Entité centrale du système, modélisant une vidéo téléversée. Elle maintient des références vers ses ressources (contenu vidéo, miniature) via des URLs ou clés de stockage objet, et inclut les métadonnées générées ou éditées (titre, résumé). Le champ \texttt{status} (énumération : PENDING, PROCESSING, COMPLETED, FAILED) orchestre explicitement le cycle de vie asynchrone du traitement IA. Cela permet au frontend de suivre l'avancement sans polling continu. Le champ \texttt{duration} capture la durée en secondes, essentiel pour les opérations de découpage temporel. Le timestamp \texttt{uploadedAt} est automatiquement défini à la création.
\end{minipage}

\vspace{0.5cm}

\noindent\textbf{Entity : VideoTranscript}

\noindent\begin{minipage}[t]{0.33\textwidth}
\vspace{0pt}
\centering
\begin{tikzpicture}[
    x=1.5cm,
    y=1.cm,
    class/.style={rectangle, minimum width=3.3cm, minimum height=0.55cm, font=\large\bfseries, line width=1.2pt, align=center},
    divider/.style={line width=1.2pt},
    attr/.style={font=\normalsize, align=left, text width=5cm}
]
\draw[divider] (0,0) rectangle (3.6,4.4);
\draw[divider] (0,3.7) -- (3.6,3.7);
\draw[divider] (0,0.6) -- (3.6,0.6);

\node[class] at (1.8,4.05) {VideoTranscript};
\node[attr, anchor=north west] at (0.15,3.55) {
- id: UUID \\
- startTime: Double \\
- endTime: Double \\
- transcriptText: Text
};
\end{tikzpicture}
\end{minipage}\hfill
\begin{minipage}[t]{0.62\textwidth}
\vspace{0pt}
Modélise un segment de la transcription, associé à un intervalle temporel précis. À la différence d'une approche monolithique stockant le texte complet, cette granularité segmentée offre plusieurs avantages : (i) \textbf{traçabilité temporelle} -- chaque segment peut être relié à un intervalle précis de la vidéo; (ii) \textbf{résilience} -- en cas d'échec partiel du traitement IA, les segments complétés restent valides; (iii) \textbf{granularité adaptée au RAG} -- les segments constituent la base naturelle du découpage en chunks pour l'indexation vectorielle.
\end{minipage}

\vspace{0.5cm}

\noindent\textbf{Entity : Message}

\noindent\begin{minipage}[t]{0.33\textwidth}
\vspace{0pt}
\centering
\begin{tikzpicture}[
    x=1.5cm,
    y=1.cm,
    class/.style={rectangle, minimum width=3.3cm, minimum height=0.55cm, font=\large\bfseries, line width=1.2pt, align=center},
    divider/.style={line width=1.2pt},
    attr/.style={font=\normalsize, align=left, text width=5cm}
]
\draw[divider] (0,0) rectangle (3.6,4.4);
\draw[divider] (0,3.7) -- (3.6,3.7);
\draw[divider] (0,0.6) -- (3.6,0.6);

\node[class] at (1.8,4.05) {Message};
\node[attr, anchor=north west] at (0.15,3.55) {
- id: UUID \\
- text: String \\
- sender: Enum \\
- sentAt: DateTime
};
\end{tikzpicture}
\end{minipage}\hfill
\begin{minipage}[t]{0.62\textwidth}
\vspace{0pt}
Enregistre chaque échange du chatbot avec l'utilisateur, y compris le timestamp et le rôle de l'émetteur (énumération : USER ou BOT). La persistence de cet historique dépasse le simple besoin d'affichage : elle permet l'\textbf{audit transactionnel}, la \textbf{reconstruction du contexte multi-requête} (chaque nouvelle question peut s'appuyer sur l'historique), et les \textbf{analyses ultérieures} (métriques d'engagement, amélioration du système). Le timestamp \texttt{sentAt} est automatiquement défini au moment de la création.
\end{minipage}

\subsubsection{Endpoints clés}

Les endpoints REST d'OCULA sont organisés par domaine fonctionnel. Chaque groupe expose un ensemble cohérent d'opérations sur une ressource métier.

\paragraph{Arborescence des endpoints}

\begin{enumerate}
    \item \textbf{Authentification} (\textit{/auth})
    \begin{itemize}
        \item[\textbullet] \textbf{POST}
        \begin{itemize}
            \item \texttt{POST /auth/signup} -- Inscription. Accepte email, mot de passe et données profil. Retourne l'utilisateur persisté ou erreur si email déjà enregistré.
            \item \texttt{POST /auth/login} -- Connexion. Accepte email et mot de passe. Retourne JWT, durée d'expiration et infos utilisateur ou erreur en cas de credentials invalides.
        \end{itemize}
    \end{itemize}

    \item \textbf{Gestion des vidéos} (\textit{/videos})
    \begin{itemize}
        \item[\textbullet] \textbf{POST}
        \begin{itemize}
            \item \texttt{POST /videos/upload} -- Upload. Accepte fichier vidéo, miniature, titre, résumé et ID utilisateur. Crée les clés blob, stocke les fichiers, crée l'entité Video en base (statut PENDING), retourne les détails de la vidéo créée ou erreur.
            \item \texttt{POST /videos/process/\{id\}} -- Déclenchement IA. Enclenche le pipeline de transcription asynchrone via le service Flask. Retourne le statut de la tâche.
        \end{itemize}
        \item[\textbullet] \textbf{GET}
        \begin{itemize}
            \item \texttt{GET /videos/\{id\}} -- Lecture. Retourne les détails d'une vidéo ou erreur si absent.
            \item \texttt{GET /videos/page/\{userId\}} -- Récupère la liste paginée des vidéos d'un utilisateur.
        \end{itemize}
        \item[\textbullet] \textbf{PUT}
        \begin{itemize}
            \item \texttt{PUT /videos/update/\{id\}} -- Mise à jour. Modifie titre, résumé ou avatar. Format multipart. Retourne les détails actualisés de la vidéo ou erreur.
            \item \texttt{PUT /videos/update/status/\{id\}} -- Mise à jour de statut (interne). Endpoint appelé par le service IA pour signaler l'avancement (PROCESSING, COMPLETED, FAILED). Déclenche notification WebSocket.
        \end{itemize}
    \end{itemize}

    \item \textbf{Transcripts} (\textit{/video-transcripts})
    \begin{itemize}
        \item[\textbullet] \textbf{POST}
        \begin{itemize}
            \item \texttt{POST /video-transcripts} -- Persistance segment. Retourne l'entité créée ou erreur en cas de validation échouée (ex : endTime < startTime).
        \end{itemize}
    \end{itemize}

    \item \textbf{Messages (Chatbot)} (\textit{/messages})
    \begin{itemize}
        \item[\textbullet] \textbf{GET}
        \begin{itemize}
            \item \texttt{GET /messages/page/\{videoId\}} -- Historique paginé. Retourne les messages d'une vidéo. Erreur si vidéo absente.
        \end{itemize}
        \item[\textbullet] \textbf{POST}
        \begin{itemize}
            \item \texttt{POST /messages} -- Envoi message.
        \end{itemize}
    \end{itemize}

    \item \textbf{Stockage objet} (\textit{/files})
    \begin{itemize}
        \item[\textbullet] \textbf{GET}
        \begin{itemize}
            \item \texttt{GET /files/get-url?key=...} -- Génération d'URL signée. Accepte la clé blob, retourne une URL sécurisée.
        \end{itemize}
    \end{itemize}

    \item \textbf{Utilisateurs} (\textit{/users})
    \begin{itemize}
        \item[\textbullet] \textbf{PUT}
        \begin{itemize}
            \item \texttt{PUT /users/update/\{id\}} -- Profil. Mise à jour prénom, nom, email, mot de passe, avatar. Accepte multipart/form-data. Validation : email doit être unique, mot de passe doit respecter contraintes (min 8 caractères). Retourne les informations actualisées de l'utilisateur ou erreur.
        \end{itemize}
    \end{itemize}
\end{enumerate}

\subsubsection{Sécurité : authentification et autorisation}

La chaîne de sécurité du backend repose sur deux mécanismes complémentaires : authentification JWT pour les utilisateurs et vérification de clé API interne pour les callbacks du service IA.

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/Chaîne de sécurité backen.png}
\caption{Chaîne de sécurité backend : JWT utilisateur et authentification machine-à-machine}
\label{fig:backend-security-chain}
\end{figure}

\noindent Cette figure montre le flux de sécurité appliqué à chaque requête entrante. Comme l'illustre la figure~\ref{fig:backend-security-chain}, la chaîne de sécurité du backend implémente une stratégie de défense en profondeur avec deux sentinelles : validation de route, puis authentification (clé API interne ou JWT utilisateur). Chaque requête entrante est d'abord validée pour confirmer que la route cible existe. Si la route est publique (e.g., \path{/auth/signup}, \path{/auth/login}), la requête est directement acheminée vers le contrôleur approprié. En revanche, si la route est protégée, le système vérifie d'abord la présence d'une clé API interne (utilisée par le service IA pour les callbacks d'avancement). Si cette clé n'est pas valide, la requête est soumise à une vérification JWT : le token JWT fourni par le client est validé et décodé pour extraire l'identité utilisateur. Si le token est invalide ou expiré, une réponse 401 Unauthorized est retournée. Cette architecture garantit l'isolation sécurisée des appels machine-à-machine (backend $\leftrightarrow$ service IA) et protège les ressources utilisateur contre les accès non authentifiés.

\paragraph{Autorisation et isolation des ressources \\}

Au-delà de l'authentification, le backend implémente un contrôle d'accès granulaire basé sur l'\textbf{ownership}. Chaque ressource métier (vidéos, transcriptions, messages) est explicitement liée à un utilisateur propriétaire. Avant de retourner une ressource, le système effectue une \textbf{vérification d'appartenance} : il s'assure que l'identité utilisateur extraite du JWT correspond au propriétaire de la ressource demandée.

\subsubsection{Extraits de code importants des services}

Cette section présente les implémentations clés des services backend qui constituent le cœur fonctionnel de l'application OCULA.

\paragraph{Service d'authentification et gestion de tokens JWT \\}

Le service JWT génère et valide les tokens d'authentification, permettant les sessions sécurisées côté client.

\begin{lstlisting}[language=Java, caption={JwtService : génération et validation des tokens JWT},label={lst:jwt-service}]
public String generateToken(Map<String, Object> extraClaims, UserDetails userDetails) {
    return buildToken(extraClaims, userDetails, jwtExpiration);
}

private String buildToken(Map<String, Object> extraClaims, 
                         UserDetails userDetails, long expiration) {
    return Jwts.builder()
            .setClaims(extraClaims)
            .setSubject(userDetails.getUsername())
            .setIssuedAt(new Date(System.currentTimeMillis()))
            .setExpiration(new Date(System.currentTimeMillis() + expiration))
            .signWith(getSignInKey(), SignatureAlgorithm.HS256)
            .compact();
}

public boolean isTokenValid(String token, UserDetails userDetails) {
    final String username = extractUsername(token);
    return (username.equals(userDetails.getUsername())) && !isTokenExpired(token);
}
\end{lstlisting}

\noindent
\textbf{Description :} Le token JWT est construit avec les claims utilisateur, un timestamp d'émission, une date d'expiration et est signé avec une clé secrète HS256. La validation vérifie que l'utilisateur du token correspond à celui de la requête et que le token n'a pas expiré.

\paragraph{Authentification des utilisateurs \\}

Le service d'authentification gère l'inscription sécurisée et la connexion avec hachage de mot de passe.

\begin{lstlisting}[language=Java, caption={AuthenticationService : inscription et authentification},label={lst:auth-service}]
public User signup(RegisterUserDto input) throws UsedEmailException {
    if (userRepository.existsByEmail(input.getEmail())) {
        throw new UsedEmailException();
    }
    User user = new User();
    user.setFirstName(input.getFirstName());
    user.setLastName(input.getLastName());
    user.setEmail(input.getEmail());
    user.setPassword(passwordEncoder.encode(input.getPassword()));
    return userRepository.save(user);
}

public User authenticate(LoginUserDto input) throws InvalidParamsException {
    authenticationManager.authenticate(
            new UsernamePasswordAuthenticationToken(
                    input.getEmail(), input.getPassword()));
    User user = userRepository.findByEmail(input.getEmail()).orElse(null);
    if (user != null && checkPassword(input.getPassword(), user.getPassword())) {
        return user;
    } else {
        throw new InvalidParamsException();
    }
}
\end{lstlisting}

\noindent
\textbf{Description :} L'inscription vérifie d'abord l'unicité de l'email, crée un nouvel utilisateur et encode le mot de passe avant la sauvegarde. L'authentification utilise le gestionnaire Spring Security pour valider les credentials.

\paragraph{Gestion du stockage en blob Azure \\}

Le service BlobStorage encapsule toutes les opérations de fichiers avec Azure Blob Storage, incluant les URLs signées temporaires.

\begin{lstlisting}[language=Java, caption={BlobStorageService : upload et accès sécurisé aux fichiers},label={lst:blob-service}]
public void uploadFile(String fileName, InputStream data, long size) {
    BlobClient blobClient = containerClient.getBlobClient(fileName);
    blobClient.upload(data, size, true);
}

public String getFileUrl(String fileName) {
    BlobClient blobClient = containerClient.getBlobClient(fileName);
    OffsetDateTime expiryTime = OffsetDateTime.now().plusHours(6);
    BlobSasPermission permission = new BlobSasPermission().setReadPermission(true);
    BlobServiceSasSignatureValues values = new BlobServiceSasSignatureValues(expiryTime, permission)
            .setStartTime(OffsetDateTime.now().minusMinutes(5));
    return blobClient.getBlobUrl() + "?" + blobClient.generateSas(values);
}

public void deleteFile(String fileName) {
    BlobClient blobClient = containerClient.getBlobClient(fileName);
    blobClient.deleteIfExists();
}
\end{lstlisting}

\noindent
\textbf{Description :} L'upload stocke les fichiers directement dans un conteneur Azure. Les URLs de fichier incluent une signature SAS (Shared Access Signature) valide 6 heures, permettant l'accès temporaire sécurisé sans authentification. La suppression utilise \texttt{deleteIfExists()} pour éviter les erreurs.

\paragraph{Gestion des utilisateurs avec avatars \\}

Le service utilisateur encapsule la création, modification et suppression d'utilisateurs, avec gestion des avatars en stockage blob.

\begin{lstlisting}[language=Java, caption={UserService : création et mise à jour avec gestion des avatars},label={lst:user-service}]
public UserDto updateUser(UUID id, String avatarKey, UpdateUserDto userDto)
        throws UserNotFoundException, UsedEmailException {
    Optional<User> optionalUser = userRepository.findById(id);
    if (optionalUser.isEmpty()) {
        throw new UserNotFoundException();
    }
    User user = optionalUser.get();
    if (avatarKey != null) {
        if (user.getAvatar() != null) {
            this.blobStorageService.deleteFile(user.getAvatar());
        }
        user.setAvatar(avatarKey);
    }
    String newEmail = userDto.getEmail();
    if (newEmail != null && !newEmail.isBlank() && 
        !user.getEmail().equals(newEmail) && 
        userRepository.existsByEmail(newEmail)) {
        throw new UsedEmailException();
    }
    // mise à jour d'autres champs...
    userRepository.save(user);
    return userMapper.toDto(user);
}
\end{lstlisting}

\noindent
\textbf{Description :} Lors de la mise à jour, si un nouvel avatar est fourni, l'ancien est supprimé de Blob Storage. La modification d'email vérifie qu'il n'existe pas déjà en base de données.

\paragraph{Gestion des vidéos et statuts \\}

Le service vidéo offre des opérations CRUD complètes et permet de mettre à jour le statut de traitement des vidéos.

\begin{lstlisting}[language=Java, caption={VideoService : création avec upload et gestion de statuts},label={lst:video-service}]
public VideoDto createVideoWithUpload(String content, String thumbnail, 
        String title, String summary, UUID userId, Duration duration)
        throws UserNotFoundException {
    User user = userRepository.findById(userId).orElse(null);
    if (user == null) {
        throw new UserNotFoundException();
    }
    Video video = new Video();
    video.setTitle(title);
    video.setSummary(summary);
    video.setContent(content);
    video.setThumbnail(thumbnail);
    video.setUser(user);
    video.setDuration(duration);
    Video savedVideo = videoRepository.save(video);
    return videoMapper.toDto(savedVideo);
}

public VideoDto updateStatus(UUID id, VideoStatus videoStatus) 
        throws VideoNotFoundException {
    Video video = videoRepository.findById(id).orElse(null);
    if (video == null) {
        throw new VideoNotFoundException();
    }
    video.setStatus(videoStatus);
    videoRepository.save(video);
    return videoMapper.toDto(video);
}
\end{lstlisting}

\noindent
\textbf{Description :} La création avec upload associe directement une vidéo à un utilisateur et enregistre sa durée. L'update de statut permet de tracker la progression du traitement (PENDING, PROCESSING, COMPLETED, etc.).

\paragraph{Service de messages et chat avec IA \\}

Le service de messages gère la conversation bidirectionnelle entre l'utilisateur et l'IA, en sauvegardant les questions et réponses.

\begin{lstlisting}[language=Java, caption={MessageService : conversation avec l'IA},label={lst:message-service}]
public AskMessageResponse askQuestion(UUID videoId, String question) 
        throws VideoNotFoundException {
    MessageDto userInput = MessageDto.builder()
            .videoId(videoId)
            .sender(Sender.USER)
            .text(question)
            .build();
    MessageDto savedUserMessage = createMessage(userInput);

    ChatResponse aiResponse = aiService.ask(videoId.toString(), question);
    String answer = aiResponse != null ? aiResponse.getAnswer() : null;
    if (answer == null || answer.isBlank()) {
        answer = "I couldn't generate an answer from the available context.";
    }

    MessageDto botOutput = MessageDto.builder()
            .videoId(videoId)
            .sender(Sender.BOT)
            .text(answer)
            .build();
    MessageDto savedBotMessage = createMessage(botOutput);

    return new AskMessageResponse(savedUserMessage, savedBotMessage, 
            aiResponse != null ? aiResponse.getSources() : null);
}
\end{lstlisting}

\noindent
\textbf{Description :} La méthode orchestre l'interaction : elle crée un message utilisateur, appelle le service IA, gère les réponses vides, puis sauvegarde la réponse du bot avec les sources utilisées pour la génération.

\paragraph{Gestion des transcriptions vidéo \\}

Le service de transcription valide les segments temporels et gère les transcriptions avec tri automatique.

\begin{lstlisting}[language=Java, caption={VideoTranscriptService : création avec validation temporelle},label={lst:transcript-service}]
public VideoTranscriptDto createVideoTranscript(VideoTranscriptDto dto)
        throws VideoNotFoundException, InvalidTranscriptException {
    if (dto == null || dto.getVideoId() == null) {
        throw new InvalidTranscriptException("videoId is required");
    }
    if (!videoRepository.existsById(dto.getVideoId())) {
        throw new VideoNotFoundException();
    }
    validateTimes(dto.getStartTime(), dto.getEndTime());

    Video video = videoRepository.findById(dto.getVideoId()).orElse(null);
    VideoTranscript videoTranscript = videoTranscriptMapper.toEntity(dto);
    videoTranscript.setVideo(video);
    VideoTranscript savedVideoTranscript = videoTranscriptRepository.save(videoTranscript);
    return videoTranscriptMapper.toDto(savedVideoTranscript);
}

private void validateTimes(Double startTime, Double endTime) 
        throws InvalidTranscriptException {
    if (startTime == null || endTime == null) {
        throw new InvalidTranscriptException("startTime and endTime are required");
    }
    if (startTime < 0 || endTime < 0) {
        throw new InvalidTranscriptException("Times must be positive");
    }
    if (endTime < startTime) {
        throw new InvalidTranscriptException("endTime must be >= startTime");
    }
}
\end{lstlisting}

\noindent
\textbf{Description :} La validation s'effectue en trois étapes : vérification de la présence, positivité des timestamps et cohérence de l'ordre. Les transcriptions sont automatiquement triées par temps de début lors de la récupération.

\paragraph{Service d'intégration IA \\}

Le service IA encapsule les appels au microservice Python, en découplant le backend du service IA.

\begin{lstlisting}[language=Java, caption={AiService : communication avec le service IA Python},label={lst:ai-service}]
public ProcessResponse process(String videoId, String objectKey) {
    ProcessRequest request = new ProcessRequest();
    request.setVideo_id(videoId);
    request.setObject_key(objectKey);

    return aiServiceWebClient
            .post()
            .uri("/process")
            .bodyValue(request)
            .retrieve()
            .bodyToMono(ProcessResponse.class)
            .block();
}

public ChatResponse ask(String videoId, String question) {
    ChatRequest request = new ChatRequest();
    request.setVideo_id(videoId);
    request.setQuestion(question);

    return aiServiceWebClient
            .post()
            .uri("/chat")
            .bodyValue(request)
            .retrieve()
            .bodyToMono(ChatResponse.class)
            .block();
}
\end{lstlisting}

\noindent
\textbf{Description :} Le service utilise un \texttt{WebClient} Spring injecté pour faire des appels HTTP asynchrones au service Python. Le \texttt{.block()} convertit la réponse réactive Mono en résultat synchrone. Les deux endpoints clés sont \texttt{/process} pour l'indexation vidéo et \texttt{/chat} pour les requêtes conversationnelles.

\subsection{Service IA : LangChain + Flask}
Le traitement intelligent des vidéos est assuré par un microservice dédié développé en Python et exposé via Flask. 
Ce service prend en charge les opérations computationnelles intensives du système : transcription audio, génération d’embeddings, indexation vectorielle et mécanisme de question–réponse basé sur le RAG.

Contrairement au backend Spring Boot, qui gère les aspects transactionnels et la logique métier applicative, le service IA est spécialisé dans les traitements liés aux modèles de langage et à la recherche sémantique. 
Cette séparation permet d’isoler les dépendances liées aux modèles IA, de limiter l’impact des traitements lourds sur le backend principal et de faire évoluer indépendamment les composants.

L’orchestration interne repose sur LangChain, utilisé ici comme couche d’intégration entre les différents modules IA, et non comme framework applicatif complet. 
LangChain structure les interactions entre les embeddings, la base vectorielle et le modèle de langage externe.

Concrètement, son utilisation dans le service IA couvre :

\begin{itemize}
    \item \textbf{Embeddings} : la classe \path{HuggingFaceEmbeddings} 
    charge le modèle \path{sentence-transformers/all-MiniLM-L6-v2} afin de vectoriser les chunks avant leur stockage dans ChromaDB.
    
    \item \textbf{Base vectorielle} : \path{langchain\chroma.Chroma} sert d’interface avec ChromaDB pour l’ajout de documents (\path{add_documents}) et la recherche par similarité (\path{similarity_search_with_score}).
    
    \item \textbf{Prompting et chaining} : les différents prompts (titre, résumé, réécriture, RAG) sont définis via \path{PromptTemplate} et exécutés sous forme de pipeline :
    \begin{figure}[H]
    \centering
    \includegraphics[width=0.82\textwidth]{images/langchain-prompt-chain.png}
    \caption{Pipeline LangChain de prompting et chaining}
    \label{fig:langchain-prompt-chain}
    \end{figure}

    \item \textbf{Appel au LLM} : \path{ChatCerebras} permet d’interagir avec le modèle externe \path{gpt-oss-120b} pour la génération du titre, du résumé et des réponses RAG.
\end{itemize}

LangChain assure ainsi la cohérence du flux complet, reliant définition du prompt, appel au modèle, parsing des sorties et interaction avec la base vectorielle. 
Les sous-sections suivantes détaillent successivement les différentes phases du pipeline : transcription, chunking, génération d’embeddings et mécanisme RAG.


\subsubsection{Extraction audio et transcription (Whisper)}

Le déclenchement du traitement d’une vidéo est effectué via l’endpoint \path{/process} exposé par le service Flask. 
Afin d’éviter le blocage de la requête HTTP, le pipeline est exécuté dans un thread Python dédié. 
Cette approche permet au backend principal de rester réactif tout en assurant un traitement potentiellement long.

\paragraph{Chargement du modèle Whisper}

Le modèle Whisper est chargé une seule fois à l’initialisation du service IA :

\begin{lstlisting}[language=Python, caption={Chargement du modèle Whisper}, label={lst:whisper-load}]
    # ai_part/services/ai_models.py
    import whisper
    whisper_model = whisper.load_model("base")
\end{lstlisting}


Le choix du modèle \path{"base"} constitue un compromis entre précision de transcription et coût computationnel, adapté à une exécution locale.

\paragraph{Pipeline de traitement}

Lorsqu’une requête de traitement est reçue, la fonction principale orchestre les différentes étapes :

\begin{lstlisting}[language=Python, caption={Chargement du modèle Whisper}, label={lst:whisper-load}]
# ai_part/services/processing.py
def processing_task(video_id, object_key):
    try:
        update_status(video_id, "PROCESSING")

        temp_filename = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_file:
                temp_filename = temp_file.name
                print(f"Downloading blob: {object_key} to {temp_filename}")
                blob_client = container_client.get_blob_client(object_key)
                blob_data = blob_client.download_blob().readall()
                temp_file.write(blob_data)
                
            transcription = whisper_model.transcribe(temp_filename, verbose=True)
            save_transcript_chunk(video_id, transcription['segments'])
            embed_script(video_id, chunk_video_transcript(transcription))
            analyze(video_id, transcription['text'])

        finally:
            if temp_filename and os.path.exists(temp_filename):
                try:
                    os.remove(temp_filename)
                except Exception as e:
                    print(f"Warning: Could not remove temp file {temp_filename}: {e}")
        
        update_status(video_id, "COMPLETED")

    except Exception as e:
        print(f"Error in transcription task: {e}")
        update_status(video_id, "FAILED")
    pass
\end{lstlisting}

La vidéo est d’abord téléchargée depuis Azure Blob Storage à l’aide de sa clé d’objet (\path{object\key}) vers un fichier temporaire local. 
La transcription est ensuite effectuée via \path{whisper\model.transcribe}, qui retourne :

\begin{itemize}
    \item un texte global (\path{text}),
    \item une liste de segments contenant \path{start}, \path{end} et \path{text}.
\end{itemize}

Les segments sont persistés côté backend sous forme d’entités \textit{VideoTranscript}, conservant ainsi la granularité temporelle nécessaire à la traçabilité et au futur découpage en chunks.

Le statut de la vidéo est mis à jour au début (\path{PROCESSING}) puis à la fin du traitement (\path{COMPLETED}). 
En cas d’exception, le statut peut être positionné à \path{FAILED}, garantissant une gestion explicite du cycle de vie du traitement.

Le fichier temporaire est nettoyé systématiquement en fin de traitement (\path{finally}), même en cas d’erreur.

Cette gestion d’état et de ressources permet au frontend d’afficher l’avancement du traitement tout en assurant la cohérence et la robustesse du système dans un contexte distribué.

\subsubsection{Chunking : stratégies et paramètres}
\paragraph{Reconstruction des bornes temporelles des chunks} 
\mbox{}\\
Le découpage en chunks est réalisé sur le texte complet issu de Whisper (\path{transcription['text']}) via \path{RecursiveCharacterTextSplitter}. 
Cependant, pour préserver la traçabilité temporelle, chaque chunk doit être associé à un intervalle \path{[start, end]} exploitable comme source dans l’interface.

Pour cela, le service construit d’abord une correspondance (\textit{segment\_map}) entre la progression en caractères dans le texte global et les segments Whisper. 
Chaque segment est représenté par une borne \path{end\char} et ses timestamps \path{start/end}. 
Ensuite, pour chaque chunk extrait, sa position \path{[chunk\start, chunk\end]} dans le texte global est retrouvée, puis convertie en timestamps en recherchant les segments couvrant ces bornes.

Cette approche permet d’associer à chaque chunk un intervalle temporel cohérent, tout en conservant un découpage textuel adapté au retrieval.
\begin{lstlisting}[language=Python, caption={Découpage en chunks avec reconstruction des timestamps}, label={lst:chunk-video-transcript}]
def chunk_video_transcript(transcription, chunk_size=RAG_CHUNK_SIZE, chunk_overlap=RAG_CHUNK_OVERLAP):
    if not transcription:
        return []

    full_text = transcription['text']
    segments = transcription['segments']
    
    segment_map = []
    current_char = 0
    for seg in segments:
        length = len(seg['text'])
        segment_map.append({
            'end_char': current_char + length,
            'start': seg['start'],
            'end': seg['end']
        })
        current_char += length

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    text_chunks = splitter.split_text(full_text)
    
    result_chunks = []
    current_search_start = 0
    
    for chunk in text_chunks:
        chunk_start = full_text.find(chunk, current_search_start)
        if chunk_start == -1:
            chunk_start = full_text.find(chunk)
        if chunk_start == -1:
            continue
            
        chunk_end = chunk_start + len(chunk)
        current_search_start = chunk_start + 1
        
        start_time = segment_map[0]['start']
        for item in segment_map:
            if item['end_char'] > chunk_start:
                start_time = item['start']
                break
        
        end_time = segment_map[-1]['end']
        for item in segment_map:
            if item['end_char'] >= chunk_end:
                end_time = item['end']
                break
        
        result_chunks.append({
            'text': chunk,
            'start': start_time,
            'end': end_time
        })

    print("result_chunks", result_chunks)
    return result_chunks
\end{lstlisting}

\subsubsection{Génération d'embeddings (MiniLM)}

Après l’étape de chunking, chaque chunk est converti en représentation vectorielle afin de permettre une recherche sémantique efficace lors du mécanisme RAG. 
Le service IA utilise le modèle \path{sentence-transformers/all-MiniLM-L6-v2}, exécuté localement sur CPU, via l’intégration LangChain \path{HuggingFaceEmbeddings}.

\paragraph{Initialisation du modèle d'embedding}

Le modèle d’embedding est chargé une seule fois à l’initialisation du service IA, afin d’éviter une surcharge à chaque requête. 
Dans l’implémentation actuelle, les embeddings ne sont pas normalisés lors de l’encodage (\path{normalize\embeddings=False}), et l’exécution est configurée sur CPU.


\begin{lstlisting}[language=Python, caption={Initialisation du modèle d'embedding}, label={lst:embed-model-init}]
# ai_part/services/ai_models.py
from langchain_huggingface import HuggingFaceEmbeddings

model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': False}

embed_model = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)
\end{lstlisting}

\paragraph{Construction des documents vectoriels}

Chaque chunk est encapsulé dans un objet \path{Document} (LangChain) contenant :
(i) le texte du chunk (\path{page\content}), et (ii) les métadonnées nécessaires à la traçabilité : \path{video\id}, \path{start\time} et \path{end\time}. 
Ces métadonnées permettent notamment de restituer des sources temporelles associées aux réponses du chatbot.

\begin{lstlisting}[language=Python, caption={Création des documents avec métadonnées}, label={lst:doc-metadata}]
# ai_part/services/database.py
from langchain_core.documents import Document

doc = Document(
    page_content=chunk['text'],
    metadata={
        "video_id": video_id,
        "start_time": chunk['start'],
        "end_time": chunk['end'],
    }
)
\end{lstlisting}

\paragraph{Indexation dans ChromaDB}

Les documents vectoriels sont stockés dans ChromaDB via l’interface \path{langchain\chroma.Chroma}. 
L’indexation est organisée par vidéo : chaque vidéo possède une collection dédiée nommée \path{video\<video\id>}. 
Cette stratégie limite le bruit inter-vidéos et garantit que la recherche sémantique effectuée lors du chat reste strictement associée à la vidéo consultée.

\begin{lstlisting}[language=Python, caption={Indexation des documents dans une collection Chroma dédiée}, label={lst:chroma-index}]
# ai_part/services/database.py
from langchain_chroma import Chroma

collection_name = f"video_{str(video_id)}"

vector_store = Chroma(
    client=ai_models.db_client,
    collection_name=collection_name,
    embedding_function=ai_models.embed_model
)
vector_store.add_documents(docs_to_add)
\end{lstlisting}

\paragraph{Intégration au pipeline de traitement}

L’indexation des chunks intervient immédiatement après la transcription et le chunking, au sein du pipeline de traitement de la vidéo :

\begin{lstlisting}[language=Python, caption={Enchaînement transcription $\rightarrow$ chunking $\rightarrow$ embeddings}, label={lst:pipeline-embed}]
# ai_part/services/processing.py
transcription = whisper_model.transcribe(temp_filename, verbose=True)
chunks = chunk_video_transcript(transcription)
embed_script(video_id, chunks)
\end{lstlisting}

Ainsi, à l’issue du traitement, chaque vidéo est associée à une collection Chroma contenant les chunks vectorisés et leurs métadonnées temporelles, base nécessaire au retrieval utilisé dans la phase RAG.

\subsubsection{RAG : retrieval, filtrage et construction du contexte}

Le mécanisme RAG (Retrieval-Augmented Generation) constitue le cœur du système de question–réponse. 
Il permet de générer une réponse contextualisée à partir des chunks précédemment indexés dans ChromaDB.

\paragraph{Réécriture optionnelle de la requête}

Avant la phase de retrieval, le service IA peut appliquer une réécriture de la question utilisateur via la fonction \texttt{rewrite\_query}. 
Cette étape est activée uniquement si le paramètre \texttt{RAG\_ENABLE\_QUERY\_REWRITE} est positionné à \texttt{True}.

L’objectif est d’améliorer la qualité du retrieval en reformulant la question sous une forme plus descriptive ou plus adaptée à la recherche sémantique.

Afin de garantir un comportement stable, plusieurs contraintes sont appliquées :
\begin{itemize}
    \item normalisation sur une seule ligne (suppression des retours multiples),
    \item limitation de la longueur via \texttt{RAG\_QUERY\_REWRITE\_MAX\_CHARS},
    \item retour à la question originale en cas d’échec ou de sortie vide.
\end{itemize}

Cette étape constitue une optimisation contrôlée du pipeline et n’altère jamais la requête initiale en cas d’erreur.
\paragraph{Retrieval vectoriel et filtrage par distance}

La recherche est effectuée exclusivement dans la collection associée à la vidéo (\texttt{video\_<video\_id>}) afin de garantir l’isolation des contenus.

\begin{lstlisting}[language=Python, caption={Retrieval et filtrage par distance}, label={lst:rag-core}]
vector_store = Chroma(
    client=ai_models.db_client,
    collection_name=f"video_{video_id}",
    embedding_function=ai_models.embed_model
)

doc_scores = vector_store.similarity_search_with_score(
    retrieval_question,
    k=RAG_TOP_K
)

filtered = [
    (doc, distance)
    for doc, distance in doc_scores
    if float(distance) <= RAG_MAX_DISTANCE
]

if not filtered:
    fallback_count = max(1, min(RAG_FALLBACK_TOP_K, len(doc_scores)))
    filtered = doc_scores[:fallback_count]
\end{lstlisting}

Le score retourné est traité comme une \textbf{distance} : plus la distance est faible, plus le document est pertinent.  
Un seuil configurable (\texttt{RAG\_MAX\_DISTANCE}) permet d’exclure les passages trop éloignés sémantiquement. 
En l’absence de résultat satisfaisant, un mécanisme de repli conserve les meilleurs éléments du top-k initial.
Le diagramme suivant présente la séquence complète d’exécution du pipeline RAG lors d’une requête utilisateur. 
Il met en évidence les interactions entre le backend, le service IA, la base vectorielle et le modèle de langage.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/rag_seq.png}
    \caption{Séquence d’exécution du pipeline RAG}
    \label{fig:rag-sequence}
\end{figure}
Le pipeline peut être décomposé en plusieurs étapes successives.
\paragraph{Reranking hybride lexical et sémantique}

Après la phase de retrieval initiale, le service IA peut appliquer un mécanisme de reranking via la fonction \texttt{rerank\_context}, si le paramètre \texttt{RAG\_ENABLE\_RERANK} est activé.

Contrairement à une simple sélection par distance, cette étape combine deux signaux complémentaires :

\begin{itemize}
    \item \textbf{Similarité sémantique} : dérivée de la distance retournée par Chroma, transformée en score via la formule $1 / (1 + distance)$. Cette transformation garantit une valeur comprise dans $(0,1]$, où une distance faible produit une similarité élevée.
    
    \item \textbf{Recouvrement lexical} : proportion de termes communs entre la question et le contenu du chunk. Les tokens sont extraits via une tokenisation par expression régulière, avec suppression d’un ensemble de stopwords (anglais et français).
\end{itemize}

Le score final est calculé comme une combinaison pondérée :

\[
\text{score}_{final} =
\alpha \cdot \text{similarité}_{sémantique}
+
(1-\alpha) \cdot \text{recouvrement}_{lexical}
\]

où $\alpha$ correspond au paramètre \texttt{RAG\_RERANK\_SEMANTIC\_WEIGHT}.

Les documents sont ensuite triés par score décroissant et seuls les \texttt{RAG\_RERANK\_TOP\_N} meilleurs passages sont conservés.

Cette approche hybride permet de corriger certaines limites du retrieval purement vectoriel, notamment lorsque la similarité sémantique ne capture pas explicitement des mots-clés importants présents dans la question.




\paragraph{Construction du contexte}

Après le reranking éventuel, les documents retenus sont transformés en un contexte structuré via la fonction \texttt{format\_context}.
Chaque chunk est préfixé par une balise temporelle stable.

\begin{lstlisting}[language=Python, caption={Construction du contexte annoté}]
def format_context(doc_scores):
    parts = []
    sources = []

    for doc, score in doc_scores:
        metadata = doc.metadata or {}
        start = round(float(metadata.get("start_time", 0)), 2)
        end = round(float(metadata.get("end_time", 0)), 2)
        text = doc.page_content or ""

        parts.append(f"[{start}-{end}] {text}")
        sources.append({
            "start_time": start,
            "end_time": end,
            "text": text,
            "score": float(score)
        })

    return "\n\n".join(parts), sources
\end{lstlisting}

Les timestamps sont arrondis afin de garantir un format stable, facilitant la détection ultérieure des citations.
\paragraph{Appel au modèle de langage}

Le contexte ainsi construit est injecté dans le prompt RAG via une chaîne LangChain.

\begin{lstlisting}[language=Python, caption={Génération de la réponse via LangChain}]
chain = rag_prompt | ai_models.llm | StrOutputParser()

answer = chain.invoke({
    "question": question,
    "context": context_text
})
\end{lstlisting}

Le modèle reçoit la question originale ainsi que le contexte concaténé issu des chunks sélectionnés.
\paragraph{Validation des citations et filtrage des sources}

Après génération, deux post-traitements garantissent la cohérence et l’explicabilité du système.

\begin{lstlisting}[language=Python, caption={Insertion automatique de citations}]
answer = ensure_citations(answer, sources)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption={Filtrage des sources réellement utilisées}]
used_sources = filter_sources_by_answer(answer, sources)
\end{lstlisting}

Si aucun contexte exploitable n’est disponible, la réponse de repli est :

\begin{lstlisting}[language=Python]
if not context_text:
    return {
        "answer": "I don't have enough context from this video yet.",
        "sources": []
    }
\end{lstlisting}

La réponse finale retournée au backend contient \texttt{answer} et \texttt{sources}, encapsulés ensuite dans un objet \texttt{AskMessageResponse}.

La structure retournée par le service IA contient donc la réponse générée ainsi que la liste des sources associées. 
Le backend encapsule ensuite cette réponse dans un objet \texttt{AskMessageResponse} transmis au frontend.

\medskip

Le comportement du pipeline RAG repose sur un ensemble de paramètres configurables permettant d’ajuster finement la stratégie de récupération, de filtrage et de reranking. 
Ces paramètres sont définis au niveau du service IA et peuvent être adaptés selon les besoins en précision, robustesse ou performance.

\begin{table}[H]
\centering
\begin{tabular}{|l|c|l|}
\hline
Paramètre & Type & Rôle \\
\hline
RAG\_TOP\_K & int & Nombre initial de passages récupérés \\
RAG\_MAX\_DISTANCE & float & Seuil maximal de distance acceptable \\
RAG\_FALLBACK\_TOP\_K & int & Nombre conservé si seuil trop strict \\
RAG\_ENABLE\_QUERY\_REWRITE & bool & Activation réécriture requête \\
RAG\_ENABLE\_RERANK & bool & Activation reranking \\
RAG\_RERANK\_SEMANTIC\_WEIGHT & float & Pondération sémantique \\
RAG\_RERANK\_TOP\_N & int & Nombre final de passages retenus \\
\hline
\end{tabular}
\caption{Paramètres configurables du pipeline RAG}
\label{tab:rag-parameters-final}
\end{table}

Cette paramétrisation offre une flexibilité importante mais introduit également certaines limites. 
En particulier, le reranking lexical repose sur une tokenisation simple basée sur des expressions régulières. 
Cette approche reste sensible aux variations linguistiques, aux synonymes et aux différences morphologiques, ce qui constitue une limite identifiée du mécanisme hybride.





\subsubsection{Appel au LLM externe et post-traitements}

La génération des sorties textuelles repose sur un modèle de langage externe intégré via LangChain. 
Le service IA utilise l’interface \texttt{ChatCerebras} pour interagir avec le modèle \texttt{gpt-oss-120b}.

\paragraph{Configuration du modèle}

Le modèle est configuré avec des paramètres favorisant la stabilité et la précision :

\begin{itemize}
    \item \textbf{temperature = 0.1} afin de limiter la variabilité,
    \item \textbf{max\_tokens = 2048} pour encadrer la longueur maximale de génération.
\end{itemize}

Ce paramétrage est cohérent avec un système de question–réponse fondé sur un contexte contraint, où la fidélité au transcript prime sur la créativité.


Le modèle ne reçoit que le contexte explicitement construit lors de la phase RAG, ce qui limite les risques d’hallucination hors contexte.

\paragraph{Génération du titre et du résumé}

Le même mécanisme d’appel LLM est réutilisé pour enrichir automatiquement les métadonnées de la vidéo après transcription complète.  
Deux prompts dédiés sont définis (\texttt{title\_prompt} et \texttt{summary\_prompt}) et exécutés via la même structure de chaîne LangChain :

\begin{lstlisting}[language=Python, caption={Génération LLM du titre et du résumé}]
title_chain = title_prompt | ai_models.llm | StrOutputParser()
summary_chain = summary_prompt | ai_models.llm | StrOutputParser()

title = title_chain.invoke({"text": transcript})
summary = summary_chain.invoke({"text": transcript})
\end{lstlisting}

Les résultats sont ensuite persistés côté backend via une mise à jour de l’entité \textit{Video}, notamment des champs \texttt{title} et \texttt{summary}. 
Cette séparation garantit que le service IA ne manipule pas directement la base relationnelle.

\paragraph{Post-traitements et robustesse}

Après génération d’une réponse RAG, deux mécanismes assurent la cohérence :

\begin{itemize}
    \item insertion automatique de citations si aucune n’est détectée,
    \item filtrage des sources effectivement utilisées dans la réponse.
\end{itemize}

En cas d’absence de contexte exploitable, une réponse explicite est renvoyée :

\begin{lstlisting}[language=Python]
"I don't have enough context from this video yet."
\end{lstlisting}

La structure finale renvoyée au backend contient \texttt{answer} et \texttt{sources}. 
Le backend encapsule ensuite cette réponse dans un objet \texttt{AskMessageResponse} avant transmission au frontend.

\subsection{Stockage et Dockerisation}

\subsubsection{Stockage vidéo et fichiers (Azure Blob Storage)}

Notre application Ocula utilise le service Azure Blob Storage pour stocker l’ensemble des fichiers importés par les utilisateurs. Ceux-ci peuvent notamment enregistrer leur avatar, les vidéos à traiter ainsi que les miniatures (thumbnails) extraites de ces vidéos.

\begin{lstlisting}
	public void uploadFile(String fileName, InputStream data, long size) {
		BlobClient blobClient = containerClient.getBlobClient(fileName);
		blobClient.upload(data, size, true);
	}
\end{lstlisting}

La fonction \textbf{uploadFile} permet d’envoyer un fichier vers Azure Blob Storage. Elle utilise le nom du fichier comme clé unique pour récupérer un client Azure (BlobClient), puis elle transfère les données du fichier vers le conteneur de stockage. Si un fichier portant le même nom existe déjà, il est remplacé. Cette méthode sert donc à stocker physiquement les fichiers des utilisateurs dans le cloud.

\begin{lstlisting}
	@PostMapping("/upload")
	public ResponseEntity<VideoDto> createVideoWithUpload(
	@RequestParam("video") MultipartFile video,
	@RequestParam("thumbnail") MultipartFile thumbnail,
	@RequestParam("title") String title,
	@RequestParam("summary") String summary,
	@RequestParam("userId") UUID userId,
	@RequestParam("duration") String durationSeconds) throws IOException, UserNotFoundException {
		
		Duration duration = Duration.ofSeconds(Long.parseLong(durationSeconds));
		
		String videoKey = userId.toString() + "/videos/" + UUID.randomUUID().toString()
		+ video.getOriginalFilename().replaceAll("[^a-zA-Z0-9._-]", "_");
		String thumbnailKey = userId.toString() + "/thumbnails/" + UUID.randomUUID().toString()
		+ thumbnail.getOriginalFilename().replaceAll("[^a-zA-Z0-9._-]", "_");
		
		blobStorageService.uploadFile(videoKey, video.getInputStream(), video.getSize());
		blobStorageService.uploadFile(thumbnailKey, thumbnail.getInputStream(), thumbnail.getSize());
		
		VideoDto createdVideo = videoService.createVideoWithUpload(videoKey, thumbnailKey, title, summary, userId,
		duration);
		
		return ResponseEntity.ok(createdVideo);
	}
\end{lstlisting}

La méthode \textbf{createVideoWithUpload} sert à gérer tout le processus d’upload côté application. Elle reçoit les fichiers envoyés par l’utilisateur (vidéo et miniature), génère des clés uniques pour organiser leur stockage dans Azure Blob Storage, puis appelle le service d’upload pour les enregistrer. Enfin, elle sauvegarde dans la base de données les informations liées à la vidéo ainsi que les chemins des fichiers stockés.

Pour accéder à l’un de ces fichiers, nous utilisons une clé unique qui permet de générer une URL temporaire et sécurisée, garantissant un accès contrôlé aux ressources.

\begin{lstlisting}
	public String getFileUrl(String fileName) {
		BlobClient blobClient = containerClient.getBlobClient(fileName);
		
		OffsetDateTime expiryTime = OffsetDateTime.now().plusHours(6);
		BlobSasPermission permission = new BlobSasPermission().setReadPermission(true);
		
		BlobServiceSasSignatureValues values = new BlobServiceSasSignatureValues(expiryTime, permission)
		.setStartTime(OffsetDateTime.now().minusMinutes(5));
		
		return blobClient.getBlobUrl() + "?" + blobClient.generateSas(values);
	}
\end{lstlisting}

La fonction \textbf{getFileUrl} permet de générer un lien sécurisé pour accéder à un fichier stocké dans Azure Blob Storage. Elle crée un token SAS (Shared Access Signature) qui donne une permission de lecture limitée dans le temps. L’URL finale contient ce token, ce qui permet d’accéder temporairement au fichier sans exposer les informations sensibles du compte Azure.

\subsubsection{Base de données}

Notre application Ocula repose sur une architecture hybride utilisant deux types de bases de données complémentaires : une base de données relationnelle \textbf{PostgreSQL} et une base de données vectorielle \textbf{ChromaDB}. Ce choix permet de répondre aux différents besoins de stockage et de traitement des données de l’application.

La base de données \textbf{PostgreSQL} est utilisée pour stocker les données structurées de l’application. Elle contient notamment les informations relatives aux utilisateurs, aux vidéos importées, les messages et les transcriptions détectée.

En complément, \textbf{ChromaDB} est utilisée comme base de données vectorielle afin de gérer les représentations vectorielle de la transcription de vidéo. Ces vecteurs sont stockés afin de les exploiter dans les traitements de RAG.

L’utilisation combinée de ces deux bases permet ainsi de séparer les responsabilités : \textbf{PostgreSQL} assure la gestion des données structurées, tandis que \textbf{ChromaDB} prend en charge le stockage et l’interrogation des données vectorielles nécessaires aux fonctionnement d’intelligence artificielle de l’application.

\subsubsection{Dockerisation}

L'ensemble de l'application \textbf{OCULA} est conteneurisé à l'aide de \textbf{Docker} et orchestré avec \textbf{Docker Compose}. Cette approche garantit la portabilité de l'application, l'isolation des environnements et une gestion simplifiée des dépendances entre les services.

\begin{lstlisting}
	FROM maven:3.9.9-eclipse-temurin-21 AS build
	WORKDIR /app
	
	COPY pom.xml .
	COPY .mvn .mvn
	COPY mvnw mvnw
	
	RUN mvn -q -DskipTests dependency:go-offline
	
	COPY src src
	RUN mvn -q -DskipTests package
	
	FROM eclipse-temurin:21-jre
	WORKDIR /app
	
	COPY --from=build /app/target/*.jar /app/app.jar
	
	EXPOSE 8080
	
	ENTRYPOINT ["java", "-jar", "/app/app.jar"]
\end{lstlisting}

Ce Dockerfile permet d'exécuter le backend d'Ocula. La première étape utilise Maven pour télécharger les dépendances, compiler le code source et générer un fichier JAR. La seconde étape utilise une image contenant le Java, copie le JAR généré, expose le port 8080 et lance automatiquement l’application avec la commande java -jar.

\begin{lstlisting}
	FROM python:3.10-slim
	
	ENV PYTHONDONTWRITEBYTECODE=1 \
	PYTHONUNBUFFERED=1 \
	VIRTUAL_ENV=/opt/venv \
	PATH="/opt/venv/bin:$PATH"
	
	WORKDIR /app
	
	COPY requirements.txt /app/requirements.txt
	RUN apt-get update && apt-get install -y ffmpeg \
	&& rm -rf /var/lib/apt/lists/* \
	&& python -m venv /opt/venv \
	&& pip install --upgrade pip \
	&& pip install -r /app/requirements.txt
	
	COPY . /app
	
	EXPOSE 5000
	ENV PORT=5000
	
	CMD ["python", "app.py"]
\end{lstlisting}

Ce Dockerfile permet d'exécuter la partie IA d'Ocula. Il utilise une image de Python 3.10, configure des variables d’environnement, puis crée un environnement virtuel dans lequel il installe les dépendances listées dans le fichier requirements.txt, ainsi que l’outil ffmpeg nécessaire au fonctionnement de Whisper. Ensuite, il copie le code source dans le conteneur, expose le port 5000 et lance automatiquement l’application avec la commande python app.py.

\begin{lstlisting}
	FROM node:20-alpine AS build
	
	WORKDIR /app
	
	COPY package*.json ./
	RUN npm install
	
	COPY . .
	RUN npm run build 
	
	FROM nginx:1.27-alpine
	
	COPY nginx.conf /etc/nginx/conf.d/default.conf
	COPY --from=build /app/dist/ocula-frontend/browser /usr/share/nginx/html
	
	EXPOSE 80
	
	CMD ["nginx", "-g", "daemon off;"]
\end{lstlisting} 

Ce Dockerfile sert à construire et lancer le frontend de l’application. Il utilise d’abord Node.js pour installer les dépendances du projet, puis il utilise Nginx pour servir les fichiers générés. Enfin, il expose le port 80 et démarre le serveur Nginx pour rendre l’application accessible.


\section{Protocole de test}

\subsection{Jeu de vidéos utilisé}
Notre application accepte plusieurs formats vidéo couramment utilisés, notamment : MP4, MOV, AVI, WebM et MKV. Cette diversité permet d’assurer la compatibilité avec la majorité des sources vidéo utilisées par les utilisateurs.

La taille maximale des vidéos a été fixée à 150~Mo afin de garantir le bon fonctionnement du système. Cette limite permet de respecter les contraintes du stockage Azure Blob Storage, de maintenir des performances acceptables lors des traitements IA exécutés en local (transcription, embeddings), 
et de contrôler la consommation de tokens de LLM utilisé pour la génération de texte.

Le jeu de test comprenait des vidéos présentant des caractéristiques variées :

\begin{itemize}
	\item différentes durées (courtes, moyennes et longues) 
	\item plusieurs langues et accents 
	\item différents niveaux de qualité audio
	\item divers domaines de contenu
\end{itemize}

\subsection{Scénarios de test} 

\paragraph{Test d'importation de vidéo}

Ce test vise à vérifier le bon fonctionnement du processus d’upload des vidéos. 
Il consiste à téléverser des fichiers de formats différents (MP4, MOV, AVI, WebM, MKV) 
et de tailles variées, tout en contrôlant les points suivants :

\begin{itemize}
	\item validation du format et de la taille maximale autorisée ;
	\item enregistrement correct des métadonnées dans la base de données ;
	\item stockage réussi du fichier dans le système de stockage objet ;
	\item génération correcte de la clé d’accès et de l’URL sécurisée ;
	\item mise à jour du statut initial de la vidéo (PENDING).
\end{itemize}

\paragraph{Test de transcription de vidéo}

Ce test permet d’évaluer le pipeline de traitement automatique après l’importation. 
Il consiste à lancer l’analyse d’une vidéo et à vérifier les étapes suivantes :

\begin{itemize}
	\item extraction correcte de l’audio à partir de la vidéo ;
	\item génération d’une transcription cohérente et segmentée ;
	\item découpage du texte en chunks exploitables ;
	\item création et stockage des embeddings dans la base vectorielle ;
	\item mise à jour progressive des statuts de traitement jusqu’à COMPLETED.
\end{itemize}


\section{Conformité et considérations éthiques}

\subsection{Propriété intellectuelle et droits sur les contenus vidéo}

Chaque auteur dispose de droits pour protéger son œuvre, comprenant un droit moral et un droit patrimonial. Le droit moral permet à l’auteur de protéger son nom ainsi que l’intégrité de son œuvre contre toute exploitation. Le droit patrimonial, qui dure jusqu'à 70 ans après le décès de l’auteur, lui permet d’en tirer un profit\cite{associations_gouv_droit_image}. 

Chaque personne dispose d’un droit exclusif sur son image (photo ou vidéo) ainsi que sur son utilisation. Elle peut s’opposer à l’utilisation de cette image sans son autorisation.

Pour diffuser une vidéo dans notre application Ocula il faudrait obtenir\cite{associations_gouv_droit_image}:

\begin{itemize}
	\item une autorisation de la personne concernée.
	\item une autorisation écrite si la personne est reconnaissable dans la vidéo.
	\item l’accord de l’auteur de la vidéo au titre du droit d’auteur. 
\end{itemize}

\subsection{Limites et usages responsables}

L’utilisation de contenus dans l’application Ocula doit respecter certaines limites afin de garantir un usage responsable et conforme à la loi. Il est notamment interdit de diffuser des images ou des vidéos portant atteinte à la vie privée, à la dignité ou à la réputation des personnes concernées. Toute utilisation doit être faite avec le consentement préalable des individus identifiables et dans le respect du droit d’auteur.

L’utilisation de l’application Ocula est strictement encadrée afin de garantir un usage responsable et éthique. Il est notamment interdit de téléverser, diffuser ou partager des contenus inappropriés, tels que ceux portant contenant des propos violents, haineux, discriminatoires ou illégaux.

\section{Conclusion et perspectives}

\subsection{Bilan du projet}

Le projet OCULA avait pour objectif de transformer une vidéo en une ressource intelligemment exploitable, capable d’être synthétisée et interrogée en langage naturel. 
Au-delà de la simple implémentation technique, ce projet nous a confrontés à des problématiques concrètes liées au traitement du langage, à l’architecture logicielle et à l’intégration de modèles d’intelligence artificielle dans un système complet.

Sur le plan fonctionnel, nous avons conçu un pipeline cohérent permettant :
\begin{itemize}
    \item la transcription automatique d’une vidéo,
    \item la génération d’un titre et d’un résumé pertinents,
    \item l’indexation sémantique du transcript,
    \item l’interrogation contextualisée via un mécanisme RAG,
    \item la restitution de réponses accompagnées de références temporelles.
\end{itemize}

Sur le plan architectural, le projet nous a amenés à structurer une application distribuée en trois couches distinctes (frontend, backend, service IA), à gérer un traitement asynchrone avec suivi d’état explicite, et à assurer une séparation claire des responsabilités entre les composants.

D’un point de vue technique, nous avons approfondi plusieurs concepts majeurs :
\begin{itemize}
    \item fonctionnement des embeddings et recherche vectorielle,
    \item intégration d’un modèle de langage externe via LangChain,
    \item conception d’un pipeline Retrieval-Augmented Generation robuste,
    \item gestion des seuils, du reranking et des mécanismes de fallback,
    \item mise en place de mécanismes d’explicabilité via citations temporelles.
\end{itemize}

Le projet nous a également confrontés aux limites concrètes des modèles de langage, notamment le risque d’hallucination, la dépendance à la qualité de la transcription et l’importance du contrôle du contexte fourni au modèle.

Au-delà des aspects techniques, OCULA nous a permis de mieux comprendre les enjeux liés à la conception d’un système intelligent complet : 
il ne s’agit pas uniquement d’utiliser un modèle performant, mais d’orchestrer correctement les données, les flux et les contraintes afin de garantir robustesse, pertinence et explicabilité.
\subsection{Améliorations techniques possibles}

Bien que le système OCULA soit pleinement fonctionnel dans son périmètre actuel, plusieurs axes d’amélioration technique peuvent être envisagés afin d’en accroître la performance, la scalabilité et les capacités d’analyse.

Ces évolutions concernent principalement l’optimisation du pipeline de traitement, l’enrichissement multimodal du contenu analysé ainsi que l’adaptation de l’infrastructure à un contexte de déploiement à plus grande échelle.

Les sous-sections suivantes présentent les principales pistes identifiées.
\subsubsection{Streaming transcription}

Dans la version actuelle d’OCULA, la transcription est réalisée de manière batch : la vidéo est d’abord entièrement uploadée, puis traitée dans son intégralité avant que les résultats ne soient disponibles. 
Cette approche présente l’avantage de la simplicité et de la robustesse, mais elle introduit une latence perceptible pour l’utilisateur, notamment pour des vidéos longues.

Une évolution naturelle du système consisterait à intégrer un mécanisme de transcription en streaming. 
Dans ce modèle, l’audio serait traité progressivement au fur et à mesure de sa réception ou de son extraction, permettant une génération incrémentale du transcript.

Une telle approche offrirait plusieurs avantages :
\begin{itemize}
    \item réduction significative du temps d’attente avant disponibilité des premiers résultats,
    \item possibilité d’indexation progressive des chunks,
    \item ouverture vers une interrogation quasi temps réel du contenu.
\end{itemize}

Cependant, cette évolution impliquerait des adaptations architecturales importantes, notamment en matière de gestion d’état, de synchronisation entre transcription partielle et indexation vectorielle, ainsi que de gestion des erreurs sur des flux continus. 
Elle nécessiterait également une réflexion sur la cohérence des timestamps et sur la mise à jour dynamique des collections vectorielles.

Le streaming transcription constitue ainsi une perspective d’amélioration ambitieuse, orientée vers une expérience utilisateur plus fluide et interactive.
\subsubsection{Analyse de vidéo frame par frame}

Dans sa version actuelle, OCULA repose exclusivement sur l’analyse du contenu audio via la transcription automatique. 
Cette approche permet d’exploiter efficacement les informations verbalisées, mais elle ignore les éléments visuels présents dans la vidéo.

Or, de nombreuses vidéos contiennent des informations cruciales qui ne sont pas exprimées oralement : 
diapositives de présentation, schémas, équations, captures d’écran, démonstrations visuelles ou textes affichés à l’écran. 
Un système fondé uniquement sur le transcript peut ainsi manquer une partie significative du contenu réel.

Une évolution vers une analyse \textit{frame par frame} permettrait d’intégrer une dimension multimodale au système. 
Cette approche pourrait inclure :
\begin{itemize}
    \item l’extraction périodique d’images clés (key frames),
    \item la reconnaissance optique de caractères (OCR) pour détecter le texte affiché,
    \item la détection d’objets ou d’éléments visuels pertinents,
    \item l’association temporelle entre contenu visuel et contenu audio.
\end{itemize}

L’intégration de ces informations dans le pipeline d’indexation nécessiterait une adaptation du modèle de données ainsi qu’un enrichissement des métadonnées stockées dans la base vectorielle. 
Les embeddings pourraient alors représenter non seulement du texte issu du transcript, mais également des descriptions générées à partir du contenu visuel.

Une telle extension transformerait OCULA en un système véritablement multimodal, capable de répondre à des questions portant sur des éléments visibles à l’écran, et non uniquement sur ce qui est prononcé. 
Elle représenterait toutefois un défi significatif en termes de coût de calcul, de stockage et de complexité algorithmique.
\subsubsection{Infrastructure}

L’architecture actuelle d’OCULA repose sur une séparation claire entre frontend, backend et service IA, déployés sous forme de conteneurs distincts. 
Cette organisation est adaptée à un contexte académique et à une charge modérée, mais plusieurs évolutions seraient nécessaires pour envisager un déploiement à plus grande échelle.

Tout d’abord, la base vectorielle pourrait être optimisée afin de mieux supporter un volume important de vidéos et de requêtes simultanées. 
Une distribution de l’index vectoriel, l’utilisation d’index plus avancés ou l’intégration d’un moteur spécialisé pour la recherche à grande échelle permettraient d’améliorer la performance et la latence.

Ensuite, le traitement asynchrone des vidéos pourrait être externalisé vers une file de messages (message broker), permettant une meilleure gestion des tâches longues et une meilleure résilience en cas de montée en charge. 
Cette approche faciliterait également la mise en place d’un système de workers dédiés au traitement IA.

L’orchestration des conteneurs pourrait également évoluer vers une solution de type Kubernetes, offrant des mécanismes de réplication automatique, de supervision et de tolérance aux pannes.

Enfin, l’ajout d’outils de monitoring et de centralisation des logs permettrait d’améliorer l’observabilité du système. 
Dans un contexte de production, le suivi des performances du pipeline RAG, des temps de réponse du LLM et des taux d’erreur deviendrait essentiel pour garantir la qualité du service.

Ces améliorations renforceraient la scalabilité, la robustesse et la maintenabilité du système, tout en préparant une éventuelle industrialisation.
\subsection{Ouverture produit : industrialisation}

Au-delà de son cadre académique, OCULA présente un potentiel d’industrialisation significatif. 
La capacité à transformer automatiquement une vidéo en ressource interrogeable et synthétisée répond à un besoin croissant dans de nombreux secteurs.

Plusieurs domaines pourraient bénéficier d’un tel système :
\begin{itemize}
    \item plateformes de formation en ligne souhaitant rendre leurs cours plus interactifs,
    \item entreprises cherchant à valoriser des enregistrements internes (réunions, conférences, webinaires),
    \item institutions académiques souhaitant indexer des archives audiovisuelles,
    \item plateformes de diffusion de contenu éducatif ou technique.
\end{itemize}

Toutefois, le passage d’un prototype académique à un produit industriel impliquerait plusieurs adaptations majeures.

Sur le plan technique, il serait nécessaire de renforcer :
\begin{itemize}
    \item la gestion de la montée en charge et la distribution des services,
    \item la sécurisation des données et la conformité réglementaire,
    \item l’optimisation des coûts liés aux appels aux modèles de langage externes,
    \item la supervision continue des performances du système.
\end{itemize}

Sur le plan fonctionnel, une industrialisation impliquerait également :
\begin{itemize}
    \item une amélioration de l’interface utilisateur,
    \item une personnalisation des paramètres d’analyse,
    \item une gestion fine des droits d’accès et des rôles,
    \item une adaptation à différents contextes linguistiques et sectoriels.
\end{itemize}

Ainsi, OCULA constitue non seulement une démonstration technique cohérente, mais également une base conceptuelle solide pour le développement futur d’outils d’exploration intelligente de contenus vidéo à grande échelle \cite{ionos_asr_guide}.

\newpage
\section*{ANNEXES}
\addcontentsline{toc}{section}{ANNEXES}

\section*{Lexique}
\addcontentsline{toc}{section}{Lexique}

\begin{description}
	\item[\textbf{ASR \textit{(Automatic Speech Recognition)}}] : Technologie de reconnaissance automatique de la parole convertissant un signal audio en texte écrit.
	
	\item[\textbf{Azure Blob Storage}] : Service de stockage objet utilisé pour conserver les fichiers volumineux tels que les vidéos, les miniatures et les avatars de manière sécurisée.
	
	\item[\textbf{ChromaDB}] : Base de données vectorielle spécialisée dans le stockage et l'interrogation rapide des représentations numériques (embeddings) nécessaires au RAG.
	
	\item[\textbf{Chunking}] : Processus consistant à regrouper et découper le texte de la transcription en blocs cohérents pour optimiser l'indexation vectorielle.
	
	\item[\textbf{Docker}] : Outil de conteneurisation garantissant la portabilité de l'application et l'isolation des environnements de développement et de déploiement.
	
	\item[\textbf{Embedding}] : Représentation vectorielle d'un texte permettant aux modèles de comprendre les relations sémantiques entre différents éléments.
	
	\item[\textbf{JWT \textit{(JSON Web Token)}}] : Mécanisme d'authentification sécurisé utilisé pour gérer les sessions utilisateurs entre le frontend et le backend.
	
	\item[\textbf{LangChain}] : Framework d'orchestration utilisé pour structurer les interactions entre les embeddings, la base vectorielle et le modèle de langage.
	
	\item[\textbf{LLM \textit{(Large Language Model)}}] : Modèle d'intelligence artificielle entraîné sur de vastes volumes de données pour comprendre et générer du langage humain.
	
	\item[\textbf{RAG \textit{(Retrieval-Augmented Generation)}}] : Technique optimisant les réponses d'un LLM en lui fournissant un contexte extrait de données externes fiables.
	
	\item[\textbf{Recherche Sémantique}] : Méthode de recherche identifiant les résultats les plus proches sémantiquement d'une requête, au-delà de la simple correspondance lexicale.
	
	\item[\textbf{URL Signée (SAS)}] : Lien temporaire sécurisé généré dynamiquement pour permettre un accès contrôlé aux fichiers stockés dans le cloud.
	
	\item[\textbf{WER \textit{(Word Error Rate)}}] : Taux d'erreur par mot, indicateur principal utilisé pour mesurer la précision des systèmes de reconnaissance automatique de la parole (ASR).
	
	\item[\textbf{GPQA \textit{(Graduate-Level Google-Proof Q\&A)}}] : Benchmark de haute difficulté conçu pour évaluer les capacités de raisonnement scientifique et de compréhension experte des modèles de langage (LLM).
	
	\item[\textbf{MMLU \textit{(Massive Multitask Language Understanding)}}] : Indice de performance standard mesurant les connaissances générales et les capacités de résolution de problèmes des modèles de langage sur un vaste ensemble de domaines académiques et professionnels.	
	
\end{description}

\newpage
\phantomsection 
\addcontentsline{toc}{section}{Références}
\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
