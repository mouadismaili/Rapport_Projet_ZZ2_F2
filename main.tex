\documentclass{rapportECL}
\usepackage{listings}
\usepackage{lipsum}
\usepackage{gensymb}
\usepackage{float}
\usepackage{xcolor}
\usepackage{csquotes}
\usepackage{biblatex}
\bibliography{biblio} 
\usepackage{minted}
\usepackage{caption}
\usepackage{amssymb} % pour \checkmark
\usepackage{verbatim}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{placeins}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage{url} % Pour gérer les URLs
\usepackage{graphicx}
\usepackage{array}     % For more control over column alignment
\usepackage{colortbl}  % Table coloring
\usepackage{tikz} % figs
\usepackage{minted}
\usetikzlibrary{shapes.geometric, arrows.meta, positioning, bending}

\title{ArchiLogiciel } %Titre du fichier
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\definecolor{codebg}{RGB}{248,250,252}
\definecolor{codeframe}{RGB}{203,213,225}
\definecolor{techheader}{HTML}{0F172A}
\definecolor{techlinea}{HTML}{F8FAFF}
\definecolor{techlineb}{HTML}{EEF4FF}
\definecolor{techroute}{HTML}{E0F2FE}

\lstdefinestyle{oculacode}{
    language=Java,
    alsoletter={@},
    backgroundcolor=\color{codebg},
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    keywordstyle=[2]\color{teal!70!black}\bfseries,
    commentstyle=\color{mygreen},
    stringstyle=\color{mymauve},
    emph={uploadFile,createVideoWithUpload,getFileUrl},
    emphstyle=\color{orange!80!black}\bfseries,
    morekeywords=[2]{PostMapping,RequestParam,ResponseEntity,MultipartFile,UUID,Duration,OffsetDateTime,BlobClient,BlobSasPermission,BlobServiceSasSignatureValues},
    frame=single,
    framerule=0.6pt,
    rulecolor=\color{codeframe},
    numbers=left,
    numberstyle=\footnotesize\color{mygray},
    stepnumber=1,
    numbersep=10pt,
    xleftmargin=0pt,
    framexleftmargin=2.2em,
    breaklines=true,
    breakatwhitespace=true,
    showspaces=false,
    showstringspaces=false,
    tabsize=4,
    keepspaces=true,
    columns=fullflexible,
    captionpos=b,
    inputencoding=utf8,
    extendedchars=true,
    literate=%
     {é}{{\'e}}1
     {è}{{\`e}}1
     {à}{{\`a}}1
     {ç}{{\c{c}}}1
     {ê}{{\^e}}1
     {ù}{{\`u}}1
     {ô}{{\^o}}1
     {â}{{\^a}}1
}
\lstset{style=oculacode,
    breaklines=true,
    columns=fullflexible}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}
\begin{document}

%----------- Informations du rapport ---------

\titre{OCULA} %Titre du fichier .pdf
\UE{\textbf{Projet ZZ2 F2}} %Nom de la UE
\enseignant{Loïc \textsc{Yon}} %Nom de l'enseignant
\eleves{Mouad \textsc{Ismaili M'hamdi} \\
{Brahim \textsc{Id Benouakrim}}\\
{Achraf \textsc{El Allali}}
 } %Nom des élèves

%----------- Initialisation -------------------
        
\fairemarges %Afficher les marges
\fairepagedegarde %Créer la page de gard
\tabledematieres %Créer la table de matières

%------------ Corps du rapport ----------------



\section{Introduction}
\subsection{Contexte et motivation}
\noindent \textit{À compléter.}
\subsection{Problématique}
\noindent \textit{À compléter.}
\subsection{Objectifs du projet}
\noindent \textit{À compléter.}
\subsection{Périmètre et hypothèses}
\noindent \textit{À compléter.}
\subsection{Organisation du rapport}
\noindent \textit{À compléter.}
\section{Organisation et gestion du projet}

Le projet OCULA, développé sur une durée de \textbf{quatre semaines}, s'est appuyé sur une organisation collaborative où chaque membre de l'équipe a participé à tous les domaines d'expertise (frontend, backend, service IA). Cette approche holistique a permis à chacun d'acquérir une compréhension globale du système et de contribuer à l'émergence d'une architecture cohérente. Cette section décrit les processus mis en place pour coordonner l'équipe, planifier les tâches et gérer les risques inhérents à un projet combinant plusieurs technologies.

\subsection{Répartition des responsabilités et approche collaborative}

L'équipe de développement comprend trois étudiants qui ont collaboré sur tous les domaines d'expertise. Plutôt que d'assigner des rôles statiques et spécialisés (un développeur backend, un développeur frontend, un ingénieur IA), nous avons adopté une stratégie d'apprentissage transversal où chacun a contribué à la conception et la réalisation de l'ensemble des composantes.

\paragraph{Principes de collaboration}

Les tâches ont été réparties selon le schéma suivant :

\begin{enumerate}
    \item \textbf{Discussions de conception initiale} : avant chaque fonctionnalité majeure, l'équipe se réunit pour en discuter l'architecture, les choix technologiques et l'interface. Cette discussion collective garantit l'alignement et permet à chacun de proposer des solutions.
    
    \item \textbf{Répartition progressive des tâches} : après cette phase, les tâches spécifiques sont distribuées entre les membres selon :
    \begin{itemize}
        \item l'intérêt technique de chacun,
        \item la charge équilibrée,
        \item la nécessité de travail conjoint (pair-programming sur les parties critiques).
    \end{itemize}
    
    \item \textbf{Participation croisée} : chaque membre a touché aux trois domaines (frontend Angular, backend Spring Boot, service IA en Python). Ce chevauchement intentionnel facilite la compréhension du système global et répartit les connaissances.
\end{enumerate}

\paragraph{Avantages de cette approche}

\begin{itemize}
    \item \textbf{Résilience du projet} : aucune personne ne devient un goulot d'étranglement pour une partie critique.
    \item \textbf{Compréhension globale} : chaque développeur peut comprendre et intervenir n'importe où dans la codebase.
    \item \textbf{Apprentissage accéléré} : travailler sur plusieurs domaines enrichit les compétences individuelles.
    \item \textbf{Qualité des décisions} : les discussions collectives favorisent des choix architecturaux bien pensés.
\end{itemize}

\subsection{Méthodologie Agile itérative}

Afin de maximiser la productivité dans un laps de temps court (quatre semaines), l'équipe a adopté une méthodologie Agile fondée sur des sprints courts.

\paragraph{Caractéristiques des sprints}

\begin{itemize}
    \item \textbf{Durée} : sprints de \textbf{2 à 3 jours}.
    \item \textbf{Cadence} : réunions de synchronisation régulières à l'issue de chaque sprint.
    \item \textbf{Contenu} : chaque sprint prédéfinit un ensemble restreint d'objectifs (nouvelles fonctionnalités, corrections, documentation).
    \item \textbf{Revue et rétrospective} : à la fin de chaque sprint, l'équipe évalue :
    \begin{itemize}
        \item ce qui a été réalisé,
        \item les blocages rencontrés,
        \item les tâches prioritaires pour le sprint suivant.
    \end{itemize}
\end{itemize}

\paragraph{Outil de suivi}

Le suivi du code et des modifications a été centralisé via \textbf{GitLab}. Chaque développeur crée une branche pour une tâche spécifique, puis pousse ses modifications vers le dépôt central. Cette discipline de versionning permet :

\begin{itemize}
    \item de tracer historiquement chaque développement,
    \item d'identifier les auteurs et dates des modifications,
    \item de faciliter les code reviews et la fusion des branches (merge requests).
\end{itemize}

\paragraph{Bénéfices de cette approche}

Ces sprints courts, associés à une équipe physiquement collocalisée, ont permis une feedback rapide et une adaptation continuelle des priorités. La courte durée du projet (quatre semaines) a justifié cette cadence rapide, incompatible avec des sprints de deux à quatre semaines des projets plus longs.

\subsection{Conception UX : maquettes et validation fonctionnelle}

Avant de commencer le développement, l'équipe a réalisé une phase de conception servant de référence pour l'implémentation.

\paragraph{Maquettage avec Figma}

Les interfaces utilisateur ont été prototypées avec \textbf{Figma}, un outil collaboratif de conception web. Les maquettes ont couvert les principales pages de l'application :

\begin{itemize}
    \item Page d'authentification (connexion/inscription),
    \item Page d'upload de vidéo,
    \item Page de liste des vidéos (« My Videos »),
    \item Page de lecture d'une vidéo, affichage du titre et résumé,
    \item Page de chatbot de question--réponse,
    \item Page de paramètres utilisateur.
\end{itemize}

Ces maquettes ont servi de référence visuelle durant toute la phase de développement du frontend, garantissant une cohérence entre la vision initiale et la réalité implémentée.

\paragraph{Validation fonctionnelle}

Outre les maquettes visuelles, l'équipe a validé les fonctionnalités dès leur implémentation selon un cycle court :

\begin{enumerate}
    \item \textbf{Développement d'une fonctionnalité} (ex. upload, transcription),
    \item \textbf{Test manuel local} par le développeur et ses pairs,
    \item \textbf{Vérification que la fonctionnalité} répond aux critères d'acceptation,
    \item \textbf{Intégration dans la branche principale} après validation.
\end{enumerate}

Cette validation continue a permis d'identifier et corriger les défauts rapidement, évitant la dérive courante dans les projets où les tests ne sont faits qu'à la fin.

\subsection{Approche DevOps et stratégie Docker-first}

La gestion des environments et du déploiement a reposé sur une stratégie favorisant la conteneurisation complète de l'application.

\paragraph{Dockerisation de tous les services}

L'ensemble de l'application (frontend, backend, service IA, base de données) a été empaquetée dans des conteneurs Docker dès le départ :

\begin{itemize}
    \item Frontend : image Nginx servant l'application Angular compilée,
    \item Backend : image Maven compilant Spring Boot et image JRE pour l'exécution,
    \item Service IA : image Python contenant LangChain, Whisper et les dépendances IA,
    \item PostgreSQL : image PostgreSQL 16 pour les données relationnelles,
    \item ChromaDB : instance dédiée à la base vectorielle.
\end{itemize}

\paragraph{Orchestration locale avec Docker Compose}

Le fichier \path{docker-compose.yml} orchestre l'ensemble des services sur une machine locale. La commande :

\begin{verbatim}
    docker compose up --build
\end{verbatim}

\noindent construit et lance tous les services en un seul geste. Cela garantit que :

\begin{itemize}
    \item les versions des dépendances sont identiques sur toutes les machines,
    \item chaque développeur dispose d'un environment identique,
    \item l'application peut être testée dans les mêmes conditions de production.
\end{itemize}

\paragraph{Absence de pipeline CI/CD}

Aucun pipeline de CI/CD (GitHub Actions, GitLab CI, Jenkins) n'a été mis en place. Les raisons principales sont :

\begin{itemize}
    \item durée du projet limitée (quatre semaines),
    \item équipe réduite (trois développeurs) capable de tester manuellement,
    \item déploiement sur infrastructure locale uniquement (pas de production cloud).
\end{itemize}

Cette simplification a permis de concentrer les efforts sur les fonctionnalités plutôt que sur l'infrastructure de déploiement continu. Toutefois, l'architecture Docker-first laisse la porte ouverte à un CI/CD futur.

\subsection{Planification et diagramme de Gantt}

La portée du projet, couvrant trois domaines techniques (frontend, backend, IA), a nécessité une planification explicite afin de synchroniser les efforts et éviter les goulots d'étranglement.

\paragraph{Phases du projet}

Le projet s'est structuré en deux grandes phases :

\begin{enumerate}
    \item \textbf{Phase de conception} (semaine 1) :
    \begin{itemize}
        \item Analyse des exigences et cas d'utilisation,
        \item Choix technologiques et architecture globale,
        \item Maquettage des interfaces,
        \item Mise en place de l'infrastructure Docker locale.
    \end{itemize}
    
    \item \textbf{Phase de développement} (semaines 2 à 4) :
    \begin{itemize}
        \item Implémentation itérative des composantes frontend, backend et IA,
        \item Intégration progressive des modules,
        \item Validation manuelle continue,
        \item Documentation du code et de l'architecture.
    \end{itemize}
\end{enumerate}

\paragraph{Outil de planification}

Un diagramme de Gantt a été élaboré afin de visualiser :

\begin{itemize}
    \item le calendrier de chaque tâche majeure,
    \item les dépendances entre tâches (ex. le backend doit exposer ses API avant que le frontend ne les consomme),
    \item la charge estimée pour chaque domaine,
    \item les jalons critiques.
\end{itemize}

Ce diagramme a servi de référence pour la révision hebdomadaire des priorités et l'ajustement du scope si nécessaire.

\paragraph{Adaptation et revisions}

En raison de la nature Agile du projet et de l'apprentissage progressif de technologies nouvelles, le plan initial a connu des ajustements. Les réunions de fin de sprint ont permis de :

\begin{itemize}
    \item identifier les tâches terminées plus tôt ou plus tard que prévu,
    \item réévaluer les dépendances,
    \item réorganiser les priorités pour les sprints suivants.
\end{itemize}

\subsection{Processus de validation et revue de code}

Bien que l'équipe soit réduite, un processus de validation du code a été instauré afin de maintenir la qualité et de favoriser le partage de connaissance.

\paragraph{Code reviews informelles}

Entre développeurs collocalisés, les reviews se déroulaient souvent de manière informelle :

\begin{itemize}
    \item \textbf{Pair-programming} : deux développeurs travaillaient ensemble sur une tâche critique, se validant mutuellement en temps réel.
    \item \textbf{Discussions de conception} : avant de commencer une implémentation, présentée rapidement aux autres pour recueillir des retours.
    \item \textbf{Tests croisés} : un développeur teste la fonctionnalité développée par un pair en local sur sa machine.
\end{itemize}

\paragraph{Rigueur sur les branches et merges}

Sur GitLab, la discipline était la suivante :

\begin{itemize}
    \item Une branche par fonctionnalité ou task (ex. \path{feature/video-upload}, \path{feature/rag-chatbot}),
    \item Des commits atomiques et descriptifs,
    \item Une relecture sommaire avant merge sur la branche principale (\path{main} ou \path{develop}).
\end{itemize}

Bien qu'informelle comparée à des processus d'entreprise strictes, cette approche a maintenu une certaine discipline tout en restant flexible.

\subsection{Difficultés rencontrées et gestion des risques}

Tout projet d'envergure rencontre des obstacles. Cette sous-section détaille ceux que l'équipe a affrontés et les stratégies adoptées pour les surmonter.

\paragraph{Difficultés principales}

\subsubsection*{1. Prise en main des technologies}

\textbf{Problème} : le projet combinait trois échos techniques distincts (Angular pour le frontend, Spring Boot pour le backend, Python/LangChain pour l'IA), chacun avec son écosystème, ses conventions et ses pièges.

\textbf{Impact} : courbe d'apprentissage initiale, ralentissements pendant les deux premières semaines.

\textbf{Mitigation} :
\begin{itemize}
    \item \textbf{Documentation et tutoriels} : chaque membre a consacré du temps à explorer la documentation officielle.
    \item \textbf{Pair-programming} : lors du premier module de chaque domaine, deux développeurs travaillaient ensemble.
    \item \textbf{Partage de connaissances} : après chaque exploration, le développeur partagait ses découvertes avec l'équipe lors des réunions.
\end{itemize}

\subsubsection*{2. Communication entre frontend et backend}

\textbf{Problème} : le frontend est client des API fournies par le backend. Un décalage dans la spécification des endpoints, des formats de requête ou des réponses JSON pouvait bloquer le frontend.

\textbf{Impact} : blocages mutuels pendant l'intégration, corrections tardives de contrats.

\textbf{Mitigation} :
\begin{itemize}
    \item \textbf{Spécification d'API partagée} : avant l'implémentation, les développeurs frontend et backend se réunissaient pour définir explicitement la signature de chaque endpoint (path, méthode HTTP, format du payload et de la réponse).
    \item \textbf{Utilisation d'une API mock} : le frontend pouvait développer en parallèle en utilisant des données mockées, sans bloquer sur le backend.
    \item \textbf{Intégration précoce} : dès qu'un endpoint était disponible, l'intégration était testée immédiatement.
\end{itemize}

\subsubsection*{3. Intégration des services IA}

\textbf{Problème} : le service IA effectue des tâches longues et coûteuses (transcription, génération d'embeddings, appel au LLM distant). L'intégration asynchrone requiert une gestion d'état et une communication robuste entre le backend et le service IA.

\textbf{Impact} : risque de timeouts, de pertes de messages, d'incohérences d'état.

\textbf{Mitigation} :
\begin{itemize}
    \item \textbf{Traitement asynchrone explicite} : les tâches lourdes sont lancées dans des threads Python séparés plutôt que dans le contexte de la requête HTTP.
    \item \textbf{Gestion d'état des vidéos} : le statut (PENDING, PROCESSING, COMPLETED, FAILED) est explicitement géré en base de données et notifié au frontend en temps réel via WebSocket.
    \item \textbf{Callbacks de notification} : le service IA notifie le backend de l'avancement via des endpoints dédiés (PUT /videos/update/status).
    \item \textbf{Tests progressifs} : chaque étape du pipeline IA (transcription, chunking, embeddings, RAG) a été testée isolément avant intégration.
\end{itemize}

\subsubsection*{4. Gestion du temps face à la courte durée du projet}

\textbf{Problème} : quatre semaines pour construire une application complète couvrant trois domaines techniques.

\textbf{Impact} : risque de scope creep, d'accumulation de dette technique, de limitation du périmètre de validation.

\textbf{Mitigation} :
\begin{itemize}
    \item \textbf{MVP bien défini} : au départ, une liste claire des fonctionnalités essentielles vs. optionnelles a été établie.
    \item \textbf{Sprints courts et revues fréquentes} : chaque sprint de 2--3 jours était suivi d'une évaluation, permettant des ajustements rapides.
    \item \textbf{Priorisation agile} : les tâches ont été hiérarchisées par valeur ajoutée et complexité, concentrant les efforts sur les éléments critiques.
    \item \textbf{Partage des responsabilités} : l'approche collaborative a évité qu'une personne ne soit « surchargée » ; les tâches se distribuaient dynamiquement selon la charge.
\end{itemize}

\paragraph{Mécanismes de gestion des risques mis en place}

\begin{itemize}
    \item \textbf{Discussions régulières d'équipe} : réunions quotidiennes de 15 minutes pour synchronisation et identification des blocages,
    \item \textbf{Répartition claire des tâches} : chaque membre savait précisément sur quoi travailler, minimisant les doublons,
    \item \textbf{Validation progressive des fonctionnalités} : chaque module était validé manuellement lors de son développement,
    \item \textbf{Consultation active de la documentation} : plutôt que de rester bloqué, les développeurs exploraient la documentation officielle des technologies pour résoudre les problèmes,
    \item \textbf{Flexibilité de scope} : capacité à ajuster les priorités et à reporter les fonctionnalités optionnelles.
\end{itemize}

\paragraph{Bilan de gestion des difficultés}

L'équipe a réussi à transformer la plupart des difficultés en opportunités d'apprentissage. La courte durée du projet, loin d'être un handicap, a imposé une discipline et une clarté favorables à l'avancement. Les trois à quatre semaines se sont écoulées de manière progressive, avec une implication collective et une capacité d'adaptation constante.

\begin{figure}[H]
\centering
\resizebox{0.9\textwidth}{!}{%
\begin{tikzpicture}[
    node distance=2.5cm,
    problem/.style={rectangle, rounded corners=3pt, draw=red!70, fill=red!20, minimum width=2.2cm, minimum height=0.6cm, align=center, font=\scriptsize\bfseries},
    solution/.style={rectangle, rounded corners=3pt, draw=green!70, fill=green!20, minimum width=2.2cm, minimum height=1.2cm, align=center, font=\scriptsize},
    arrow/.style={-{Stealth[length=3pt]}, thick, blue!70}
]

\node[problem] (p1) at (0, 0) {1. Techno};
\node[solution] (s1) at (0, -1.4) {Pair-prog\\Docs\\Partage};

\node[problem] (p2) at (2.8, 0) {2. Frontend/\\Backend};
\node[solution] (s2) at (2.8, -1.4) {API spec\\Mock\\Intégr. rapide};

\node[problem] (p3) at (5.6, 0) {3.Service IA};
\node[solution] (s3) at (5.6, -1.4) {Async\\État\\Callbacks};

\node[problem] (p4) at (8.4, 0) {4. Temps};
\node[solution] (s4) at (8.4, -1.4) {MVP\\Sprints\\Priorisation};

\draw[arrow] (p1) -- (s1);
\draw[arrow] (p2) -- (s2);
\draw[arrow] (p3) -- (s3);
\draw[arrow] (p4) -- (s4);

\end{tikzpicture}
}%
\caption{Synthèse : 4 difficultés majeures et stratégies de résolution}
\label{fig:difficulties-management}
\end{figure}

\section{État de l'art et fondements théoriques}
\subsection{Analyse de contenu vidéo : enjeux et défis}
\noindent \textit{À compléter.}
\subsubsection{Données non structurées et difficulté d'indexation}
\noindent \textit{À compléter.}
\subsubsection{Contraintes de latence, coût et confidentialité}
\noindent \textit{À compléter.}

\subsection{Reconnaissance automatique de la parole (ASR)}
ASR (automatic speech recognition) est une technologie qui convertit le langage parlé en texte écrit. Elle traite les signaux audio, identifie les modèles de parole et les transcrit en texte avec une grande précision.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{images/fonctionnement-automatic-speech-recognition.jpg}
    \caption{Reconnaissance automatique de la parole}
    \label{fig:placeholder}
\end{figure}
\subsubsection{Principes généraux de la transcription audio--texte}
Principalement on distingue entre deux approches pour la reconnaissance automatique de la parole:

\begin{itemize}
    \item \textbf{Approche hybride classique: }est une méthode qui combine un modèle acoustique, un modèle de lexique et un modèle linguistique pour transcrire la parole en texte, en s’appuyant sur des données audio alignées afin d’associer précisément les segments aux phonèmes et aux mots.

    \item  \textbf{Deep Learning: }utilise des réseaux de neurones pour convertir directement la parole en texte, sans séparer explicitement les modèles acoustique, lexique et linguistique. Elle apprend automatiquement les relations entre les sons et les mots à partir de grandes quantités de données audio.
\end{itemize}

\subsubsection{Choix de modèle (Whisper)}

Whisper est un système de reconnaissance automatique de la parole (ASR) basé sur l’intelligence artificielle, entraîné sur environ 680 000 heures de données audio multilingues provenant du web. Grâce à cette très grande diversité de données, il est particulièrement robuste face aux accents, au bruit de fond et au vocabulaire technique. Whisper est capable non seulement de transcrire la parole dans plusieurs langues, mais aussi de traduire directement ces langues vers l’anglais, ce qui en fait un outil polyvalent pour de nombreuses applications de traitement vocal et de recherche en intelligence artificielle.


\begin{table}[h]
\centering
\caption{Comparaision des Performance des Models ASR}
\begin{tabular}{lccccc}
\hline
\textbf{Criteria} & \textbf{Whisper} & \textbf{AssemblyAI U3} & \textbf{Amazon} & \textbf{Microsoft} \\
\hline
Accuracy (English) & 92.4\% & \textbf{94.1\%} & 92.4\% & 92.5\% \\
Accuracy (Multilingual) & \textbf{92.6\%} & 91.3\% & 89.9\% & 88.9\% \\
WER (English) & 6.5\% & \textbf{5.9\%} & 7.6\% & 7.5\% \\
WER (Multilingual) & \textbf{7.4\%} & 8.7\% & 10.1\% & 11.1\% \\
\hline
\end{tabular}
\end{table}


Le choix de Whisper a été évident , du a ça capacité impressive dans la transcription multilingue , et sont WER minimal par rapport aux autre modèle. De plus nous favorisons les modèles open-source, et Whisper étant le meilleur dans cette catégorie.


\subsection{Représentations vectorielles et recherche sémantique}
\subsubsection{Définition des embeddings}
Un embedding est une représentation vectorielle d’éléments comme les textes, les images, les vidéos ou les signaux audio. Cette transformation en valeurs numériques permet aux modèles de machine learning de comprendre les relations sémantiques et d’effectuer des tâches telles que la recherche, la similarité ou la classification.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/embedding.png}
    \caption{Fonctionnement de embedding}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Indexation et recherche vectorielle}
L’indexation vectorielle est une technique qui consiste à organiser des données sous forme de vecteurs numériques afin de permettre une recherche rapide et efficace dans des espaces de grande dimension.

La recherche vectorielle est le processus qui consiste à comparer le vecteur d’une requête avec ceux stockés dans cet index afin d’identifier les éléments les plus proches sémantiquement.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/example_index_vec.png}
    \caption{Illustration de l’indexation vectorielle}
    \label{fig:placeholder}
\end{figure}

La figure illustre le principe de l’indexation et de la recherche vectorielle. 
Chaque point coloré représente un objet (mot, image ou donnée) transformé en vecteur 
dans un espace numérique. Les éléments ayant un sens similaire se retrouvent proches 
les uns des autres, formant des groupes sémantiques. Par exemple, les mots ``cat'', 
``dog'' et ``lion'' sont regroupés car ils appartiennent à la même catégorie d’animaux, 
tandis que ``car'', ``truck'' et ``vehicle'' forment un autre groupe lié aux moyens 
de transport. Lors d’une recherche vectorielle, le système consiste simplement à 
retrouver les points les plus proches du vecteur de la requête afin d’identifier 
les résultats les plus pertinents. 

\subsection{Modèles de langage (LLM)}
\subsubsection{Capacités et limites des LLM}
Un LLM (Large Language Model ou Grand Modèle de Langage) est un système d'intelligence artificielle entraîné sur d'immenses quantités de données textuelles pour comprendre, générer et manipuler le langage humain de manière fluide et cohérente.

\begin{itemize}
    \item \textbf{Capacités:}
    \begin{itemize}
        \item \textbf{Génération de contenu:} ils peuvent rédiger instantanément des textes.
        \item \textbf{Synthèse d'informations:} ils peuvent résumer des documents très long en extrayant les points clés.
        \item \textbf{Traduction et adaptation de style:} Ils peuvent traduire des langues et modifier le style d'un texte.
    \end{itemize}
    \item \textbf{Limites}
    \begin{itemize}
        \item \textbf{Hallucinations:} ils peuvent confirmer des informations fausses.
        \item \textbf{Coût de calcul élevé:} ils sont très chers à entraîner et à faire fonctionner. 
    \end{itemize}
\end{itemize}

\subsubsection{Choix de modèle (gpt-oss-120b)}
Lors de notre recherche des modèles de LLM à utiliser, nous avons identifié deux candidats principaux : gpt-oss-120b et llama3.1-8b, tous deux accessibles via l’API fournie par Cerebras.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\linewidth]{images/gpt-vs-llama.png}
    \caption{Camparaison entre gpt-oss-120b et llama3.1-8b}
    \label{fig:placeholder}
\end{figure}

En analysant les deux indices de performance, GPQA (Graduate-Level Google-Proof Q\&A) et MMLU (Massive Multitask Language Understanding), nous pouvons clairement constater que le modèle gpt-oss-120b surpasse llama3.1-8b. Cette supériorité s’explique notamment par son nombre beaucoup plus élevé de paramètres (120 milliards) ainsi que par son architecture plus avancée, lui permettant d’obtenir de meilleurs résultats sur des tâches complexes de raisonnement et de compréhension.


\subsection{Retrieval-Augmented Generation (RAG)}
\subsubsection{Principe général}
Le RAG (Retrieval-Augmented Generation) est une technique qui permet d'optimiser les réponses d'un modèle d'IA en lui donnant accès à des données externes fiables, au-delà de ses connaissances d'entraînement initiales.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{images/rag_pipeline.png}
    \caption{Pipeline de RAG}
    \label{fig:placeholder}
\end{figure}

Un pipeline de RAG fonctionne en trois étapes simples : d’abord, des documents sont collectés, découpés en petits morceaux puis transformés en vecteurs pour être stockés dans une base vectorielle. Ensuite, quand un utilisateur pose une question, elle est aussi convertie en vecteur afin de retrouver les passages les plus pertinents dans cette base. Enfin, ces informations sont envoyées avec la question à un LLM, qui s’en sert comme contexte pour produire une réponse plus précise et fiable.

\subsubsection{Forces et limites du RAG}
Le RAG est une approche qui combine la recherche d’informations externes avec la génération de texte par un LLM. Cette architecture permet d’améliorer la qualité des réponses, mais elle présente également certaines contraintes techniques.

\begin{itemize}
    \item \textbf{Amélioration de la précision :} Le RAG permet de générer des réponses plus fiables en s’appuyant sur des documents externes, ce qui réduit fortement les hallucinations des modèles de langage.
    
    \item \textbf{Dépendance à la qualité de la recherche :} La performance du système dépend fortement de la pertinence des documents récupérés.
    
    \item \textbf{Latence et complexité :} L’étape de récupération des données ajoute du temps de réponse et augmente les coûts techniques et computationnels.
\end{itemize}

\section{Spécifications et conception de l'application OCULA}
\subsection{Présentation générale de la solution}
Notre application \textbf{Ocula} est une solution dédiée au traitement et à l’analyse de vidéos. Elle permet d’extraire automatiquement les transcriptions, puis de les exploiter pour générer des titres et des résumés pertinents. De plus, elle intègre une approche RAG (Retrieval-Augmented Generation) afin de répondre de manière précise et contextuelle aux questions des utilisateurs concernant le contenu des vidéos.

\subsection{Cas d'utilisation et fonctionnalités}
\subsubsection{Upload et gestion des vidéos}
Notre solution \textbf{Ocula} gère les vidéos téléversées en s’appuyant sur le service de \textbf{Azure Blob Storage}. L’utilisateur peut importer une vidéo depuis la partie client, puis une requête est envoyée au backend afin de créer automatiquement une entrée correspondante dans notre base de données. La vidéo est ensuite stockée dans notre object storage, où elle est associée à une clé unique. Cette clé permet de générer une URL sécurisée donnant accès à la vidéo lorsque cela est nécessaire.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.15\linewidth]{images/upload_video.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Transcription et génération de titre et de résumé}
Notre solution \textbf{Ocula} permet d’extraire automatiquement la transcription des vidéos en exploitant les capacités de \textbf{Whisper}. Après le téléversement d’une vidéo, une tâche de traitement est déclenchée afin de l’analyser. Une fois la transcription extraite, nous enregistrons ses différents segments, puis nous divisons le script en plusieurs chunks.

Ces chunks sont ensuite transformés en embeddings et stockés dans une base de données vectorielle, afin de pouvoir être exploités ultérieurement dans un système \textbf{RAG} (Retrieval-Augmented Generation). Enfin, grâce à une pipeline composée d’un prompt et d’un LLM (\textbf{gpt-oss-120b}), la solution génère automatiquement un titre pertinent ainsi qu’un résumé du contenu de la vidéo.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{images/transcription.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Chatbot de question--réponse sur la vidéo}
Notre solution \textbf{Ocula} intègre un assistant \textbf{chatbot} basé sur l’intelligence artificielle. Cet assistant utilise un \textbf{LLM} afin de répondre aux questions des utilisateurs tout en s’appuyant sur une approche \textbf{RAG} (Retrieval-Augmented Generation) et sur les différents chunks générés lors du traitement des vidéos.

Lorsqu’un utilisateur envoie un message, le chatbot déclenche une pipeline RAG qui recherche les informations pertinentes dans les chunks de la transcription associés à la vidéo concernée. Le contexte ainsi récupéré est ensuite transmis au LLM (\textbf{gpt-oss-120b}), ce qui lui permet de fournir des réponses précises, pertinentes et adaptées au contenu réel de la vidéo.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.1\linewidth]{images/chat_case.png}
    \caption{Enter Caption}
    \label{fig:placeholder}
\end{figure}

\subsection{Exigences fonctionnelles}

Les exigences fonctionnelles décrivent les capacités et services que doit fournir l’application OCULA du point de vue des utilisateurs et du système métier.

\subsubsection{Gestion des utilisateurs}

\begin{itemize}
	\item Le système doit permettre à un utilisateur de créer un compte à l’aide d’une adresse e-mail et d’un mot de passe.
	\item Le système doit permettre l’authentification sécurisée des utilisateurs.
	\item Le système doit permettre la modification des informations du profil (nom, prénom, e-mail, mot de passe, avatar).
	\item Le système doit gérer les sessions utilisateurs via un mécanisme d’authentification basé sur des tokens jwt.
\end{itemize}

\subsubsection{Gestion des vidéos}

\begin{itemize}
	\item Le système doit permettre à un utilisateur d’importer une vidéo depuis l’interface web.
	\item Le système doit stocker les vidéos dans un stockage objet.
	\item Le système doit associer chaque vidéo à son utilisateur propriétaire.
	\item Le système doit permettre d’afficher la liste des vidéos d’un utilisateur.
	\item Le système doit permettre de consulter les détails d’une vidéo (titre, résumé).
\end{itemize}

\subsubsection{Traitement automatique des vidéos}

\begin{itemize}
	\item Le système doit extraire automatiquement la transcription d’une vidéo importée.
	\item Le système doit segmenter la transcription en portions temporelles.
	\item Le système doit transformer ces segments en représentations vectorielles (embeddings).
	\item Le système doit générer automatiquement un titre et un résumé à partir de la transcription.
\end{itemize}

\subsubsection{Recherche sémantique et question--réponse}

\begin{itemize}
	\item Le système doit permettre à l’utilisateur de poser des questions sur le contenu d’une vidéo.
	\item Le système doit rechercher les segments les plus pertinents dans la base de données vectorielle.
	\item Le système doit générer des réponses contextuelles à l’aide d’un LLM.
	\item Le système doit conserver l’historique des échanges entre l’utilisateur et le chatbot.
\end{itemize}

\subsubsection{Suivi du traitement}

\begin{itemize}
	\item Le système doit gérer les différents états du traitement d’une vidéo (en attente, en cours, terminé, échoué).
	\item Le système doit permettre à l’utilisateur de suivre l’avancement du traitement en temps réel.
	\item Le système doit notifier l’interface utilisateur lors des changements d’état.
\end{itemize}

\subsection{Exigences non fonctionnelles}

Les exigences non fonctionnelles décrivent les contraintes techniques et qualitatives que doit respecter l’application OCULA afin d’assurer sa fiabilité, sa performance et sa sécurité.

\subsubsection{Performance et latence}

\begin{itemize}
	\item Le système doit assurer un temps de réponse rapide pour les interactions utilisateur (navigation, affichage des vidéos, consultation des résultats).
	\item Le traitement des vidéos doit être effectué de manière asynchrone afin de ne pas bloquer l’interface utilisateur.
	\item Le temps de réponse du chatbot doit rester raisonnable pour garantir une expérience fluide.
	\item Les opérations de recherche sémantique doivent être optimisées pour réduire la latence lors des requêtes.
\end{itemize}

\subsubsection{Sécurité et confidentialité}

\begin{itemize}
	\item Le système doit garantir l’authentification et l’autorisation des utilisateurs.
	\item Les données sensibles doivent être protégées lors du stockage et des communications.
	\item L’accès aux vidéos doit être restreint uniquement à leur propriétaire.
	\item la communication entre le backend et le service d’intelligence artificielle repose sur l’utilisation de clés d’API (API keys) permettant d’authentifier les requêtes et de restreindre l’accès aux endpoints internes.
\end{itemize}

\subsection{Architecture globale}
\subsubsection{Vue d'ensemble (Frontend, Backend, AI Service)}
Notre application \textbf{Ocula} suit une architecture structurée en trois principales couches. D’abord, le frontend permet à l’utilisateur de téléverser une vidéo et de demander son analyse. Ensuite, le backend gère les requêtes, enregistre les métadonnées dans la base de données et stocke les fichiers dans un système de stockage objet. Enfin, la partie IA prend en charge le traitement de la vidéo, elle génère la transcription, crée des embeddings stockés dans une base vectorielle, puis exploite ces données pour alimenter le LLM, qui produit des résumés, des titres et des réponses pertinentes aux questions des utilisateurs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{images/app_workflow.png}
    \caption{Vue d'ensemble}
    \label{fig:placeholder}
\end{figure}

\subsubsection{Choix technologiques et justification}
Pour le développement de l’application \textbf{Ocula}, nous avons choisi des technologies adaptées à une architecture moderne et scalable.

Le \textbf{backend}, nous avons choisi \textbf{Spring-Boot}, un framework Java largement adopté pour la conception d’API robustes et sécurisées. Il offre une intégration avec les bases de données relationnelles, ainsi qu’un écosystème facilitant le développement.

Le \textbf{frontend} a été développé avec \textbf{Angular}, un framework structuré et performant, particulièrement adapté aux applications web complexes. \textbf{Angular} permet une bonne organisation du code, une communication fluide avec les API REST, ainsi qu’une expérience utilisateur dynamique et réactive.

La \textbf{partie intelligence artificielle}, nous avons utilisé \textbf{Python} avec \textbf{Flask} pour exposer les services IA sous forme d’API légères et rapides. L’intégration de \textbf{LangChain} a permis de faciliter la mise en place des pipelines RAG.

Pour la \textbf{persistance des données}, nous avons choisi \textbf{PostgreSQL} comme base de données relationnelle, en raison de sa capacité à gérer des structures de données complexes. En complément, nous avons utilisé \textbf{ChromaDB} comme base de données vectorielle, spécialement conçue pour le stockage et la recherche d’embeddings.

\subsection{Conception}
\subsubsection{Modèle conceptuel et logique}

Le modèle conceptuel d’OCULA est structuré autour de l’entité \textit{Video}, qui constitue le cœur fonctionnel du système. Bien que l’application soit multi-utilisateur, l’ensemble du cycle de traitement IA est vidéo-centré : les opérations d’analyse, de transcription, de génération d’embeddings et de question–réponse s’appuient systématiquement sur l’identifiant \path{video\_id}.
Ainsi, l’entité \textit{User} possède les vidéos, mais c’est la \textit{Video} qui orchestre le pipeline métier.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{images/diagramme de class entités.png}
\caption{Diagramme de classes du domaine métier d’OCULA}
\label{fig:domain-model}
\end{figure}

\paragraph{Relation User -- Video}

Un utilisateur peut posséder plusieurs vidéos (\textbf{1--N}). Cette relation n’est pas uniquement logique, elle répond à plusieurs exigences architecturales :

\begin{itemize}
    \item \textbf{Sécurité} : le contrôle d’accès repose sur le propriétaire de la vidéo ; un utilisateur ne peut accéder qu’à ses propres ressources.
    \item \textbf{Isolation des données} : chaque compte constitue un espace logique distinct.
    \item \textbf{Scalabilité} : les requêtes peuvent être filtrées et paginées par \path{userId}, améliorant les performances.
    \item \textbf{Architecture multi-tenant} : la base est partagée, mais le cloisonnement est assuré au niveau applicatif.
\end{itemize}

\paragraph{Relation Video -- VideoTranscript}  


Contrairement à une approche monolithique consistant à stocker une transcription complète sous forme d’un simple champ texte, OCULA adopte une modélisation segmentée. Une vidéo possède donc \textbf{0..*} entités \textit{VideoTranscript}, chacune caractérisée par un \path{startTime}, un \path{endTime} et un \path{transcriptText}.

Ce choix répond à plusieurs objectifs :

\begin{itemize}
    \item \textbf{Traçabilité temporelle} : chaque portion de texte peut être reliée à un intervalle précis de la vidéo.
    \item \textbf{Robustesse du pipeline} : en cas d’échec partiel du traitement, les segments déjà générés restent persistés.
    \item \textbf{Réindexation ciblée} : il est possible de retraiter ou ré-encoder certains segments sans recalcul global.
    \item \textbf{Granularité adaptée au RAG} : les segments constituent la base naturelle du découpage en chunks.
\end{itemize}

\paragraph{Relation Video -- Message}

Chaque vidéo peut contenir \textbf{0..*} messages correspondant aux échanges entre l’utilisateur et le chatbot. La persistance de ces messages dépasse le simple besoin d’affichage :

\begin{itemize}
    \item \textbf{Historique multi-session} : la conversation est conservée même après rechargement ou changement d’appareil.
    \item \textbf{Audit et explicabilité} : il est possible d’analyser les requêtes et réponses produites.
    \item \textbf{Évolutivité} : ces données peuvent servir à des analyses ultérieures ou à l’amélioration du système.
\end{itemize}

\paragraph{Gestion d’état du traitement}

L’attribut \path{status} de l’entité \textit{Video} (PENDING, PROCESSING, COMPLETED, FAILED) modélise explicitement le cycle de vie du traitement asynchrone.
\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/Gestion d’état du traitement-2026-02-22-192854.png}
\caption{Gestion d’état du traitement (VideoStatus)}
\label{fig:domain-model}
\end{figure}
Dans un système distribué où la transcription et l’indexation peuvent prendre plusieurs minutes, ce champ est essentiel pour :

\begin{itemize}
    \item informer le frontend de l’avancement,
    \item permettre une mise à jour en temps réel via WebSocket,
    \item gérer proprement les erreurs et les tentatives de relance,
    \item éviter les états incohérents ou bloqués.
\end{itemize}

Ainsi, le modèle conceptuel ne se limite pas à une simple représentation des données : il structure le comportement global du système et soutient la robustesse de l’architecture.

\subsubsection{Modélisation des transcriptions, chunks et embeddings}

Si le modèle précédent décrit la structure relationnelle des données métier, le mécanisme de recherche sémantique repose sur une modélisation complémentaire adaptée au RAG.

\medskip
\textbf{1. De la transcription segmentée aux chunks}

La transcription est initialement stockée sous forme de segments temporels (\textit{VideoTranscript}). 
Toutefois, une recherche sémantique efficace ne peut s’appuyer ni sur un texte monolithique complet, ni sur des segments trop courts et isolés.

Le système applique donc une étape de \textit{chunking}, consistant à regrouper et découper les segments afin de produire des blocs textuels cohérents et exploitables pour l’indexation vectorielle.

Cette granularité intermédiaire permet :

\begin{itemize}
    \item d’améliorer la précision lors de la récupération de contexte,
    \item de réduire le bruit informationnel,
    \item d’optimiser l’utilisation de la fenêtre de contexte du LLM,
    \item de maintenir un équilibre entre cohérence globale et pertinence locale.
\end{itemize}

\medskip
\textbf{2. Génération des embeddings}


Chaque chunk est transformé en vecteur numérique à l’aide du modèle 
\path{sentence-transformers/all-MiniLM-L6-v2}, chargé via \textit{HuggingFaceEmbeddings}.

Ce modèle encode les textes en vecteurs de dimension fixe permettant une comparaison sémantique par mesure de similarité ou de distance vectorielle. 
Chaque chunk correspond ainsi à un triplet logique :

\begin{itemize}
    \item texte source,
    \item embedding haute dimension,
    \item métadonnées associées.
\end{itemize}

Cette représentation vectorielle permet de comparer la question de l’utilisateur avec le contenu vidéo et d’identifier les passages les plus pertinents.

\medskip
\textbf{3. Organisation dans la base vectorielle}

Les embeddings sont stockés dans ChromaDB selon une organisation par vidéo. 
Pour chaque vidéo, une collection dédiée (\path{video\_<video\_id>}) est utilisée ou créée lors de l’initialisation du flux d’indexation.

Chaque document vectoriel contient :

\begin{itemize}

    \item le texte du chunk,
    \item l’embedding associé,
    \item des métadonnées : \path{video\_id}, \path{start\_time}, \path{end\_time}.
\end{itemize}
\begin{figure}[H]
    \centering
        \includegraphics[width=0.4\textwidth]{images/chroma_document_model.png}
    \caption{Structure d’un document vectoriel dans ChromaDB}
    \label{fig:chroma-document}
\end{figure}

Lors d’une requête utilisateur, le mécanisme RAG :

\begin{itemize}
    \item effectue une recherche par similarité dans la collection de la vidéo concernée,
    \item sélectionne les $k$ passages les plus proches,
    \item applique un filtrage et un éventuel \textit{reranking} afin d’améliorer la pertinence du contexte transmis au LLM.
\end{itemize}

Cette organisation garantit :

\begin{itemize}
    \item l’isolation des recherches à une vidéo donnée,
    \item la traçabilité des sources utilisées dans la réponse,
    \item une correspondance directe entre passage récupéré et intervalle temporel.
\end{itemize}

Le modèle relationnel assure la cohérence transactionnelle des données métier, tandis que la couche vectorielle optimise la récupération contextuelle pour le RAG ; les deux couches sont donc complémentaires et synchronisées par \path{video\_id}.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{images/rag_pipeline_matrix.png}
    \caption{Pipeline RAG : phases de retrieval, construction du contexte et génération}
    \label{fig:rag-pipeline}
\end{figure}

\subsubsection{Persistance : base relationnelle, stockage objet et base vectorielle}

La nature hétérogène des données manipulées par OCULA impose une stratégie de persistance adaptée à chaque type d’information. 
Le système distingue ainsi trois couches de stockage complémentaires : relationnelle, objet et vectorielle.

\paragraph{Base relationnelle (PostgreSQL)}

Les entités métier structurées (\textit{User}, \textit{Video}, \textit{Message}, \textit{VideoTranscript}) sont stockées dans une base relationnelle.

Ce choix permet :

\begin{itemize}
    \item de garantir l’intégrité référentielle entre les entités,
    \item d’assurer la cohérence transactionnelle au niveau des données métier,
    \item de faciliter les requêtes applicatives (filtrage, pagination, jointures),
    \item de maintenir une structure normalisée et contrôlée.
\end{itemize}

Il est toutefois important de noter qu’il n’existe pas de transaction globale entre PostgreSQL, le stockage objet et la base vectorielle : l’architecture repose sur une coordination applicative entre services.

\paragraph{Stockage objet (Azure Blob Storage)}

Les fichiers volumineux tels que les vidéos, les miniatures et les avatars sont stockés dans un service de type objet.

La base relationnelle conserve principalement la \textit{clé d’objet} (object key) associée à chaque ressource (\path{content}, \path{thumbnail}, \path{avatar}). 
Une URL signée est générée dynamiquement lors de l’accès au fichier.

Ce choix permet :

\begin{itemize}
    \item d’optimiser le stockage des contenus binaires,
    \item de réduire la charge sur la base relationnelle,
    \item d’assurer une meilleure scalabilité des médias,
    \item de séparer clairement données structurées et fichiers lourds.
\end{itemize}

\paragraph{Base vectorielle (ChromaDB)}

Les embeddings générés à partir des chunks sont stockés dans une base vectorielle dédiée (ChromaDB).

Chaque vidéo dispose d’une collection nommée \path{video\_<video\_id>}. 

Cette organisation assure :

\begin{itemize}
    \item l’isolement des recherches par vidéo,
    \item la traçabilité temporelle des passages récupérés,
    \item une recherche efficace par mesure de similarité ou distance vectorielle.
\end{itemize}


\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{images/hybrid-architecture.png}
\caption{Architecture de persistance hybride d’OCULA : base relationnelle, stockage objet et base vectorielle}
\label{fig:persistence-hybrid}
\end{figure}
 
\medskip

Le modèle relationnel assure la cohérence des données métier, le stockage objet prend en charge les contenus binaires via des clés d’objet, et la base vectorielle optimise la récupération sémantique pour le RAG. 
Ces trois couches sont complémentaires et interconnectées principalement par \path{video\_id} (et \path{user\_id} pour les ressources utilisateur). 
\medskip

La conception présentée précédemment définit la structure logique et la stratégie de persistance du système. 
Nous détaillons désormais son implémentation concrète à travers les différentes couches applicatives : frontend, backend et service IA.
\section{Implémentation}
\subsection{Frontend : application Angular}
\subsubsection{Principales pages et navigation}
\paragraph{5.1.1.1 Pages et routes}
Routes principales (source : \path{app.routes.ts}).
Le tableau~\ref{tab:frontend-routes} présente une vue synthétique des chemins disponibles, des composants associés et du niveau de protection appliqué.
\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.28}
\newcommand{\badgepub}{\colorbox{green!15}{\strut\path{publique}}}
\newcommand{\badgeguard}[1]{\colorbox{blue!10}{\strut\path{#1}}}
\begin{tabular}{|>{\raggedright\arraybackslash}p{1.9cm}|>{\raggedright\arraybackslash}p{2.7cm}|>{\raggedright\arraybackslash}p{2.3cm}|>{\raggedright\arraybackslash}p{7.3cm}|}
\hline
\rowcolor{techheader}
\textcolor{white}{\textbf{Route}} & \textcolor{white}{\textbf{Page}} & \textcolor{white}{\textbf{Protection}} & \textcolor{white}{\textbf{R\^ole principal}} \\
\hline
\rowcolor{techlinea}
\cellcolor{techroute}\path{/} & Page d'accueil & \badgepub & Page d'accueil et point d'entr\'ee vers l'authentification. \\
\hline
\rowcolor{techlineb}
\cellcolor{techroute}\path{/auth} & Page d'authentification & \badgeguard{LoginGuard} & Connexion/inscription dans une seule vue. \\
\hline
\rowcolor{techlinea}
\cellcolor{techroute}\path{/upload} & Page d'upload & \badgeguard{AuthGuard} & Page pour envoyer une vid\'eo \`a la plateforme. \\
\hline
\rowcolor{techlineb}
\cellcolor{techroute}\path{/myvideos} & Page My Videos & \badgeguard{AuthGuard} & Liste des vid\'eos de l'utilisateur. \\
\hline
\rowcolor{techlinea}
\cellcolor{techroute}\path{/video/:id} & Page vid\'eo & \badgeguard{AuthGuard} & Lecture vid\'eo, affichage des m\'etadonn\'ees (titre/r\'esum\'e) et suivi du statut de traitement en temps r\'eel. \\
\hline
\rowcolor{techlineb}
\cellcolor{techroute}\path{/settings} & Page param\`etres & \badgeguard{AuthGuard} & Mise \`a jour du profil (nom/pr\'enom/avatar), de l'email et du mot de passe. \\
\hline
\end{tabular}
\caption{Routes frontend, pages associ\'ees et responsabilit\'es}
\label{tab:frontend-routes}
\end{table}

\paragraph{5.1.1.2 La navigation métier et les redirections automatiques}

Pour compléter cette vue statique, la figure~\ref{fig:frontend-nav} illustre la navigation réelle entre les pages, y compris les redirections liées aux guards et les transitions métier.
\begin{figure}[H]
\centering
\resizebox{\textwidth}{!}{%
\begin{tikzpicture}[
    page/.style={rectangle, rounded corners=6pt, draw=#1!70, fill=#1!10,
                 text width=3.7cm, align=center, font=\scriptsize\bfseries,
                 minimum height=1.0cm, inner sep=3pt},
    guard/.style={rectangle, rounded corners=3pt, draw=orange!70, fill=orange!15,
                 text width=2.5cm, align=center, font=\scriptsize, minimum height=0.6cm, inner sep=2.5pt},
    arrow/.style={-{Stealth[length=4pt]}, thick, #1},
    dashed arrow/.style={-{Stealth[length=4pt]}, dashed, thick, #1},
    label/.style={font=\scriptsize\itshape, text=gray, fill=white, inner sep=1.5pt, align=center}
]

%% NOEUDS PAGES (espacement augmenté)
\node[page=gray]   (home)     at (0,0)        {/ \\ Page d'accueil};
\node[page=gray]   (auth)     at (6.4,0)      {/auth \\ Page d'authentification};

\node[page=green]  (myvideos) at (0,-3.6)     {/myvideos \\ Page My Videos};
\node[page=green]  (upload)   at (6.4,-3.6)   {/upload \\ Page d'upload};

\node[page=green]  (video)    at (0,-7.2)     {/video/:id \\ Page vidéo};
\node[page=green]  (settings) at (6.4,-7.2)   {/settings \\ Page paramètres};

%% GUARDS
\node[guard]  (ag1) at (0,-2.1)   {AuthGuard};
\node[guard]  (ag2) at (6.4,-2.1) {LoginGuard};
\node[guard]  (ag3) at (0,-5.7)   {AuthGuard};
\node[guard]  (ag4) at (6.4,-5.7) {AuthGuard};

%% NAVIGATION DIRECTE
\draw[arrow=black] (home) -- node[label, above]{Sign in / Sign up} (auth);
\draw[arrow=black] (home) -- (ag1);
\draw[arrow=black] (ag1) -- (myvideos);
\draw[arrow=black] (auth) -- (ag2);
\draw[arrow=black] (ag2) -- (upload);
\draw[arrow=black] (myvideos) -- (ag3);
\draw[arrow=black] (ag3) -- (video);
\draw[arrow=black] (upload) -- (ag4);
\draw[arrow=black] (ag4) -- (settings);

%% TRANSITIONS METIER (labels déplacés)
\draw[dashed arrow=violet, bend right=22]
    (upload.south west) to node[label, pos=0.55, below]{après upload} (video.east);

\draw[dashed arrow=teal, bend right=24]
    (auth.south west) to node[label, pos=0.45, left]{après login} (myvideos.north east);

\draw[dashed arrow=gray, bend left=10]
    (myvideos.south) to node[label, pos=0.55, right]{clic vidéo} (video.north);

\draw[dashed arrow=red, bend left=30]
    (ag2.west) to node[label, pos=0.58, below]{déjà connecté\\$\rightarrow$ /myvideos} (myvideos.east);

%% LEGENDE (décalée à droite)
\node[font=\scriptsize\bfseries, draw=gray!40, rounded corners=3pt, fill=gray!5,
      text width=2.8cm, align=center] at (11.5,-0.3) {Légende};

\node[page=green, minimum height=0.55cm, text width=2.4cm, font=\scriptsize\bfseries]
    at (11.5,-1.5) {Page protégée};

\node[page=gray, minimum height=0.55cm, text width=2.4cm, font=\scriptsize\bfseries]
    at (11.5,-2.6) {Page publique};

\node[guard, minimum height=0.55cm, text width=2.4cm]
    at (11.5,-3.7) {Guard};

\draw[dashed arrow=violet] (10.7,-4.8) -- (11.8,-4.8) node[right, font=\scriptsize]{transition métier};
\draw[dashed arrow=red]    (10.7,-5.8) -- (11.8,-5.8) node[right, font=\scriptsize]{redirection guard};
\draw[arrow=black]         (10.7,-6.8) -- (11.8,-6.8) node[right, font=\scriptsize]{navigation directe};

\end{tikzpicture}
}%
\caption{Parcours principal des pages du frontend}
\label{fig:frontend-nav}
\end{figure}

\noindent \textit{Remarque :} ce schéma montre le chemin principal. L'utilisateur peut aussi changer de page via le menu de son avatar.




\subsubsection{Échanges avec le backend}

\paragraph{5.1.2.1 Rôle des services\\[0pt]} 
\begin{itemize}
    \item \textbf{AuthService        :} gère la connexion et l'inscription. Il garde les informations de session côté navigateur et permet de savoir si l'utilisateur est encore connecté.
    \item \textbf{VideoService       :} s'occupe des actions liées aux vidéos : envoyer une vidéo, afficher une vidéo, charger la liste et lancer le traitement.
    \item \textbf{BlobStorageService :} récupère les liens nécessaires pour afficher les fichiers (vidéo, miniature, avatar).
    \item \textbf{UserService        :} met à jour les informations du profil utilisateur.
    \item \textbf{WsService          :} permet de voir l'avancement du traitement d'une vidéo en direct, sans recharger la page.
\end{itemize}

\paragraph{5.1.2.2 Séquence de traitement d'une vidéo : upload, déclenchement IA et suivi temps réel}
Le schéma suivant synthétise le scénario métier principal après l'upload d'une vidéo, depuis l'appel REST initial jusqu'aux mises à jour de statut en temps réel.

\begin{figure}[H]
\centering
\resizebox{0.94\textwidth}{!}{%
\begin{tikzpicture}[
    actor/.style={rectangle, draw=gray!60, rounded corners=3pt, fill=gray!10, minimum width=2.7cm, minimum height=0.8cm, align=center, font=\scriptsize\bfseries},
    lifeline/.style={gray!60, dashed},
    call/.style={-{Stealth[length=4pt]}, thick, blue!70!black},
    async/.style={-{Stealth[length=4pt]}, thick, dashed, violet!80!black},
    note/.style={rectangle, draw=gray!40, fill=gray!7, rounded corners=2pt, font=\scriptsize, align=left}
]

\node[actor] (u) at (0,0) {UploadPage};
\node[actor] (v) at (4.2,0) {VideoService};
\node[actor] (b) at (8.4,0) {Backend API};
\node[actor] (w) at (12.6,0) {WsService};

\draw[lifeline] (u) -- (0,-6.1);
\draw[lifeline] (v) -- (4.2,-6.1);
\draw[lifeline] (b) -- (8.4,-6.1);
\draw[lifeline] (w) -- (12.6,-6.1);

\draw[call] (0,-0.9) -- node[above, font=\tiny]{1) upload + metadata + thumbnail} (4.2,-0.9);
\draw[call] (4.2,-1.6) -- node[above, font=\tiny]{2) POST /api/videos/upload} (8.4,-1.6);
\draw[call] (8.4,-2.3) -- node[above, font=\tiny]{3) id vidéo} (4.2,-2.3);
\draw[call] (4.2,-3.0) -- node[above, font=\tiny]{4) POST /api/videos/process/\{id\}} (8.4,-3.0);
\draw[async] (8.4,-4.0) -- node[above, font=\tiny]{5) statut PENDING/PROCESSING/COMPLETED} (12.6,-4.0);
\draw[call] (12.6,-4.8) -- node[above, font=\tiny]{6) refresh GET /api/videos/\{id\}} (8.4,-4.8);

\node[note] at (6.3,-5.7) {Mise à jour UI sans polling\\grâce au topic \path{/topic/videos/\{id\}}.};

\end{tikzpicture}
}%
\caption{Séquence de traitement d'une vidéo : upload, déclenchement IA et suivi temps réel}
\label{fig:frontend-services}
\end{figure}
\subsubsection{Aperçu des pages principales}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/upload.png}
    \caption{Page d'upload}
    \label{fig:frontend-upload-page}
\end{figure}
\noindent Cette page permet de téléverser une vidéo puis de lancer son traitement.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/myvideos.png}
    \caption{Page \textit{My Videos}}
    \label{fig:frontend-myvideos-page}
\end{figure}
\noindent Cette page affiche les vidéos dans une grille simple. Le menu au dessus de l'avatar, visible à droite en haut, permet aussi d'aller vers le reste des pages.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/video.png}
    \caption{Page vidéo}
    \label{fig:frontend-video-page}
\end{figure}
\noindent Cette page affiche la vidéo traitée, accompagnée d’un titre et d’un résumé. Elle propose également un espace d’échange avec l’IA pour discuter du contenu de la vidéo et poser des questions complémentaires.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.9\linewidth]{images/pages/settings.png}
    \caption{Page \textit{Settings}}
    \label{fig:frontend-settings-page}
\end{figure}
\noindent Cette page centralise les actions de gestion du compte utilisateur, notamment la mise à jour du profil, de l’adresse e-mail et du mot de passe.

\subsection{Backend : API Spring Boot}
\subsubsection{Architecture (controllers, services, repositories)}
\paragraph{5.2.1.1 Organisation en couches}
Le backend Spring Boot suit une architecture en couches qui sépare la responsabilité HTTP, la logique métier et la persistance :
\begin{itemize}
    \item \textbf{Couche controllers :} expose les endpoints REST (\path{/auth}, \path{/users}, \path{/videos}, \path{/messages}, \path{/video-transcripts}, \path{/files}).
    \item \textbf{Couche services :} applique les règles métier (contrôle d'existence, unicité email, validation des timestamps de transcript, orchestration du traitement IA, notifications WebSocket).
    \item \textbf{Couche repositories :} s'appuie sur Spring Data JPA pour accéder à PostgreSQL via des repositories typés (\path{JpaRepository}).
    \item \textbf{Couche mapping :} les mappers MapStruct isolent les conversions Entity $\leftrightarrow$ DTO.
\end{itemize}

\paragraph{5.2.1.2 Vue d'ensemble des interactions}
\begin{figure}[H]
\centering
\includegraphics[width=0.98\linewidth]{images/Architecture technique du backend.png}
\caption{Architecture technique du backend OCULA}
\label{fig:backend-architecture}
\end{figure}

\noindent Cette séparation permet de faire évoluer chaque couche de façon indépendante. Les changements d'API restent dans les controllers, les règles métier restent dans les services, et la couche d'accès aux données reste concentrée dans les repositories.

\subsubsection{Gestion des utilisateurs et des vidéos}
\paragraph{5.2.2.1 Gestion des utilisateurs}
La gestion des utilisateurs s'appuie sur deux chemins complémentaires :
\begin{itemize}
    \item \textbf{Authentification} : \path{POST /auth/signup} crée un compte avec mot de passe hashé (\path{BCrypt}), et \path{POST /auth/login} retourne un JWT (\path{token} + \path{expiresIn} + informations utilisateur).
    \item \textbf{Profil} : \path{PUT /users/update/\{id\}} accepte un payload \path{multipart/form-data} composé d'un avatar optionnel et d'un objet JSON \path{user}. Si un nouvel avatar est envoyé, l'ancien fichier est supprimé du Blob Storage avant mise à jour.
\end{itemize}

\paragraph{5.2.2.2 Gestion des vidéos et cycle de traitement}
Le cycle principal de vie d'une vidéo est orchestré par \path{VideoController} :
\begin{enumerate}
    \item \textbf{Upload} (\path{POST /videos/upload}) : réception de la vidéo, de la miniature et des métadonnées, génération de clés Blob (\path{userId/videos/...}, \path{userId/thumbnails/...}), envoi des fichiers vers Azure Blob, puis création de la ligne \path{Video} en base avec statut initial \path{PENDING}.
    \item \textbf{Déclenchement IA} (\path{POST /videos/process/\{id\}}) : délégation au service Flask via \path{AiService}.
    \item \textbf{Callbacks techniques} : le service IA met à jour l'état via \path{PUT /videos/update/status/\{id\}} (PROCESSING/COMPLETED/FAILED), injecte les transcripts via \\ \path{POST /video-transcripts}, puis met à jour titre et résumé via \path{PUT /videos/update/\{id\}}.
    \item \textbf{Retour temps réel} : chaque changement de statut déclenche \path{WebsocketNotifier} qui publie sur \path{/topic/videos/\{id\}}.
\end{enumerate}

\noindent La pagination est gérée côté backend sur les listes métier (\path{/videos/page/\{userId\}}, \path{/messages/page/\{videoId\}}), ce qui limite la charge transmise au frontend.

\subsubsection{Endpoints REST : conception et validation}
\paragraph{5.2.3.1 Endpoints clefs}
Le tableau~\ref{tab:backend-endpoints} résume les routes principales et leur rôle.
\begin{table}[H]
\centering
\small
\setlength{\tabcolsep}{4pt}
\renewcommand{\arraystretch}{1.25}
\newcommand{\backendpub}{\colorbox{green!15}{\strut\scriptsize\path{publique}}}
\newcommand{\backendjwt}{\colorbox{blue!10}{\strut\scriptsize\path{JWT}}}
\newcommand{\backendint}{\colorbox{orange!18}{\strut\scriptsize\path{interne}}}
\begin{tabular}{|>{\raggedright\arraybackslash}p{5cm}|>{\centering\arraybackslash}p{1.6cm}|>{\raggedright\arraybackslash}p{4.5cm}|>{\raggedright\arraybackslash}p{5.5cm}|}
\hline
\rowcolor{techheader}
\textcolor{white}{\textbf{Endpoint}} & \textcolor{white}{\textbf{Acc\`es}} & \textcolor{white}{\textbf{Validation / r\`egles}} & \textcolor{white}{\textbf{R\^ole / r\'eponse}} \\
\hline
\rowcolor{techlinea}
\path{POST /auth/signup} & \backendpub & Email requis, email non déjà utilisé. & Crée un utilisateur, renvoie l'entité persistée. \\
\hline
\rowcolor{techlineb}
\path{POST /auth/login} & \backendpub & Email requis, mot de passe requis, identifiants valides. & Renvoie \path{token}, \path{expiresIn}, \path{userInfo}. \\
\hline
\rowcolor{techlinea}
\path{POST /videos/upload} & \backendjwt & \path{userId} valide, fichier vidéo présent, miniature présente. & Crée la vidéo (statut \path{PENDING}) et retourne \path{VideoDto}. \\
\hline
\rowcolor{techlineb}
\path{POST /videos/process/\{id\}} & \backendjwt / \backendint & Id vidéo existant. & Déclenche \path{POST /process} côté service IA. \\
\hline
\rowcolor{techlinea}
\path{PUT /videos/update/status/\{id\}} & \backendjwt / \backendint & Id vidéo existant, statut autorisé dans \path{VideoStatus}. & Met à jour le statut et notifie WebSocket. \\
\hline
\rowcolor{techlineb}
\path{PUT /users/update/\{id\}} & \backendjwt & Id utilisateur existant, format \path{multipart/form-data} valide. & Met à jour profil/email/password/avatar. \\
\hline
\rowcolor{techlinea}
\path{GET /videos/page/\{userId\}} & \backendjwt & \path{userId} valide, paramètres de pagination cohérents. & Retourne un \path{Page<VideoDto>}. \\
\hline
\rowcolor{techlineb}
\path{POST /video-transcripts} & \backendjwt / \backendint & \path{videoId} valide, temps requis, temps positifs, \path{endTime >= startTime}. & Persiste un segment de transcript. \\
\hline
\rowcolor{techlinea}
\path{GET /messages/page/\{videoId\}} & \backendjwt & \path{videoId} valide, pagination cohérente. & Historique chat paginé pour la vidéo. \\
\hline
\path{GET /files/get-url?key=...} & \backendjwt & Clé Blob fournie, objet Blob accessible. & Renvoie une URL signée de lecture Blob. \\
\hline
\end{tabular}
\caption{Synthèse des endpoints backend principaux}
\label{tab:backend-endpoints}
\end{table}

\paragraph{5.2.3.2 Stratégie de validation et gestion d'erreurs}
La validation combine des contrôles techniques et métier :
\begin{itemize}
    \item contrôles d'existence (utilisateur, vidéo, message, transcript) avant mutation
    \item contraintes métier explicites (ex. timestamps de transcript, unicité email) dans les services
    \item exception handling centralisé via \path{@ControllerAdvice} avec mapping HTTP : \path{400} (transcript invalide), \path{401} (authentification), \path{404} (ressource absente), \path{409} (email déjà utilisé)
\end{itemize}

\subsubsection{Sécurité : authentification et autorisation}

La sécurité est définie par \path{SecurityFilterChain} en mode \textit{stateless} :
\begin{itemize}
    \item \path{/auth/**} et \path{/**} sont publics
    \item toutes les autres routes exigent une authentification
    \item deux mecanismes sont supportes : JWT utilisateur (\path{Authorization: Bearer ...}) et cle interne machine-a-machine (\path{X-API-KEY}) pour les callbacks du service IA.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/Chaîne de sécurité backen.png}
\caption{Chaîne de sécurité backend (JWT + clé interne)}
\label{fig:backend-security-chain}
\end{figure}

\subsection{Service IA : LangChain + Flask}
Le traitement intelligent des vidéos est assuré par un microservice dédié développé en Python et exposé via Flask. 
Ce service prend en charge les opérations computationnelles intensives du système : transcription audio, génération d’embeddings, indexation vectorielle et mécanisme de question–réponse basé sur le RAG.

Contrairement au backend Spring Boot, qui gère les aspects transactionnels et la logique métier applicative, le service IA est spécialisé dans les traitements liés aux modèles de langage et à la recherche sémantique. 
Cette séparation permet d’isoler les dépendances liées aux modèles IA, de limiter l’impact des traitements lourds sur le backend principal et de faire évoluer indépendamment les composants.

L’orchestration interne repose sur LangChain, utilisé ici comme couche d’intégration entre les différents modules IA, et non comme framework applicatif complet. 
LangChain structure les interactions entre les embeddings, la base vectorielle et le modèle de langage externe.

Concrètement, son utilisation dans le service IA couvre :

\begin{itemize}
    \item \textbf{Embeddings} : la classe \path{HuggingFaceEmbeddings} 
    charge le modèle \path{sentence-transformers/all-MiniLM-L6-v2} afin de vectoriser les chunks avant leur stockage dans ChromaDB.
    
    \item \textbf{Base vectorielle} : \path{langchain\_chroma.Chroma} sert d’interface avec ChromaDB pour l’ajout de documents (\path{add\_documents}) et la recherche par similarité (\path{similarity\_search\_with\_score}).
    
    \item \textbf{Prompting et chaining} : les différents prompts (titre, résumé, réécriture, RAG) sont définis via \path{PromptTemplate} et exécutés sous forme de pipeline :
    \begin{verbatim}
prompt | llm | StrOutputParser()
    \end{verbatim}
    
    \item \textbf{Appel au LLM} : \path{ChatCerebras} permet d’interagir avec le modèle externe \path{gpt-oss-120b} pour la génération du titre, du résumé et des réponses RAG.
\end{itemize}

LangChain assure ainsi la cohérence du flux complet, reliant définition du prompt, appel au modèle, parsing des sorties et interaction avec la base vectorielle. 
Les sous-sections suivantes détaillent successivement les différentes phases du pipeline : transcription, chunking, génération d’embeddings et mécanisme RAG.


\subsubsection{Extraction audio et transcription (Whisper)}

Le déclenchement du traitement d’une vidéo est effectué via l’endpoint \path{/process} exposé par le service Flask. 
Afin d’éviter le blocage de la requête HTTP, le pipeline est exécuté dans un thread Python dédié. 
Cette approche permet au backend principal de rester réactif tout en assurant un traitement potentiellement long.

\paragraph{Chargement du modèle Whisper}

Le modèle Whisper est chargé une seule fois à l’initialisation du service IA :

\begin{lstlisting}[language=Python, caption={Chargement du modèle Whisper}, label={lst:whisper-load}]
    # ai_part/services/ai_models.py
    import whisper
    whisper_model = whisper.load_model("base")
\end{lstlisting}


Le choix du modèle \path{"base"} constitue un compromis entre précision de transcription et coût computationnel, adapté à une exécution locale.

\paragraph{Pipeline de traitement}

Lorsqu’une requête de traitement est reçue, la fonction principale orchestre les différentes étapes :

\begin{lstlisting}[language=Python, caption={Chargement du modèle Whisper}, label={lst:whisper-load}]
# ai_part/services/processing.py
def processing_task(video_id, object_key):
    try:
        update_status(video_id, "PROCESSING")

        temp_filename = None
        try:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as temp_file:
                temp_filename = temp_file.name
                print(f"Downloading blob: {object_key} to {temp_filename}")
                blob_client = container_client.get_blob_client(object_key)
                blob_data = blob_client.download_blob().readall()
                temp_file.write(blob_data)
                
            transcription = whisper_model.transcribe(temp_filename, verbose=True)
            save_transcript_chunk(video_id, transcription['segments'])
            embed_script(video_id, chunk_video_transcript(transcription))
            analyze(video_id, transcription['text'])

        finally:
            if temp_filename and os.path.exists(temp_filename):
                try:
                    os.remove(temp_filename)
                except Exception as e:
                    print(f"Warning: Could not remove temp file {temp_filename}: {e}")
        
        update_status(video_id, "COMPLETED")

    except Exception as e:
        print(f"Error in transcription task: {e}")
        update_status(video_id, "FAILED")
    pass
\end{lstlisting}

La vidéo est d’abord téléchargée depuis Azure Blob Storage à l’aide de sa clé d’objet (\path{object\_key}) vers un fichier temporaire local. 
La transcription est ensuite effectuée via \path{whisper\_model.transcribe}, qui retourne :

\begin{itemize}
    \item un texte global (\path{text}),
    \item une liste de segments contenant \path{start}, \path{end} et \path{text}.
\end{itemize}

Les segments sont persistés côté backend sous forme d’entités \textit{VideoTranscript}, conservant ainsi la granularité temporelle nécessaire à la traçabilité et au futur découpage en chunks.

Le statut de la vidéo est mis à jour au début (\path{PROCESSING}) puis à la fin du traitement (\path{COMPLETED}). 
En cas d’exception, le statut peut être positionné à \path{FAILED}, garantissant une gestion explicite du cycle de vie du traitement.

Le fichier temporaire est nettoyé systématiquement en fin de traitement (\path{finally}), même en cas d’erreur.

Cette gestion d’état et de ressources permet au frontend d’afficher l’avancement du traitement tout en assurant la cohérence et la robustesse du système dans un contexte distribué.

\subsubsection{Chunking : stratégies et paramètres}
\paragraph{Reconstruction des bornes temporelles des chunks} 
\mbox{}\\
Le découpage en chunks est réalisé sur le texte complet issu de Whisper (\path{transcription['text']}) via \path{RecursiveCharacterTextSplitter}. 
Cependant, pour préserver la traçabilité temporelle, chaque chunk doit être associé à un intervalle \path{[start, end]} exploitable comme source dans l’interface.

Pour cela, le service construit d’abord une correspondance (\textit{segment\_map}) entre la progression en caractères dans le texte global et les segments Whisper. 
Chaque segment est représenté par une borne \path{end\_char} et ses timestamps \path{start/end}. 
Ensuite, pour chaque chunk extrait, sa position \path{[chunk\_start, chunk\_end]} dans le texte global est retrouvée, puis convertie en timestamps en recherchant les segments couvrant ces bornes.

Cette approche permet d’associer à chaque chunk un intervalle temporel cohérent, tout en conservant un découpage textuel adapté au retrieval.
\begin{lstlisting}[language=Python, caption={Découpage en chunks avec reconstruction des timestamps}, label={lst:chunk-video-transcript}]
def chunk_video_transcript(transcription, chunk_size=RAG_CHUNK_SIZE, chunk_overlap=RAG_CHUNK_OVERLAP):
    if not transcription:
        return []

    full_text = transcription['text']
    segments = transcription['segments']
    
    segment_map = []
    current_char = 0
    for seg in segments:
        length = len(seg['text'])
        segment_map.append({
            'end_char': current_char + length,
            'start': seg['start'],
            'end': seg['end']
        })
        current_char += length

    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    text_chunks = splitter.split_text(full_text)
    
    result_chunks = []
    current_search_start = 0
    
    for chunk in text_chunks:
        chunk_start = full_text.find(chunk, current_search_start)
        if chunk_start == -1:
            chunk_start = full_text.find(chunk)
        if chunk_start == -1:
            continue
            
        chunk_end = chunk_start + len(chunk)
        current_search_start = chunk_start + 1
        
        start_time = segment_map[0]['start']
        for item in segment_map:
            if item['end_char'] > chunk_start:
                start_time = item['start']
                break
        
        end_time = segment_map[-1]['end']
        for item in segment_map:
            if item['end_char'] >= chunk_end:
                end_time = item['end']
                break
        
        result_chunks.append({
            'text': chunk,
            'start': start_time,
            'end': end_time
        })

    print("result_chunks", result_chunks)
    return result_chunks
\end{lstlisting}

\subsubsection{Génération d'embeddings (MiniLM)}

Après l’étape de chunking, chaque chunk est converti en représentation vectorielle afin de permettre une recherche sémantique efficace lors du mécanisme RAG. 
Le service IA utilise le modèle \path{sentence-transformers/all-MiniLM-L6-v2}, exécuté localement sur CPU, via l’intégration LangChain \path{HuggingFaceEmbeddings}.

\paragraph{Initialisation du modèle d'embedding}

Le modèle d’embedding est chargé une seule fois à l’initialisation du service IA, afin d’éviter une surcharge à chaque requête. 
Dans l’implémentation actuelle, les embeddings ne sont pas normalisés lors de l’encodage (\path{normalize\_embeddings=False}), et l’exécution est configurée sur CPU.


\begin{lstlisting}[language=Python, caption={Initialisation du modèle d'embedding}, label={lst:embed-model-init}]
# ai_part/services/ai_models.py
from langchain_huggingface import HuggingFaceEmbeddings

model_name = "sentence-transformers/all-MiniLM-L6-v2"
model_kwargs = {'device': 'cpu'}
encode_kwargs = {'normalize_embeddings': False}

embed_model = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs=model_kwargs,
    encode_kwargs=encode_kwargs
)
\end{lstlisting}

\paragraph{Construction des documents vectoriels}

Chaque chunk est encapsulé dans un objet \path{Document} (LangChain) contenant :
(i) le texte du chunk (\path{page\_content}), et (ii) les métadonnées nécessaires à la traçabilité : \path{video\_id}, \path{start\_time} et \path{end\_time}. 
Ces métadonnées permettent notamment de restituer des sources temporelles associées aux réponses du chatbot.

\begin{lstlisting}[language=Python, caption={Création des documents avec métadonnées}, label={lst:doc-metadata}]
# ai_part/services/database.py
from langchain_core.documents import Document

doc = Document(
    page_content=chunk['text'],
    metadata={
        "video_id": video_id,
        "start_time": chunk['start'],
        "end_time": chunk['end'],
    }
)
\end{lstlisting}

\paragraph{Indexation dans ChromaDB}

Les documents vectoriels sont stockés dans ChromaDB via l’interface \path{langchain\_chroma.Chroma}. 
L’indexation est organisée par vidéo : chaque vidéo possède une collection dédiée nommée \path{video\_<video\_id>}. 
Cette stratégie limite le bruit inter-vidéos et garantit que la recherche sémantique effectuée lors du chat reste strictement associée à la vidéo consultée.

\begin{lstlisting}[language=Python, caption={Indexation des documents dans une collection Chroma dédiée}, label={lst:chroma-index}]
# ai_part/services/database.py
from langchain_chroma import Chroma

collection_name = f"video_{str(video_id)}"

vector_store = Chroma(
    client=ai_models.db_client,
    collection_name=collection_name,
    embedding_function=ai_models.embed_model
)
vector_store.add_documents(docs_to_add)
\end{lstlisting}

\paragraph{Intégration au pipeline de traitement}

L’indexation des chunks intervient immédiatement après la transcription et le chunking, au sein du pipeline de traitement de la vidéo :

\begin{lstlisting}[language=Python, caption={Enchaînement transcription $\rightarrow$ chunking $\rightarrow$ embeddings}, label={lst:pipeline-embed}]
# ai_part/services/processing.py
transcription = whisper_model.transcribe(temp_filename, verbose=True)
chunks = chunk_video_transcript(transcription)
embed_script(video_id, chunks)
\end{lstlisting}

Ainsi, à l’issue du traitement, chaque vidéo est associée à une collection Chroma contenant les chunks vectorisés et leurs métadonnées temporelles, base nécessaire au retrieval utilisé dans la phase RAG.

\subsubsection{RAG : retrieval et construction du prompt}
\noindent \textit{À compléter.}

\subsubsection{Appel au LLM externe et post-traitements}
\noindent \textit{À compléter.}

\subsection{Stockage et Dockerisation}

\subsubsection{Stockage vidéo et fichiers (Azure Blob Storage)}

Notre application Ocula utilise le service Azure Blob Storage pour stocker l’ensemble des fichiers importés par les utilisateurs. Ceux-ci peuvent notamment enregistrer leur avatar, les vidéos à traiter ainsi que les miniatures (thumbnails) extraites de ces vidéos.

\begin{lstlisting}
	public void uploadFile(String fileName, InputStream data, long size) {
		BlobClient blobClient = containerClient.getBlobClient(fileName);
		blobClient.upload(data, size, true);
	}
\end{lstlisting}

La fonction \textbf{uploadFile} permet d’envoyer un fichier vers Azure Blob Storage. Elle utilise le nom du fichier comme clé unique pour récupérer un client Azure (BlobClient), puis elle transfère les données du fichier vers le conteneur de stockage. Si un fichier portant le même nom existe déjà, il est remplacé. Cette méthode sert donc à stocker physiquement les fichiers des utilisateurs dans le cloud.

\begin{lstlisting}
	@PostMapping("/upload")
	public ResponseEntity<VideoDto> createVideoWithUpload(
	@RequestParam("video") MultipartFile video,
	@RequestParam("thumbnail") MultipartFile thumbnail,
	@RequestParam("title") String title,
	@RequestParam("summary") String summary,
	@RequestParam("userId") UUID userId,
	@RequestParam("duration") String durationSeconds) throws IOException, UserNotFoundException {
		
		Duration duration = Duration.ofSeconds(Long.parseLong(durationSeconds));
		
		String videoKey = userId.toString() + "/videos/" + UUID.randomUUID().toString()
		+ video.getOriginalFilename().replaceAll("[^a-zA-Z0-9._-]", "_");
		String thumbnailKey = userId.toString() + "/thumbnails/" + UUID.randomUUID().toString()
		+ thumbnail.getOriginalFilename().replaceAll("[^a-zA-Z0-9._-]", "_");
		
		blobStorageService.uploadFile(videoKey, video.getInputStream(), video.getSize());
		blobStorageService.uploadFile(thumbnailKey, thumbnail.getInputStream(), thumbnail.getSize());
		
		VideoDto createdVideo = videoService.createVideoWithUpload(videoKey, thumbnailKey, title, summary, userId,
		duration);
		
		return ResponseEntity.ok(createdVideo);
	}
\end{lstlisting}

La méthode \textbf{createVideoWithUpload} sert à gérer tout le processus d’upload côté application. Elle reçoit les fichiers envoyés par l’utilisateur (vidéo et miniature), génère des clés uniques pour organiser leur stockage dans Azure Blob Storage, puis appelle le service d’upload pour les enregistrer. Enfin, elle sauvegarde dans la base de données les informations liées à la vidéo ainsi que les chemins des fichiers stockés.

Pour accéder à l’un de ces fichiers, nous utilisons une clé unique qui permet de générer une URL temporaire et sécurisée, garantissant un accès contrôlé aux ressources.

\begin{lstlisting}
	public String getFileUrl(String fileName) {
		BlobClient blobClient = containerClient.getBlobClient(fileName);
		
		OffsetDateTime expiryTime = OffsetDateTime.now().plusHours(6);
		BlobSasPermission permission = new BlobSasPermission().setReadPermission(true);
		
		BlobServiceSasSignatureValues values = new BlobServiceSasSignatureValues(expiryTime, permission)
		.setStartTime(OffsetDateTime.now().minusMinutes(5));
		
		return blobClient.getBlobUrl() + "?" + blobClient.generateSas(values);
	}
\end{lstlisting}

La fonction \textbf{getFileUrl} permet de générer un lien sécurisé pour accéder à un fichier stocké dans Azure Blob Storage. Elle crée un token SAS (Shared Access Signature) qui donne une permission de lecture limitée dans le temps. L’URL finale contient ce token, ce qui permet d’accéder temporairement au fichier sans exposer les informations sensibles du compte Azure.

\subsubsection{Base de données}

Notre application Ocula repose sur une architecture hybride utilisant deux types de bases de données complémentaires : une base de données relationnelle \textbf{PostgreSQL} et une base de données vectorielle \textbf{ChromaDB}. Ce choix permet de répondre aux différents besoins de stockage et de traitement des données de l’application.

La base de données \textbf{PostgreSQL} est utilisée pour stocker les données structurées de l’application. Elle contient notamment les informations relatives aux utilisateurs, aux vidéos importées, les messages et les transcriptions détectée.

En complément, \textbf{ChromaDB} est utilisée comme base de données vectorielle afin de gérer les représentations vectorielle de la transcription de vidéo. Ces vecteurs sont stockés afin de les exploiter dans les traitements de RAG.

L’utilisation combinée de ces deux bases permet ainsi de séparer les responsabilités : \textbf{PostgreSQL} assure la gestion des données structurées, tandis que \textbf{ChromaDB} prend en charge le stockage et l’interrogation des données vectorielles nécessaires aux fonctionnement d’intelligence artificielle de l’application.

\subsubsection{Dockerisation}

L'ensemble de l'application \textbf{OCULA} est conteneurisé à l'aide de \textbf{Docker} et orchestré avec \textbf{Docker Compose}. Cette approche garantit la portabilité de l'application, l'isolation des environnements et une gestion simplifiée des dépendances entre les services.

\begin{lstlisting}
	FROM maven:3.9.9-eclipse-temurin-21 AS build
	WORKDIR /app
	
	COPY pom.xml .
	COPY .mvn .mvn
	COPY mvnw mvnw
	
	RUN mvn -q -DskipTests dependency:go-offline
	
	COPY src src
	RUN mvn -q -DskipTests package
	
	FROM eclipse-temurin:21-jre
	WORKDIR /app
	
	COPY --from=build /app/target/*.jar /app/app.jar
	
	EXPOSE 8080
	
	ENTRYPOINT ["java", "-jar", "/app/app.jar"]
\end{lstlisting}

Ce Dockerfile permet d'exécuter le backend d'Ocula. La première étape utilise Maven pour télécharger les dépendances, compiler le code source et générer un fichier JAR. La seconde étape utilise une image contenant le Java, copie le JAR généré, expose le port 8080 et lance automatiquement l’application avec la commande java -jar.

\begin{lstlisting}
	FROM python:3.10-slim
	
	ENV PYTHONDONTWRITEBYTECODE=1 \
	PYTHONUNBUFFERED=1 \
	VIRTUAL_ENV=/opt/venv \
	PATH="/opt/venv/bin:$PATH"
	
	WORKDIR /app
	
	COPY requirements.txt /app/requirements.txt
	RUN apt-get update && apt-get install -y ffmpeg \
	&& rm -rf /var/lib/apt/lists/* \
	&& python -m venv /opt/venv \
	&& pip install --upgrade pip \
	&& pip install -r /app/requirements.txt
	
	COPY . /app
	
	EXPOSE 5000
	ENV PORT=5000
	
	CMD ["python", "app.py"]
\end{lstlisting}

Ce Dockerfile permet d'exécuter la partie IA d'Ocula. Il utilise une image de Python 3.10, configure des variables d’environnement, puis crée un environnement virtuel dans lequel il installe les dépendances listées dans le fichier requirements.txt, ainsi que l’outil ffmpeg nécessaire au fonctionnement de Whisper. Ensuite, il copie le code source dans le conteneur, expose le port 5000 et lance automatiquement l’application avec la commande python app.py.

\begin{lstlisting}
	FROM node:20-alpine AS build
	
	WORKDIR /app
	
	COPY package*.json ./
	RUN npm install
	
	COPY . .
	RUN npm run build 
	
	FROM nginx:1.27-alpine
	
	COPY nginx.conf /etc/nginx/conf.d/default.conf
	COPY --from=build /app/dist/ocula-frontend/browser /usr/share/nginx/html
	
	EXPOSE 80
	
	CMD ["nginx", "-g", "daemon off;"]
\end{lstlisting} 

Ce Dockerfile sert à construire et lancer le frontend de l’application. Il utilise d’abord Node.js pour installer les dépendances du projet, puis il utilise Nginx pour servir les fichiers générés. Enfin, il expose le port 80 et démarre le serveur Nginx pour rendre l’application accessible.

\section{Évaluation et résultats}
\noindent \textit{À compléter.}

\subsection{Protocole de test}
\noindent \textit{À compléter.}

\subsubsection{Jeu de vidéos utilisé}
\noindent \textit{À compléter.}

\subsubsection{Scénarios de test (upload, transcription, Q\&A)}
\noindent \textit{À compléter.}

\subsubsection{Critères : qualité, latence, stabilité}
\noindent \textit{À compléter.}

\subsection{Analyse qualitative}
\noindent \textit{À compléter.}

\subsubsection{Pertinence des résumés et titres}
\noindent \textit{À compléter.}

\subsubsection{Qualité des réponses du chatbot}
\noindent \textit{À compléter.}

\subsubsection{Comportement sur contenu bruité / long / multilingue}
\noindent \textit{À compléter.}

\subsection{Analyse quantitative}
\noindent \textit{À compléter.}

\subsubsection{Temps de traitement par étape (pipeline)}
\noindent \textit{À compléter.}

\subsubsection{Temps de réponse du chatbot}
\noindent \textit{À compléter.}

\subsubsection{Impact du chunking et du top-k retrieval}
\noindent \textit{À compléter.}

\subsection{Discussion}
\noindent \textit{À compléter.}

\subsubsection{Limites observées}
\noindent \textit{À compléter.}

\subsubsection{Risques et biais potentiels}
\noindent \textit{À compléter.}

\subsubsection{Comparaison : RAG vs LLM seul}
\noindent \textit{À compléter.}

\section{Sécurité, conformité et considérations éthiques}
\noindent \textit{À compléter.}

\subsection{Confidentialité des données et stockage}
\noindent \textit{À compléter.}

\subsection{Gestion des accès et des permissions}
\noindent \textit{À compléter.}

\subsection{Propriété intellectuelle et droits sur les contenus vidéo}
\noindent \textit{À compléter.}

\subsection{Limites et usages responsables}
\noindent \textit{À compléter.}

\section{Conclusion et perspectives}
\noindent \textit{À compléter.}

\subsection{Bilan du projet}
\noindent \textit{À compléter.}

\subsection{Améliorations techniques possibles}
\noindent \textit{À compléter.}

\subsubsection{Optimisation de la recherche semantique}
\noindent \textit{À compléter.}

\subsubsection{Streaming transcription}
\noindent \textit{À compléter.}

\subsubsection{Amélioration de la segmentation (chunking adaptatif)}
\noindent \textit{À compléter.}

\subsubsection{Analyse de video frame par frame}
\noindent \textit{À compléter.}

\subsubsection{Infrastructure}
\noindent \textit{À compléter.}

\subsection{Ouverture produit : industrialisation}
\noindent \textit{À compléter.}



% \section{État de l'art}

% Cette section présente les principales applications et plateformes existantes proposant des fonctionnalités proches de celles du projet OCULA. Pour chaque concurrent, sont analysés les services proposés, le modèle économique ainsi que les principales limitations.

% \subsection{Analyse des solutions existantes}

% \subsubsection{Otter.ai}

% \textbf{Description} \\
% Otter.ai est une application spécialisée dans la transcription automatique de contenus audio et vidéo, principalement orientée vers les réunions professionnelles. Elle propose également des résumés automatiques et une recherche textuelle dans les transcriptions.

% \textbf{Fonctionnalités principales}
% \begin{itemize}
%     \item Transcription automatique en temps réel ou différé
%     \item Résumé automatique de réunions
%     \item Recherche par mots-clés dans les transcriptions
% \end{itemize}

% \textbf{Modèle économique}  
% Solution freemium avec des fonctionnalités avancées accessibles uniquement via abonnement payant.

% \textbf{Limites}
% \begin{itemize}
%     \item Orienté réunions, peu adapté aux vidéos longues génériques
%     \item Absence de chatbot conversationnel basé sur RAG
%     \item Pipeline de traitement fermé et non personnalisable
% \end{itemize}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{image.png}
%     \caption{Page tarification de Otter.ai}
%     \label{fig:otter}
% \end{figure}
% ---------------------------------------

% \subsubsection{Fireflies.ai}

% \textbf{Description} \\  
% Fireflies.ai est un outil d’analyse automatique de réunions intégrant transcription, résumé et recherche. Il s’intègre directement à des plateformes de visioconférence.

% \textbf{Fonctionnalités principales}
% \begin{itemize}
%     \item Transcription automatique de réunions
%     \item Résumés et points clés
%     \item Recherche dans les conversations
% \end{itemize}

% \textbf{Modèle économique}  
% Service payant avec abonnement mensuel.

% \textbf{Limites}
% \begin{itemize}
%     \item Fortement dépendant des outils de visioconférence
%     \item Pas conçu pour l’analyse libre de fichiers vidéo
%     \item Absence d’interrogation conversationnelle avancée
% \end{itemize}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{image2.png}
%     \caption{Service et tarification Fireflies.ai}
%     \label{fig:placeholder}
% \end{figure}

% % --------------------------------------------------

% \subsubsection{Google (YouTube / Vertex AI)}

% \textbf{Description}  
% Google propose plusieurs solutions permettant l’analyse de contenus vidéo. YouTube intègre la génération automatique de sous-titres, tandis que Vertex AI et Speech-to-Text permettent la transcription et l’analyse via API.

% \textbf{Fonctionnalités principales}
% \begin{itemize}
%     \item Transcription automatique de vidéos
%     \item Recherche textuelle basique
%     \item APIs d’analyse audio et vidéo
% \end{itemize}

% \textbf{Modèle économique}  
% YouTube est gratuit pour l’utilisateur final, tandis que les APIs Google Cloud sont facturées à l’usage.

% \textbf{Limites}
% \begin{itemize}
%     \item Pas de chatbot conversationnel sur le contenu vidéo
%     \item Solutions fragmentées (plusieurs services à combiner)
%     \item Mise en place technique complexe
% \end{itemize}


% % --------------------------------------------------

% \subsubsection{OpenAI (ChatGPT / Whisper API)}

% \textbf{Description}  
% OpenAI propose des modèles de langage et de transcription permettant l’analyse de contenus textuels et audio. ChatGPT permet l’interrogation conversationnelle de documents, tandis que Whisper assure une transcription automatique de haute qualité.

% \textbf{Fonctionnalités principales}
% \begin{itemize}
%     \item Transcription audio multilingue
%     \item Génération de résumés
%     \item Question-réponse conversationnelle
% \end{itemize}

% \textbf{Modèle économique}  
% APIs payantes facturées à l’usage, avec accès gratuit limité via l’interface ChatGPT.

% \textbf{Limites}
% \begin{itemize}
%     \item Pas de gestion native de vidéos complètes
%     \item Absence de pipeline clé-en-main pour l’analyse vidéo
%     \item Pas d’indexation temporelle automatique
% \end{itemize}


% \begin{table}[H]
% \centering
% \renewcommand{\arraystretch}{1.3}
% \begin{tabular}{|l|c|c|c|c|}
% \hline
% \textbf{Solution} & \textbf{Vidéo} & \textbf{Résumé} & \textbf{Chat RAG} & \textbf{Pipeline ouvert} \\
% \hline
% Otter.ai & \checkmark & \checkmark & \texttimes & \texttimes \\
% Fireflies.ai & \checkmark & \checkmark & \texttimes & \texttimes \\
% Google (YouTube) & \checkmark & \texttimes & \texttimes & \texttimes \\
% OpenAI (API) & \texttimes & \checkmark & \checkmark & \texttimes \\
% \textbf{OCULA} & \checkmark & \checkmark & \checkmark & \checkmark \\
% \hline
% \end{tabular}
% \caption{Comparaison fonctionnelle entre OCULA et les solutions existantes}
% \label{tab:comparison}
% \end{table}

% % ==================================================
% \section{Différenciation du projet OCULA}

% Bien que plusieurs applications et plateformes proposent des fonctionnalités proches de celles du projet OCULA, aucune ne couvre l’ensemble du périmètre fonctionnel visé. OCULA se distingue des solutions existantes à plusieurs niveaux, tant sur le plan fonctionnel que technique.

% \subsection{Différence de périmètre fonctionnel}

% Les applications existantes se concentrent généralement sur une seule tâche spécifique, telle que la transcription automatique de réunions, le résumé de documents textuels ou l’édition audio et vidéo. À l’inverse, OCULA adopte une approche globale de l’analyse de contenu vidéo.

% OCULA permet :
% \begin{itemize}
%     \item l’upload libre de vidéos, indépendamment de leur source,
%     \item la transcription automatique du contenu audio,
%     \item la génération d’un titre et d’un résumé représentatif,
%     \item l’interrogation conversationnelle du contenu via un chatbot,
%     \item la recherche d’information contextualisée dans l’ensemble de la vidéo.
% \end{itemize}

% Cette combinaison de fonctionnalités, appliquée spécifiquement au contenu vidéo, constitue une première différence majeure avec les solutions existantes.

% \subsection{Différence dans l’approche technique}

% Contrairement aux applications commerciales clés-en-main, OCULA repose sur une architecture modulaire et transparente. Le pipeline de traitement est entièrement contrôlé et comprend des étapes explicites de transcription, de segmentation, d’indexation sémantique et de génération de réponses.

% L’utilisation de la \textit{Retrieval-Augmented Generation} (RAG) permet d’ancrer les réponses du chatbot dans le contenu réel de la vidéo, réduisant ainsi les hallucinations des modèles de langage. Cette approche est rarement proposée de manière explicite dans les applications existantes, qui reposent souvent sur des traitements opaques.

% \subsection{Différence par rapport aux APIs et frameworks}

% Des plateformes comme OpenAI ou Google proposent des APIs puissantes pour la transcription, la génération de texte ou l’analyse sémantique. Toutefois, ces solutions fournissent uniquement des briques technologiques et non une application utilisateur complète.

% OCULA se distingue en intégrant ces briques au sein d’une application cohérente, orientée utilisateur final, tout en gérant des problématiques concrètes telles que :
% \begin{itemize}
%     \item le stockage de données volumineuses,
%     \item le traitement asynchrone de tâches longues,
%     \item la persistance des résultats,
%     \item l’interaction continue entre l’utilisateur et le contenu analysé.
% \end{itemize}

% \subsection{Différence en termes de finalité}

% Enfin, OCULA se positionne comme un projet à la fois expérimental et pédagogique. L’objectif n’est pas de concurrencer directement des solutions commerciales existantes, mais de démontrer la faisabilité et l’intérêt d’une architecture complète d’analyse et d’interrogation de contenus vidéo basée sur les technologies actuelles de l’intelligence artificielle.

% Ainsi, OCULA se distingue par son caractère intégratif, sa flexibilité et sa transparence, là où les solutions existantes restent soit partielles, soit fermées, soit orientées vers des usages spécifiques.

% % --------------------------------------------------
% % ==================================================
% \section{Justification du choix des modèles}

% Dans le cadre du projet OCULA, le choix des modèles de traitement automatique du langage et de la parole a été guidé par plusieurs critères : qualité des résultats, coût d’utilisation, facilité d’intégration, contrôle du pipeline et adéquation avec un projet académique. Cette section détaille les modèles retenus ainsi que les raisons pour lesquelles certaines alternatives n’ont pas été privilégiées.

% \subsection{Modèle d'embeddings : \path{all-MiniLM-L6-v2}}

% Le modèle \path{sentence-transformers/all-MiniLM-L6-v2} est utilisé pour transformer des phrases en représentations vectorielles numériques. Ces vecteurs sont ensuite exploités pour la recherche sémantique dans le cadre de la méthode \textit{Retrieval-Augmented Generation} (RAG).

% \textbf{Justification du choix}
% \begin{itemize}
%     \item Modèle open-source, utilisable localement sans coût d’API.
%     \item Temps d’inférence réduit, adapté à une exécution sur CPU.
%     \item Bon compromis entre performance sémantique et légèreté.
%     \item Large adoption dans les systèmes RAG académiques et industriels.
% \end{itemize}

% \textbf{Alternatives considérées et limites}
% \begin{itemize}
%     \item Modèles plus volumineux (ex. MPNet, BERT large) : performances légèrement supérieures mais coûts computationnels plus élevés.
%     \item Embeddings via API (OpenAI, Cohere) : qualité élevée mais dépendance à un service externe payant.
% \end{itemize}

% Ainsi, \path{all-MiniLM-L6-v2} représente un compromis optimal entre coût, performance et simplicité d’intégration.

% % --------------------------------------------------

% \subsection{Modèle de transcription : Whisper}

% Le modèle Whisper est utilisé pour la transcription automatique des pistes audio extraites des vidéos.

% \textbf{Justification du choix}
% \begin{itemize}
%     \item Modèle open-source permettant une exécution locale.
%     \item Très bonne qualité de transcription, même en présence de bruit.
%     \item Support multilingue natif.
%     \item Large reconnaissance académique et industrielle.
% \end{itemize}

% \textbf{Alternatives considérées et limites}
% \begin{itemize}
%     \item APIs cloud (Google Speech-to-Text, Azure Speech) : solutions performantes mais facturées à l’usage.
%     \item Modèles plus légers (Vosk) : coût nul mais qualité inférieure.
% \end{itemize}

% L’utilisation de Whisper en local permet de réduire les coûts tout en conservant une qualité de transcription élevée, ce qui est particulièrement adapté à un projet académique manipulant des données volumineuses.

% % --------------------------------------------------

% \subsection{Modèle de langage : \path{gpt-oss-120b}}

% Le modèle \path{gpt-oss-120b}, accessible via une API externe fournie par Cerebras, est utilisé pour la compréhension et la génération de texte, notamment pour la génération de résumés et l’interrogation conversationnelle.

% \textbf{Justification du choix}
% \begin{itemize}
%     \item Modèle open-source, garantissant une transparence du fonctionnement.
%     \item Capacités avancées de raisonnement et de génération de texte.
%     \item Accès via API évitant la gestion d’une infrastructure lourde.
%     \item Coût inférieur aux grands modèles propriétaires à performances comparables.
% \end{itemize}

% \textbf{Alternatives considérées et limites}
% \begin{itemize}
%     \item Modèles propriétaires (OpenAI, Google) : très bonnes performances mais coûts élevés et dépendance à des services fermés.
%     \item Modèles open-source auto-hébergés (LLaMA, Mistral) : contrôle total mais besoins matériels importants (GPU).
% \end{itemize}

% Le choix de \path{gpt-oss-120b} permet ainsi de bénéficier de la puissance d’un grand modèle de langage tout en conservant une approche ouverte et économiquement raisonnable.



% % --------------------------------------------------

% \subsection{Synthèse des choix}

% La combinaison de modèles locaux pour les tâches intensives en données (embeddings, transcription) et d’un modèle de langage accessible via API pour les tâches de génération permet d’optimiser les coûts tout en garantissant de bonnes performances. Cette approche hybride offre un équilibre pertinent entre contrôle, scalabilité et qualité, en adéquation avec les objectifs du projet OCULA.




\end{document}
